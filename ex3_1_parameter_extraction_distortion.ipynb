{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Distortion Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMiPTdSA8C2F",
        "outputId": "a297ba92-a5c6-4015-d92d-1e9487daf896"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import os"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gIdrZtW_JYf",
        "outputId": "c1bb6969-15a9-44eb-8cdd-8d79c89208f2"
      },
      "outputs": [],
      "source": [
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050*3\n",
        "\n",
        "mfcc = torchaudio.transforms.MFCC(\n",
        "    sample_rate = SAMPLE_RATE, \n",
        "    n_mfcc = 64,\n",
        "    melkwargs = {\n",
        "        \"n_fft\": 1024,\n",
        "        \"hop_length\": 1024,\n",
        "        \"n_mels\": 64,\n",
        "        \"center\": False})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Functions for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.gtfxdataset import GtFxDataset\n",
        "from src.util import plot_spectrogram\n",
        "from src.extrector import train\n",
        "from src.extrector import model\n",
        "from torch import nn\n",
        "\n",
        "AUDIO_DIR = \"_assets/DATASET/GT-FX-C53/\"\n",
        "ANNOTATIONS_FILE = os.path.join(AUDIO_DIR, \"train.csv\")\n",
        "EVU_ANNOTATIONS_FILE = os.path.join(AUDIO_DIR, \"evaluation.csv\")\n",
        "EFFECT_MAP = [\"distortion\", \"chorus\", \"tremolo\", \"delay\", \"reverb\"]\n",
        "\n",
        "def load_train_data(effect):\n",
        "    \n",
        "    fxData = GtFxDataset(ANNOTATIONS_FILE,\n",
        "                        AUDIO_DIR,\n",
        "                        mfcc,\n",
        "                        SAMPLE_RATE,\n",
        "                        NUM_SAMPLES,\n",
        "                        device,\n",
        "                        effect=EFFECT_MAP[effect])\n",
        "    return fxData\n",
        "\n",
        "def load_evaluation_data(effect):\n",
        "\n",
        "    evuData = GtFxDataset(EVU_ANNOTATIONS_FILE,\n",
        "                        AUDIO_DIR,\n",
        "                        mfcc,\n",
        "                        SAMPLE_RATE,\n",
        "                        NUM_SAMPLES,\n",
        "                        device,\n",
        "                        effect=EFFECT_MAP[effect])\n",
        "\n",
        "    BATCH_SIZE = round(len(evuData) / 1500)\n",
        "    val_dataloader = train.create_data_loader(evuData, BATCH_SIZE)\n",
        "    return val_dataloader\n",
        "\n",
        "def split_data(data):\n",
        "\n",
        "    BATCH_SIZE = round(len(data) / 1500)\n",
        "\n",
        "    split_ratio = [0.9, 0.1]\n",
        "    train_set, test_set = torch.utils.data.random_split(data, lengths=split_ratio)\n",
        "\n",
        "    train_dataloader = train.create_data_loader(train_set, BATCH_SIZE)\n",
        "    test_dataloader = train.create_data_loader(test_set, BATCH_SIZE)\n",
        "\n",
        "    return train_dataloader, test_dataloader   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Add Tensorboard to record data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "EXPERIMENT_NAME = \"c53_parameter\"\n",
        "LOG_DIR = \"_log/\" + EXPERIMENT_NAME\n",
        "EVU_DIR = \"_log/Evaluation/\"\n",
        "\n",
        "if not os.path.exists('%s' % LOG_DIR):\n",
        "    os.makedirs('%s' % LOG_DIR)\n",
        "\n",
        "if not os.path.exists('%s' % EVU_DIR):\n",
        "    os.makedirs('%s' % EVU_DIR)\n",
        "\n",
        "log_writer = SummaryWriter(LOG_DIR)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8O9r5WI2zuI",
        "outputId": "ed2a4ebd-f644-4e35-8fda-86170924dfe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cpu\n",
            "There are 188940 samples in the dataset.\n",
            "Shape of signal: torch.Size([1, 64, 64])\n",
            "input feature:\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "loss: 0.085134  [  0/170046]\n",
            "loss: 0.015244  [2520/170046]\n",
            "loss: 0.007454  [5040/170046]\n",
            "loss: 0.004504  [7560/170046]\n",
            "loss: 0.005647  [10080/170046]\n",
            "loss: 0.003639  [12600/170046]\n",
            "loss: 0.002676  [15120/170046]\n",
            "loss: 0.003893  [17640/170046]\n",
            "loss: 0.002521  [20160/170046]\n",
            "loss: 0.004664  [22680/170046]\n",
            "loss: 0.004139  [25200/170046]\n",
            "loss: 0.003062  [27720/170046]\n",
            "loss: 0.001950  [30240/170046]\n",
            "loss: 0.002622  [32760/170046]\n",
            "loss: 0.002293  [35280/170046]\n",
            "loss: 0.002934  [37800/170046]\n",
            "loss: 0.005538  [40320/170046]\n",
            "loss: 0.002925  [42840/170046]\n",
            "loss: 0.002415  [45360/170046]\n",
            "loss: 0.001705  [47880/170046]\n",
            "loss: 0.003448  [50400/170046]\n",
            "loss: 0.001715  [52920/170046]\n",
            "loss: 0.004025  [55440/170046]\n",
            "loss: 0.003019  [57960/170046]\n",
            "loss: 0.001744  [60480/170046]\n",
            "loss: 0.006267  [63000/170046]\n",
            "loss: 0.002142  [65520/170046]\n",
            "loss: 0.003047  [68040/170046]\n",
            "loss: 0.002993  [70560/170046]\n",
            "loss: 0.001879  [73080/170046]\n",
            "loss: 0.002556  [75600/170046]\n",
            "loss: 0.006701  [78120/170046]\n",
            "loss: 0.006528  [80640/170046]\n",
            "loss: 0.002299  [83160/170046]\n",
            "loss: 0.003880  [85680/170046]\n",
            "loss: 0.001585  [88200/170046]\n",
            "loss: 0.001886  [90720/170046]\n",
            "loss: 0.001396  [93240/170046]\n",
            "loss: 0.002082  [95760/170046]\n",
            "loss: 0.001312  [98280/170046]\n",
            "loss: 0.001530  [100800/170046]\n",
            "loss: 0.001976  [103320/170046]\n",
            "loss: 0.002197  [105840/170046]\n",
            "loss: 0.001521  [108360/170046]\n",
            "loss: 0.001308  [110880/170046]\n",
            "loss: 0.001915  [113400/170046]\n",
            "loss: 0.001156  [115920/170046]\n",
            "loss: 0.001651  [118440/170046]\n",
            "loss: 0.002903  [120960/170046]\n",
            "loss: 0.002181  [123480/170046]\n",
            "loss: 0.001391  [126000/170046]\n",
            "loss: 0.001331  [128520/170046]\n",
            "loss: 0.003713  [131040/170046]\n",
            "loss: 0.001337  [133560/170046]\n",
            "loss: 0.005865  [136080/170046]\n",
            "loss: 0.001819  [138600/170046]\n",
            "loss: 0.002284  [141120/170046]\n",
            "loss: 0.001253  [143640/170046]\n",
            "loss: 0.001221  [146160/170046]\n",
            "loss: 0.002465  [148680/170046]\n",
            "loss: 0.002123  [151200/170046]\n",
            "loss: 0.002016  [153720/170046]\n",
            "loss: 0.001765  [156240/170046]\n",
            "loss: 0.001198  [158760/170046]\n",
            "loss: 0.001766  [161280/170046]\n",
            "loss: 0.001491  [163800/170046]\n",
            "loss: 0.002377  [166320/170046]\n",
            "loss: 0.001900  [168840/170046]\n",
            "distortion gain: avg MSE: 0.005395, avg abs error: 0.0638\n",
            "learning rate: 0.001000 -> 0.000910\n",
            "---------------------------\n",
            "\n",
            "Epoch 2\n",
            "loss: 0.001769  [  0/170046]\n",
            "loss: 0.002424  [2520/170046]\n",
            "loss: 0.001065  [5040/170046]\n",
            "loss: 0.001037  [7560/170046]\n",
            "loss: 0.001645  [10080/170046]\n",
            "loss: 0.001274  [12600/170046]\n",
            "loss: 0.001388  [15120/170046]\n",
            "loss: 0.002101  [17640/170046]\n",
            "loss: 0.002102  [20160/170046]\n",
            "loss: 0.001071  [22680/170046]\n",
            "loss: 0.001824  [25200/170046]\n",
            "loss: 0.001811  [27720/170046]\n",
            "loss: 0.001202  [30240/170046]\n",
            "loss: 0.000935  [32760/170046]\n",
            "loss: 0.000978  [35280/170046]\n",
            "loss: 0.001101  [37800/170046]\n",
            "loss: 0.001357  [40320/170046]\n",
            "loss: 0.001955  [42840/170046]\n",
            "loss: 0.000924  [45360/170046]\n",
            "loss: 0.001186  [47880/170046]\n",
            "loss: 0.001116  [50400/170046]\n",
            "loss: 0.001095  [52920/170046]\n",
            "loss: 0.001332  [55440/170046]\n",
            "loss: 0.001154  [57960/170046]\n",
            "loss: 0.001419  [60480/170046]\n",
            "loss: 0.000997  [63000/170046]\n",
            "loss: 0.001226  [65520/170046]\n",
            "loss: 0.001331  [68040/170046]\n",
            "loss: 0.001078  [70560/170046]\n",
            "loss: 0.001052  [73080/170046]\n",
            "loss: 0.001365  [75600/170046]\n",
            "loss: 0.000925  [78120/170046]\n",
            "loss: 0.000989  [80640/170046]\n",
            "loss: 0.001462  [83160/170046]\n",
            "loss: 0.002249  [85680/170046]\n",
            "loss: 0.001234  [88200/170046]\n",
            "loss: 0.001278  [90720/170046]\n",
            "loss: 0.001685  [93240/170046]\n",
            "loss: 0.001775  [95760/170046]\n",
            "loss: 0.002134  [98280/170046]\n",
            "loss: 0.000982  [100800/170046]\n",
            "loss: 0.000791  [103320/170046]\n",
            "loss: 0.001173  [105840/170046]\n",
            "loss: 0.001309  [108360/170046]\n",
            "loss: 0.000937  [110880/170046]\n",
            "loss: 0.001005  [113400/170046]\n",
            "loss: 0.001205  [115920/170046]\n",
            "loss: 0.001247  [118440/170046]\n",
            "loss: 0.000904  [120960/170046]\n",
            "loss: 0.001883  [123480/170046]\n",
            "loss: 0.001002  [126000/170046]\n",
            "loss: 0.001049  [128520/170046]\n",
            "loss: 0.001138  [131040/170046]\n",
            "loss: 0.001193  [133560/170046]\n",
            "loss: 0.000929  [136080/170046]\n",
            "loss: 0.000948  [138600/170046]\n",
            "loss: 0.001007  [141120/170046]\n",
            "loss: 0.001250  [143640/170046]\n",
            "loss: 0.000902  [146160/170046]\n",
            "loss: 0.001304  [148680/170046]\n",
            "loss: 0.001295  [151200/170046]\n",
            "loss: 0.001136  [153720/170046]\n",
            "loss: 0.001018  [156240/170046]\n",
            "loss: 0.001054  [158760/170046]\n",
            "loss: 0.002159  [161280/170046]\n",
            "loss: 0.000768  [163800/170046]\n",
            "loss: 0.000965  [166320/170046]\n",
            "loss: 0.001983  [168840/170046]\n",
            "distortion gain: avg MSE: 0.001054, avg abs error: 0.0252\n",
            "learning rate: 0.000910 -> 0.000820\n",
            "---------------------------\n",
            "\n",
            "Epoch 3\n",
            "loss: 0.001372  [  0/170046]\n",
            "loss: 0.002010  [2520/170046]\n",
            "loss: 0.001139  [5040/170046]\n",
            "loss: 0.001222  [7560/170046]\n",
            "loss: 0.000808  [10080/170046]\n",
            "loss: 0.000964  [12600/170046]\n",
            "loss: 0.001328  [15120/170046]\n",
            "loss: 0.001227  [17640/170046]\n",
            "loss: 0.001106  [20160/170046]\n",
            "loss: 0.001649  [22680/170046]\n",
            "loss: 0.001037  [25200/170046]\n",
            "loss: 0.001026  [27720/170046]\n",
            "loss: 0.000899  [30240/170046]\n",
            "loss: 0.000887  [32760/170046]\n",
            "loss: 0.000802  [35280/170046]\n",
            "loss: 0.001177  [37800/170046]\n",
            "loss: 0.000983  [40320/170046]\n",
            "loss: 0.000814  [42840/170046]\n",
            "loss: 0.001130  [45360/170046]\n",
            "loss: 0.000875  [47880/170046]\n",
            "loss: 0.001099  [50400/170046]\n",
            "loss: 0.001025  [52920/170046]\n",
            "loss: 0.001065  [55440/170046]\n",
            "loss: 0.000929  [57960/170046]\n",
            "loss: 0.001127  [60480/170046]\n",
            "loss: 0.000950  [63000/170046]\n",
            "loss: 0.001239  [65520/170046]\n",
            "loss: 0.000877  [68040/170046]\n",
            "loss: 0.001118  [70560/170046]\n",
            "loss: 0.000953  [73080/170046]\n",
            "loss: 0.002131  [75600/170046]\n",
            "loss: 0.001395  [78120/170046]\n",
            "loss: 0.000984  [80640/170046]\n",
            "loss: 0.000885  [83160/170046]\n",
            "loss: 0.001218  [85680/170046]\n",
            "loss: 0.001027  [88200/170046]\n",
            "loss: 0.001159  [90720/170046]\n",
            "loss: 0.000848  [93240/170046]\n",
            "loss: 0.001121  [95760/170046]\n",
            "loss: 0.001121  [98280/170046]\n",
            "loss: 0.001307  [100800/170046]\n",
            "loss: 0.000851  [103320/170046]\n",
            "loss: 0.000937  [105840/170046]\n",
            "loss: 0.001175  [108360/170046]\n",
            "loss: 0.001277  [110880/170046]\n",
            "loss: 0.000806  [113400/170046]\n",
            "loss: 0.001319  [115920/170046]\n",
            "loss: 0.000759  [118440/170046]\n",
            "loss: 0.001094  [120960/170046]\n",
            "loss: 0.000934  [123480/170046]\n",
            "loss: 0.000731  [126000/170046]\n",
            "loss: 0.001033  [128520/170046]\n",
            "loss: 0.000977  [131040/170046]\n",
            "loss: 0.000912  [133560/170046]\n",
            "loss: 0.001048  [136080/170046]\n",
            "loss: 0.000788  [138600/170046]\n",
            "loss: 0.001052  [141120/170046]\n",
            "loss: 0.001032  [143640/170046]\n",
            "loss: 0.000826  [146160/170046]\n",
            "loss: 0.001337  [148680/170046]\n",
            "loss: 0.001168  [151200/170046]\n",
            "loss: 0.001780  [153720/170046]\n",
            "loss: 0.000844  [156240/170046]\n",
            "loss: 0.000876  [158760/170046]\n",
            "loss: 0.000849  [161280/170046]\n",
            "loss: 0.000801  [163800/170046]\n",
            "loss: 0.001427  [166320/170046]\n",
            "loss: 0.000760  [168840/170046]\n",
            "distortion gain: avg MSE: 0.000841, avg abs error: 0.0225\n",
            "learning rate: 0.000820 -> 0.000730\n",
            "---------------------------\n",
            "\n",
            "Epoch 4\n",
            "loss: 0.000977  [  0/170046]\n",
            "loss: 0.000817  [2520/170046]\n",
            "loss: 0.000669  [5040/170046]\n",
            "loss: 0.000995  [7560/170046]\n",
            "loss: 0.001236  [10080/170046]\n",
            "loss: 0.000771  [12600/170046]\n",
            "loss: 0.000590  [15120/170046]\n",
            "loss: 0.000741  [17640/170046]\n",
            "loss: 0.000676  [20160/170046]\n",
            "loss: 0.000901  [22680/170046]\n",
            "loss: 0.000915  [25200/170046]\n",
            "loss: 0.000753  [27720/170046]\n",
            "loss: 0.000708  [30240/170046]\n",
            "loss: 0.000931  [32760/170046]\n",
            "loss: 0.000939  [35280/170046]\n",
            "loss: 0.001059  [37800/170046]\n",
            "loss: 0.000844  [40320/170046]\n",
            "loss: 0.001030  [42840/170046]\n",
            "loss: 0.001011  [45360/170046]\n",
            "loss: 0.001046  [47880/170046]\n",
            "loss: 0.000750  [50400/170046]\n",
            "loss: 0.000877  [52920/170046]\n",
            "loss: 0.000646  [55440/170046]\n",
            "loss: 0.000786  [57960/170046]\n",
            "loss: 0.000817  [60480/170046]\n",
            "loss: 0.000638  [63000/170046]\n",
            "loss: 0.000636  [65520/170046]\n",
            "loss: 0.001141  [68040/170046]\n",
            "loss: 0.000864  [70560/170046]\n",
            "loss: 0.001078  [73080/170046]\n",
            "loss: 0.000925  [75600/170046]\n",
            "loss: 0.000662  [78120/170046]\n",
            "loss: 0.000633  [80640/170046]\n",
            "loss: 0.001265  [83160/170046]\n",
            "loss: 0.000842  [85680/170046]\n",
            "loss: 0.000766  [88200/170046]\n",
            "loss: 0.000731  [90720/170046]\n",
            "loss: 0.001230  [93240/170046]\n",
            "loss: 0.000769  [95760/170046]\n",
            "loss: 0.000690  [98280/170046]\n",
            "loss: 0.000867  [100800/170046]\n",
            "loss: 0.000938  [103320/170046]\n",
            "loss: 0.000787  [105840/170046]\n",
            "loss: 0.000663  [108360/170046]\n",
            "loss: 0.000889  [110880/170046]\n",
            "loss: 0.000860  [113400/170046]\n",
            "loss: 0.001278  [115920/170046]\n",
            "loss: 0.001618  [118440/170046]\n",
            "loss: 0.001988  [120960/170046]\n",
            "loss: 0.000941  [123480/170046]\n",
            "loss: 0.001104  [126000/170046]\n",
            "loss: 0.000900  [128520/170046]\n",
            "loss: 0.000679  [131040/170046]\n",
            "loss: 0.000929  [133560/170046]\n",
            "loss: 0.000766  [136080/170046]\n",
            "loss: 0.000557  [138600/170046]\n",
            "loss: 0.001273  [141120/170046]\n",
            "loss: 0.000568  [143640/170046]\n",
            "loss: 0.000722  [146160/170046]\n",
            "loss: 0.000742  [148680/170046]\n",
            "loss: 0.000719  [151200/170046]\n",
            "loss: 0.000752  [153720/170046]\n",
            "loss: 0.000561  [156240/170046]\n",
            "loss: 0.001475  [158760/170046]\n",
            "loss: 0.001094  [161280/170046]\n",
            "loss: 0.000874  [163800/170046]\n",
            "loss: 0.000848  [166320/170046]\n",
            "loss: 0.000706  [168840/170046]\n",
            "distortion gain: avg MSE: 0.000767, avg abs error: 0.0215\n",
            "learning rate: 0.000730 -> 0.000640\n",
            "---------------------------\n",
            "\n",
            "Epoch 5\n",
            "loss: 0.000584  [  0/170046]\n",
            "loss: 0.000829  [2520/170046]\n",
            "loss: 0.000691  [5040/170046]\n",
            "loss: 0.000761  [7560/170046]\n",
            "loss: 0.000746  [10080/170046]\n",
            "loss: 0.000892  [12600/170046]\n",
            "loss: 0.000885  [15120/170046]\n",
            "loss: 0.000768  [17640/170046]\n",
            "loss: 0.000689  [20160/170046]\n",
            "loss: 0.000678  [22680/170046]\n",
            "loss: 0.001023  [25200/170046]\n",
            "loss: 0.000595  [27720/170046]\n",
            "loss: 0.000673  [30240/170046]\n",
            "loss: 0.000786  [32760/170046]\n",
            "loss: 0.000755  [35280/170046]\n",
            "loss: 0.001075  [37800/170046]\n",
            "loss: 0.000773  [40320/170046]\n",
            "loss: 0.000567  [42840/170046]\n",
            "loss: 0.000912  [45360/170046]\n",
            "loss: 0.000926  [47880/170046]\n",
            "loss: 0.000819  [50400/170046]\n",
            "loss: 0.000643  [52920/170046]\n",
            "loss: 0.000942  [55440/170046]\n",
            "loss: 0.000735  [57960/170046]\n",
            "loss: 0.000930  [60480/170046]\n",
            "loss: 0.000742  [63000/170046]\n",
            "loss: 0.000806  [65520/170046]\n",
            "loss: 0.000687  [68040/170046]\n",
            "loss: 0.000742  [70560/170046]\n",
            "loss: 0.000779  [73080/170046]\n",
            "loss: 0.000695  [75600/170046]\n",
            "loss: 0.000633  [78120/170046]\n",
            "loss: 0.000680  [80640/170046]\n",
            "loss: 0.000794  [83160/170046]\n",
            "loss: 0.000586  [85680/170046]\n",
            "loss: 0.000661  [88200/170046]\n",
            "loss: 0.001095  [90720/170046]\n",
            "loss: 0.000770  [93240/170046]\n",
            "loss: 0.000892  [95760/170046]\n",
            "loss: 0.000998  [98280/170046]\n",
            "loss: 0.000777  [100800/170046]\n",
            "loss: 0.001051  [103320/170046]\n",
            "loss: 0.000683  [105840/170046]\n",
            "loss: 0.000719  [108360/170046]\n",
            "loss: 0.000887  [110880/170046]\n",
            "loss: 0.000709  [113400/170046]\n",
            "loss: 0.000840  [115920/170046]\n",
            "loss: 0.000723  [118440/170046]\n",
            "loss: 0.000869  [120960/170046]\n",
            "loss: 0.000543  [123480/170046]\n",
            "loss: 0.001077  [126000/170046]\n",
            "loss: 0.000642  [128520/170046]\n",
            "loss: 0.000766  [131040/170046]\n",
            "loss: 0.001270  [133560/170046]\n",
            "loss: 0.000793  [136080/170046]\n",
            "loss: 0.000557  [138600/170046]\n",
            "loss: 0.000699  [141120/170046]\n",
            "loss: 0.000746  [143640/170046]\n",
            "loss: 0.000872  [146160/170046]\n",
            "loss: 0.001054  [148680/170046]\n",
            "loss: 0.000593  [151200/170046]\n",
            "loss: 0.000679  [153720/170046]\n",
            "loss: 0.000538  [156240/170046]\n",
            "loss: 0.000790  [158760/170046]\n",
            "loss: 0.000677  [161280/170046]\n",
            "loss: 0.000823  [163800/170046]\n",
            "loss: 0.001048  [166320/170046]\n",
            "loss: 0.000724  [168840/170046]\n",
            "distortion gain: avg MSE: 0.000908, avg abs error: 0.0237\n",
            "learning rate: 0.000640 -> 0.000550\n",
            "---------------------------\n",
            "\n",
            "Epoch 6\n",
            "loss: 0.000725  [  0/170046]\n",
            "loss: 0.000530  [2520/170046]\n",
            "loss: 0.000576  [5040/170046]\n",
            "loss: 0.000773  [7560/170046]\n",
            "loss: 0.000668  [10080/170046]\n",
            "loss: 0.000709  [12600/170046]\n",
            "loss: 0.000563  [15120/170046]\n",
            "loss: 0.000613  [17640/170046]\n",
            "loss: 0.001031  [20160/170046]\n",
            "loss: 0.000508  [22680/170046]\n",
            "loss: 0.000523  [25200/170046]\n",
            "loss: 0.000708  [27720/170046]\n",
            "loss: 0.000569  [30240/170046]\n",
            "loss: 0.000712  [32760/170046]\n",
            "loss: 0.000544  [35280/170046]\n",
            "loss: 0.000493  [37800/170046]\n",
            "loss: 0.000633  [40320/170046]\n",
            "loss: 0.000861  [42840/170046]\n",
            "loss: 0.000663  [45360/170046]\n",
            "loss: 0.000711  [47880/170046]\n",
            "loss: 0.000447  [50400/170046]\n",
            "loss: 0.000540  [52920/170046]\n",
            "loss: 0.000566  [55440/170046]\n",
            "loss: 0.000653  [57960/170046]\n",
            "loss: 0.000550  [60480/170046]\n",
            "loss: 0.000689  [63000/170046]\n",
            "loss: 0.000741  [65520/170046]\n",
            "loss: 0.000703  [68040/170046]\n",
            "loss: 0.000644  [70560/170046]\n",
            "loss: 0.000610  [73080/170046]\n",
            "loss: 0.000672  [75600/170046]\n",
            "loss: 0.000443  [78120/170046]\n",
            "loss: 0.000507  [80640/170046]\n",
            "loss: 0.000626  [83160/170046]\n",
            "loss: 0.000633  [85680/170046]\n",
            "loss: 0.000586  [88200/170046]\n",
            "loss: 0.000562  [90720/170046]\n",
            "loss: 0.000648  [93240/170046]\n",
            "loss: 0.000494  [95760/170046]\n",
            "loss: 0.000390  [98280/170046]\n",
            "loss: 0.000661  [100800/170046]\n",
            "loss: 0.000943  [103320/170046]\n",
            "loss: 0.000653  [105840/170046]\n",
            "loss: 0.001014  [108360/170046]\n",
            "loss: 0.000462  [110880/170046]\n",
            "loss: 0.000591  [113400/170046]\n",
            "loss: 0.000645  [115920/170046]\n",
            "loss: 0.000559  [118440/170046]\n",
            "loss: 0.000617  [120960/170046]\n",
            "loss: 0.000463  [123480/170046]\n",
            "loss: 0.000548  [126000/170046]\n",
            "loss: 0.000657  [128520/170046]\n",
            "loss: 0.000612  [131040/170046]\n",
            "loss: 0.000541  [133560/170046]\n",
            "loss: 0.000763  [136080/170046]\n",
            "loss: 0.000693  [138600/170046]\n",
            "loss: 0.000536  [141120/170046]\n",
            "loss: 0.001098  [143640/170046]\n",
            "loss: 0.000787  [146160/170046]\n",
            "loss: 0.000812  [148680/170046]\n",
            "loss: 0.000533  [151200/170046]\n",
            "loss: 0.000485  [153720/170046]\n",
            "loss: 0.000709  [156240/170046]\n",
            "loss: 0.000427  [158760/170046]\n",
            "loss: 0.000520  [161280/170046]\n",
            "loss: 0.000478  [163800/170046]\n",
            "loss: 0.000528  [166320/170046]\n",
            "loss: 0.000468  [168840/170046]\n",
            "distortion gain: avg MSE: 0.000648, avg abs error: 0.0198\n",
            "learning rate: 0.000550 -> 0.000460\n",
            "---------------------------\n",
            "\n",
            "Epoch 7\n",
            "loss: 0.000585  [  0/170046]\n",
            "loss: 0.000529  [2520/170046]\n",
            "loss: 0.000570  [5040/170046]\n",
            "loss: 0.000466  [7560/170046]\n",
            "loss: 0.000460  [10080/170046]\n",
            "loss: 0.000466  [12600/170046]\n",
            "loss: 0.000555  [15120/170046]\n",
            "loss: 0.000532  [17640/170046]\n",
            "loss: 0.000855  [20160/170046]\n",
            "loss: 0.000523  [22680/170046]\n",
            "loss: 0.000661  [25200/170046]\n",
            "loss: 0.000561  [27720/170046]\n",
            "loss: 0.000433  [30240/170046]\n",
            "loss: 0.000501  [32760/170046]\n",
            "loss: 0.000641  [35280/170046]\n",
            "loss: 0.000448  [37800/170046]\n",
            "loss: 0.000572  [40320/170046]\n",
            "loss: 0.000569  [42840/170046]\n",
            "loss: 0.000542  [45360/170046]\n",
            "loss: 0.000483  [47880/170046]\n",
            "loss: 0.000505  [50400/170046]\n",
            "loss: 0.000431  [52920/170046]\n",
            "loss: 0.000562  [55440/170046]\n",
            "loss: 0.000444  [57960/170046]\n",
            "loss: 0.000565  [60480/170046]\n",
            "loss: 0.000499  [63000/170046]\n",
            "loss: 0.000544  [65520/170046]\n",
            "loss: 0.000543  [68040/170046]\n",
            "loss: 0.000699  [70560/170046]\n",
            "loss: 0.000670  [73080/170046]\n",
            "loss: 0.000644  [75600/170046]\n",
            "loss: 0.000573  [78120/170046]\n",
            "loss: 0.000396  [80640/170046]\n",
            "loss: 0.000481  [83160/170046]\n",
            "loss: 0.000499  [85680/170046]\n",
            "loss: 0.000635  [88200/170046]\n",
            "loss: 0.000556  [90720/170046]\n",
            "loss: 0.000782  [93240/170046]\n",
            "loss: 0.000378  [95760/170046]\n",
            "loss: 0.000620  [98280/170046]\n",
            "loss: 0.000492  [100800/170046]\n",
            "loss: 0.000288  [103320/170046]\n",
            "loss: 0.000735  [105840/170046]\n",
            "loss: 0.000800  [108360/170046]\n",
            "loss: 0.000506  [110880/170046]\n",
            "loss: 0.000425  [113400/170046]\n",
            "loss: 0.000600  [115920/170046]\n",
            "loss: 0.000443  [118440/170046]\n",
            "loss: 0.000476  [120960/170046]\n",
            "loss: 0.000430  [123480/170046]\n",
            "loss: 0.000600  [126000/170046]\n",
            "loss: 0.000576  [128520/170046]\n",
            "loss: 0.000368  [131040/170046]\n",
            "loss: 0.000687  [133560/170046]\n",
            "loss: 0.000514  [136080/170046]\n",
            "loss: 0.000523  [138600/170046]\n",
            "loss: 0.000569  [141120/170046]\n",
            "loss: 0.000778  [143640/170046]\n",
            "loss: 0.000724  [146160/170046]\n",
            "loss: 0.000662  [148680/170046]\n",
            "loss: 0.000517  [151200/170046]\n",
            "loss: 0.000544  [153720/170046]\n",
            "loss: 0.000376  [156240/170046]\n",
            "loss: 0.000547  [158760/170046]\n",
            "loss: 0.000430  [161280/170046]\n",
            "loss: 0.000581  [163800/170046]\n",
            "loss: 0.000333  [166320/170046]\n",
            "loss: 0.000751  [168840/170046]\n",
            "distortion gain: avg MSE: 0.000725, avg abs error: 0.0211\n",
            "learning rate: 0.000460 -> 0.000370\n",
            "---------------------------\n",
            "\n",
            "Epoch 8\n",
            "loss: 0.001133  [  0/170046]\n",
            "loss: 0.000359  [2520/170046]\n",
            "loss: 0.000457  [5040/170046]\n",
            "loss: 0.000485  [7560/170046]\n",
            "loss: 0.000437  [10080/170046]\n",
            "loss: 0.000495  [12600/170046]\n",
            "loss: 0.000399  [15120/170046]\n",
            "loss: 0.000385  [17640/170046]\n",
            "loss: 0.000384  [20160/170046]\n",
            "loss: 0.000385  [22680/170046]\n",
            "loss: 0.000564  [25200/170046]\n",
            "loss: 0.000507  [27720/170046]\n",
            "loss: 0.000409  [30240/170046]\n",
            "loss: 0.000551  [32760/170046]\n",
            "loss: 0.000670  [35280/170046]\n",
            "loss: 0.000515  [37800/170046]\n",
            "loss: 0.000471  [40320/170046]\n",
            "loss: 0.000296  [42840/170046]\n",
            "loss: 0.000547  [45360/170046]\n",
            "loss: 0.000531  [47880/170046]\n",
            "loss: 0.000637  [50400/170046]\n",
            "loss: 0.000374  [52920/170046]\n",
            "loss: 0.000556  [55440/170046]\n",
            "loss: 0.000587  [57960/170046]\n",
            "loss: 0.000492  [60480/170046]\n",
            "loss: 0.000507  [63000/170046]\n",
            "loss: 0.000530  [65520/170046]\n",
            "loss: 0.000363  [68040/170046]\n",
            "loss: 0.000400  [70560/170046]\n",
            "loss: 0.000462  [73080/170046]\n",
            "loss: 0.000604  [75600/170046]\n",
            "loss: 0.000896  [78120/170046]\n",
            "loss: 0.000402  [80640/170046]\n",
            "loss: 0.000342  [83160/170046]\n",
            "loss: 0.000398  [85680/170046]\n",
            "loss: 0.000790  [88200/170046]\n",
            "loss: 0.000569  [90720/170046]\n",
            "loss: 0.000431  [93240/170046]\n",
            "loss: 0.000505  [95760/170046]\n",
            "loss: 0.000444  [98280/170046]\n",
            "loss: 0.000397  [100800/170046]\n",
            "loss: 0.000651  [103320/170046]\n",
            "loss: 0.000504  [105840/170046]\n",
            "loss: 0.000623  [108360/170046]\n",
            "loss: 0.000502  [110880/170046]\n",
            "loss: 0.000533  [113400/170046]\n",
            "loss: 0.000485  [115920/170046]\n",
            "loss: 0.000535  [118440/170046]\n",
            "loss: 0.000600  [120960/170046]\n",
            "loss: 0.000458  [123480/170046]\n",
            "loss: 0.000384  [126000/170046]\n",
            "loss: 0.000436  [128520/170046]\n",
            "loss: 0.000374  [131040/170046]\n",
            "loss: 0.000565  [133560/170046]\n",
            "loss: 0.000587  [136080/170046]\n",
            "loss: 0.000461  [138600/170046]\n",
            "loss: 0.000523  [141120/170046]\n",
            "loss: 0.000394  [143640/170046]\n",
            "loss: 0.000506  [146160/170046]\n",
            "loss: 0.000596  [148680/170046]\n",
            "loss: 0.000425  [151200/170046]\n",
            "loss: 0.000507  [153720/170046]\n",
            "loss: 0.000406  [156240/170046]\n",
            "loss: 0.000473  [158760/170046]\n",
            "loss: 0.000510  [161280/170046]\n",
            "loss: 0.000380  [163800/170046]\n",
            "loss: 0.000543  [166320/170046]\n",
            "loss: 0.000546  [168840/170046]\n",
            "distortion gain: avg MSE: 0.000495, avg abs error: 0.0171\n",
            "learning rate: 0.000370 -> 0.000280\n",
            "---------------------------\n",
            "\n",
            "Epoch 9\n",
            "loss: 0.000389  [  0/170046]\n",
            "loss: 0.000375  [2520/170046]\n",
            "loss: 0.000369  [5040/170046]\n",
            "loss: 0.000434  [7560/170046]\n",
            "loss: 0.000437  [10080/170046]\n",
            "loss: 0.000421  [12600/170046]\n",
            "loss: 0.000286  [15120/170046]\n",
            "loss: 0.000503  [17640/170046]\n",
            "loss: 0.000426  [20160/170046]\n",
            "loss: 0.000461  [22680/170046]\n",
            "loss: 0.000402  [25200/170046]\n",
            "loss: 0.000388  [27720/170046]\n",
            "loss: 0.000393  [30240/170046]\n",
            "loss: 0.000461  [32760/170046]\n",
            "loss: 0.000411  [35280/170046]\n",
            "loss: 0.000386  [37800/170046]\n",
            "loss: 0.000312  [40320/170046]\n",
            "loss: 0.000450  [42840/170046]\n",
            "loss: 0.000458  [45360/170046]\n",
            "loss: 0.000389  [47880/170046]\n",
            "loss: 0.000397  [50400/170046]\n",
            "loss: 0.000402  [52920/170046]\n",
            "loss: 0.000369  [55440/170046]\n",
            "loss: 0.000394  [57960/170046]\n",
            "loss: 0.000377  [60480/170046]\n",
            "loss: 0.000505  [63000/170046]\n",
            "loss: 0.000511  [65520/170046]\n",
            "loss: 0.000395  [68040/170046]\n",
            "loss: 0.000437  [70560/170046]\n",
            "loss: 0.000308  [73080/170046]\n",
            "loss: 0.000323  [75600/170046]\n",
            "loss: 0.000464  [78120/170046]\n",
            "loss: 0.000418  [80640/170046]\n",
            "loss: 0.000354  [83160/170046]\n",
            "loss: 0.000375  [85680/170046]\n",
            "loss: 0.000395  [88200/170046]\n",
            "loss: 0.000425  [90720/170046]\n",
            "loss: 0.000402  [93240/170046]\n",
            "loss: 0.000411  [95760/170046]\n",
            "loss: 0.000460  [98280/170046]\n",
            "loss: 0.000424  [100800/170046]\n",
            "loss: 0.000459  [103320/170046]\n",
            "loss: 0.000414  [105840/170046]\n",
            "loss: 0.000548  [108360/170046]\n",
            "loss: 0.000516  [110880/170046]\n",
            "loss: 0.000477  [113400/170046]\n",
            "loss: 0.000409  [115920/170046]\n",
            "loss: 0.000411  [118440/170046]\n",
            "loss: 0.000731  [120960/170046]\n",
            "loss: 0.000430  [123480/170046]\n",
            "loss: 0.000450  [126000/170046]\n",
            "loss: 0.000302  [128520/170046]\n",
            "loss: 0.000519  [131040/170046]\n",
            "loss: 0.000408  [133560/170046]\n",
            "loss: 0.000502  [136080/170046]\n",
            "loss: 0.000341  [138600/170046]\n",
            "loss: 0.000373  [141120/170046]\n",
            "loss: 0.000385  [143640/170046]\n",
            "loss: 0.000529  [146160/170046]\n",
            "loss: 0.000398  [148680/170046]\n",
            "loss: 0.000363  [151200/170046]\n",
            "loss: 0.000377  [153720/170046]\n",
            "loss: 0.000511  [156240/170046]\n",
            "loss: 0.000394  [158760/170046]\n",
            "loss: 0.000388  [161280/170046]\n",
            "loss: 0.000345  [163800/170046]\n",
            "loss: 0.000423  [166320/170046]\n",
            "loss: 0.000379  [168840/170046]\n",
            "distortion gain: avg MSE: 0.000449, avg abs error: 0.0162\n",
            "learning rate: 0.000280 -> 0.000190\n",
            "---------------------------\n",
            "\n",
            "Epoch 10\n",
            "loss: 0.000452  [  0/170046]\n",
            "loss: 0.000379  [2520/170046]\n",
            "loss: 0.000342  [5040/170046]\n",
            "loss: 0.000368  [7560/170046]\n",
            "loss: 0.000346  [10080/170046]\n",
            "loss: 0.000410  [12600/170046]\n",
            "loss: 0.000325  [15120/170046]\n",
            "loss: 0.000331  [17640/170046]\n",
            "loss: 0.000264  [20160/170046]\n",
            "loss: 0.000364  [22680/170046]\n",
            "loss: 0.000474  [25200/170046]\n",
            "loss: 0.000480  [27720/170046]\n",
            "loss: 0.000346  [30240/170046]\n",
            "loss: 0.000279  [32760/170046]\n",
            "loss: 0.000345  [35280/170046]\n",
            "loss: 0.000323  [37800/170046]\n",
            "loss: 0.000326  [40320/170046]\n",
            "loss: 0.000380  [42840/170046]\n",
            "loss: 0.000298  [45360/170046]\n",
            "loss: 0.000340  [47880/170046]\n",
            "loss: 0.000386  [50400/170046]\n",
            "loss: 0.000349  [52920/170046]\n",
            "loss: 0.000398  [55440/170046]\n",
            "loss: 0.000316  [57960/170046]\n",
            "loss: 0.000396  [60480/170046]\n",
            "loss: 0.000289  [63000/170046]\n",
            "loss: 0.000411  [65520/170046]\n",
            "loss: 0.000336  [68040/170046]\n",
            "loss: 0.000334  [70560/170046]\n",
            "loss: 0.000331  [73080/170046]\n",
            "loss: 0.000307  [75600/170046]\n",
            "loss: 0.000484  [78120/170046]\n",
            "loss: 0.000332  [80640/170046]\n",
            "loss: 0.000435  [83160/170046]\n",
            "loss: 0.000343  [85680/170046]\n",
            "loss: 0.000459  [88200/170046]\n",
            "loss: 0.000354  [90720/170046]\n",
            "loss: 0.000331  [93240/170046]\n",
            "loss: 0.000508  [95760/170046]\n",
            "loss: 0.000341  [98280/170046]\n",
            "loss: 0.000381  [100800/170046]\n",
            "loss: 0.000330  [103320/170046]\n",
            "loss: 0.000377  [105840/170046]\n",
            "loss: 0.000409  [108360/170046]\n",
            "loss: 0.000379  [110880/170046]\n",
            "loss: 0.000414  [113400/170046]\n",
            "loss: 0.000405  [115920/170046]\n",
            "loss: 0.000391  [118440/170046]\n",
            "loss: 0.000461  [120960/170046]\n",
            "loss: 0.000392  [123480/170046]\n",
            "loss: 0.000317  [126000/170046]\n",
            "loss: 0.000301  [128520/170046]\n",
            "loss: 0.000452  [131040/170046]\n",
            "loss: 0.000363  [133560/170046]\n",
            "loss: 0.000386  [136080/170046]\n",
            "loss: 0.000373  [138600/170046]\n",
            "loss: 0.000432  [141120/170046]\n",
            "loss: 0.000417  [143640/170046]\n",
            "loss: 0.000328  [146160/170046]\n",
            "loss: 0.000389  [148680/170046]\n",
            "loss: 0.000321  [151200/170046]\n",
            "loss: 0.000337  [153720/170046]\n",
            "loss: 0.000410  [156240/170046]\n",
            "loss: 0.000412  [158760/170046]\n",
            "loss: 0.000365  [161280/170046]\n",
            "loss: 0.000257  [163800/170046]\n",
            "loss: 0.000486  [166320/170046]\n",
            "loss: 0.000291  [168840/170046]\n",
            "distortion gain: avg MSE: 0.000401, avg abs error: 0.0153\n",
            "learning rate: 0.000190 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 11\n",
            "loss: 0.000276  [  0/170046]\n",
            "loss: 0.000277  [2520/170046]\n",
            "loss: 0.000274  [5040/170046]\n",
            "loss: 0.000286  [7560/170046]\n",
            "loss: 0.000311  [10080/170046]\n",
            "loss: 0.000375  [12600/170046]\n",
            "loss: 0.000344  [15120/170046]\n",
            "loss: 0.000248  [17640/170046]\n",
            "loss: 0.000457  [20160/170046]\n",
            "loss: 0.000488  [22680/170046]\n",
            "loss: 0.000280  [25200/170046]\n",
            "loss: 0.000318  [27720/170046]\n",
            "loss: 0.000367  [30240/170046]\n",
            "loss: 0.000332  [32760/170046]\n",
            "loss: 0.000352  [35280/170046]\n",
            "loss: 0.000287  [37800/170046]\n",
            "loss: 0.000350  [40320/170046]\n",
            "loss: 0.000450  [42840/170046]\n",
            "loss: 0.000371  [45360/170046]\n",
            "loss: 0.000307  [47880/170046]\n",
            "loss: 0.000331  [50400/170046]\n",
            "loss: 0.000371  [52920/170046]\n",
            "loss: 0.000326  [55440/170046]\n",
            "loss: 0.000242  [57960/170046]\n",
            "loss: 0.000343  [60480/170046]\n",
            "loss: 0.000290  [63000/170046]\n",
            "loss: 0.000263  [65520/170046]\n",
            "loss: 0.000350  [68040/170046]\n",
            "loss: 0.000350  [70560/170046]\n",
            "loss: 0.000333  [73080/170046]\n",
            "loss: 0.000380  [75600/170046]\n",
            "loss: 0.000327  [78120/170046]\n",
            "loss: 0.000320  [80640/170046]\n",
            "loss: 0.000330  [83160/170046]\n",
            "loss: 0.000286  [85680/170046]\n",
            "loss: 0.000264  [88200/170046]\n",
            "loss: 0.000303  [90720/170046]\n",
            "loss: 0.000297  [93240/170046]\n",
            "loss: 0.000342  [95760/170046]\n",
            "loss: 0.000391  [98280/170046]\n",
            "loss: 0.000328  [100800/170046]\n",
            "loss: 0.000244  [103320/170046]\n",
            "loss: 0.000348  [105840/170046]\n",
            "loss: 0.000371  [108360/170046]\n",
            "loss: 0.000262  [110880/170046]\n",
            "loss: 0.000252  [113400/170046]\n",
            "loss: 0.000438  [115920/170046]\n",
            "loss: 0.000364  [118440/170046]\n",
            "loss: 0.000336  [120960/170046]\n",
            "loss: 0.000327  [123480/170046]\n",
            "loss: 0.000288  [126000/170046]\n",
            "loss: 0.000417  [128520/170046]\n",
            "loss: 0.000259  [131040/170046]\n",
            "loss: 0.000305  [133560/170046]\n",
            "loss: 0.000333  [136080/170046]\n",
            "loss: 0.000332  [138600/170046]\n",
            "loss: 0.000239  [141120/170046]\n",
            "loss: 0.000321  [143640/170046]\n",
            "loss: 0.000250  [146160/170046]\n",
            "loss: 0.000381  [148680/170046]\n",
            "loss: 0.000338  [151200/170046]\n",
            "loss: 0.000372  [153720/170046]\n",
            "loss: 0.000336  [156240/170046]\n",
            "loss: 0.000276  [158760/170046]\n",
            "loss: 0.000251  [161280/170046]\n",
            "loss: 0.000370  [163800/170046]\n",
            "loss: 0.000238  [166320/170046]\n",
            "loss: 0.000301  [168840/170046]\n",
            "distortion gain: avg MSE: 0.000383, avg abs error: 0.0149\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 12\n",
            "loss: 0.000349  [  0/170046]\n",
            "loss: 0.000334  [2520/170046]\n",
            "loss: 0.000268  [5040/170046]\n",
            "loss: 0.000256  [7560/170046]\n",
            "loss: 0.000385  [10080/170046]\n",
            "loss: 0.000397  [12600/170046]\n",
            "loss: 0.000254  [15120/170046]\n",
            "loss: 0.000300  [17640/170046]\n",
            "loss: 0.000361  [20160/170046]\n",
            "loss: 0.000291  [22680/170046]\n",
            "loss: 0.000309  [25200/170046]\n",
            "loss: 0.000343  [27720/170046]\n",
            "loss: 0.000243  [30240/170046]\n",
            "loss: 0.000259  [32760/170046]\n",
            "loss: 0.000211  [35280/170046]\n",
            "loss: 0.000273  [37800/170046]\n",
            "loss: 0.000301  [40320/170046]\n",
            "loss: 0.000337  [42840/170046]\n",
            "loss: 0.000319  [45360/170046]\n",
            "loss: 0.000293  [47880/170046]\n",
            "loss: 0.000345  [50400/170046]\n",
            "loss: 0.000314  [52920/170046]\n",
            "loss: 0.000249  [55440/170046]\n",
            "loss: 0.000328  [57960/170046]\n",
            "loss: 0.000319  [60480/170046]\n",
            "loss: 0.000290  [63000/170046]\n",
            "loss: 0.000319  [65520/170046]\n",
            "loss: 0.000289  [68040/170046]\n",
            "loss: 0.000323  [70560/170046]\n",
            "loss: 0.000348  [73080/170046]\n",
            "loss: 0.000294  [75600/170046]\n",
            "loss: 0.000297  [78120/170046]\n",
            "loss: 0.000359  [80640/170046]\n",
            "loss: 0.000327  [83160/170046]\n",
            "loss: 0.000334  [85680/170046]\n",
            "loss: 0.000300  [88200/170046]\n",
            "loss: 0.000378  [90720/170046]\n",
            "loss: 0.000259  [93240/170046]\n",
            "loss: 0.000344  [95760/170046]\n",
            "loss: 0.000300  [98280/170046]\n",
            "loss: 0.000375  [100800/170046]\n",
            "loss: 0.000387  [103320/170046]\n",
            "loss: 0.000240  [105840/170046]\n",
            "loss: 0.000373  [108360/170046]\n",
            "loss: 0.000361  [110880/170046]\n",
            "loss: 0.000378  [113400/170046]\n",
            "loss: 0.000382  [115920/170046]\n",
            "loss: 0.000344  [118440/170046]\n",
            "loss: 0.000305  [120960/170046]\n",
            "loss: 0.000431  [123480/170046]\n",
            "loss: 0.000284  [126000/170046]\n",
            "loss: 0.000324  [128520/170046]\n",
            "loss: 0.000298  [131040/170046]\n",
            "loss: 0.000265  [133560/170046]\n",
            "loss: 0.000270  [136080/170046]\n",
            "loss: 0.000272  [138600/170046]\n",
            "loss: 0.000414  [141120/170046]\n",
            "loss: 0.000342  [143640/170046]\n",
            "loss: 0.000300  [146160/170046]\n",
            "loss: 0.000309  [148680/170046]\n",
            "loss: 0.000233  [151200/170046]\n",
            "loss: 0.000274  [153720/170046]\n",
            "loss: 0.000310  [156240/170046]\n",
            "loss: 0.000393  [158760/170046]\n",
            "loss: 0.000307  [161280/170046]\n",
            "loss: 0.000236  [163800/170046]\n",
            "loss: 0.000295  [166320/170046]\n",
            "loss: 0.000443  [168840/170046]\n",
            "distortion gain: avg MSE: 0.000380, avg abs error: 0.0148\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 13\n",
            "loss: 0.000240  [  0/170046]\n",
            "loss: 0.000280  [2520/170046]\n",
            "loss: 0.000273  [5040/170046]\n",
            "loss: 0.000333  [7560/170046]\n",
            "loss: 0.000264  [10080/170046]\n",
            "loss: 0.000200  [12600/170046]\n",
            "loss: 0.000289  [15120/170046]\n",
            "loss: 0.000392  [17640/170046]\n",
            "loss: 0.000217  [20160/170046]\n",
            "loss: 0.000304  [22680/170046]\n",
            "loss: 0.000203  [25200/170046]\n",
            "loss: 0.000258  [27720/170046]\n",
            "loss: 0.000379  [30240/170046]\n",
            "loss: 0.000257  [32760/170046]\n",
            "loss: 0.000264  [35280/170046]\n",
            "loss: 0.000437  [37800/170046]\n",
            "loss: 0.000351  [40320/170046]\n",
            "loss: 0.000282  [42840/170046]\n",
            "loss: 0.000272  [45360/170046]\n",
            "loss: 0.000367  [47880/170046]\n",
            "loss: 0.000357  [50400/170046]\n",
            "loss: 0.000301  [52920/170046]\n",
            "loss: 0.000294  [55440/170046]\n",
            "loss: 0.000312  [57960/170046]\n",
            "loss: 0.000269  [60480/170046]\n",
            "loss: 0.000227  [63000/170046]\n",
            "loss: 0.000334  [65520/170046]\n",
            "loss: 0.000352  [68040/170046]\n",
            "loss: 0.000458  [70560/170046]\n",
            "loss: 0.000344  [73080/170046]\n",
            "loss: 0.000295  [75600/170046]\n",
            "loss: 0.000283  [78120/170046]\n",
            "loss: 0.000374  [80640/170046]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m optimiser \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(cnn\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mLEARNING_RATE)\n\u001b[0;32m     45\u001b[0m \u001b[39m# train model\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m train\u001b[39m.\u001b[39;49mtrain(cnn,\n\u001b[0;32m     47\u001b[0m             train_dataloader,\n\u001b[0;32m     48\u001b[0m             test_dataloader,\n\u001b[0;32m     49\u001b[0m             loss_fn,\n\u001b[0;32m     50\u001b[0m             optimiser,\n\u001b[0;32m     51\u001b[0m             device,\n\u001b[0;32m     52\u001b[0m             log_writer,\n\u001b[0;32m     53\u001b[0m             EPOCHS,\n\u001b[0;32m     54\u001b[0m             WEIGHTS_PATH,\n\u001b[0;32m     55\u001b[0m             effect\u001b[39m=\u001b[39;49mfx)\n\u001b[0;32m     57\u001b[0m _, _, log \u001b[39m=\u001b[39m train\u001b[39m.\u001b[39mtest(cnn, val_dataloader, device, effect\u001b[39m=\u001b[39mfx)\n\u001b[0;32m     58\u001b[0m \u001b[39mfor\u001b[39;00m _, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(log):\n",
            "File \u001b[1;32md:\\final\\Guitar-MIR\\src\\extrector\\train.py:110\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_data_loader, test_data_loader, loss_fn, optimiser, device, writer, epochs, path, effect)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m    109\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 110\u001b[0m     tr_loss, tr_err, _ \u001b[39m=\u001b[39m train_single_epoch(model,\n\u001b[0;32m    111\u001b[0m                                             train_data_loader,\n\u001b[0;32m    112\u001b[0m                                             loss_fn,\n\u001b[0;32m    113\u001b[0m                                             optimiser,\n\u001b[0;32m    114\u001b[0m                                             device,\n\u001b[0;32m    115\u001b[0m                                             effect)\n\u001b[0;32m    116\u001b[0m     ts_loss, ts_err, _ \u001b[39m=\u001b[39m test(model, test_data_loader, device, loss_fn, effect)\n\u001b[0;32m    118\u001b[0m     writer\u001b[39m.\u001b[39madd_scalars(\u001b[39m\"\u001b[39m\u001b[39mLoss/\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m PARA_MAP[effect], {\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m: tr_loss, \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m: ts_loss}, epoch)\n",
            "File \u001b[1;32md:\\final\\Guitar-MIR\\src\\extrector\\train.py:24\u001b[0m, in \u001b[0;36mtrain_single_epoch\u001b[1;34m(model, data_loader, loss_fn, optimiser, device, effect)\u001b[0m\n\u001b[0;32m     20\u001b[0m log \u001b[39m=\u001b[39m []\n\u001b[0;32m     22\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m---> 24\u001b[0m \u001b[39mfor\u001b[39;00m batch, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data_loader):\n\u001b[0;32m     26\u001b[0m     X, _, _, labels, filenames \u001b[39m=\u001b[39m data\n\u001b[0;32m     27\u001b[0m     X \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mto(device)\n",
            "File \u001b[1;32mc:\\Users\\roger\\miniconda3\\envs\\torchaudio\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32mc:\\Users\\roger\\miniconda3\\envs\\torchaudio\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32mc:\\Users\\roger\\miniconda3\\envs\\torchaudio\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\roger\\miniconda3\\envs\\torchaudio\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32mc:\\Users\\roger\\miniconda3\\envs\\torchaudio\\lib\\site-packages\\torch\\utils\\data\\dataset.py:298\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    297\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> 298\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
            "File \u001b[1;32md:\\final\\Guitar-MIR\\src\\gtfxdataset.py:48\u001b[0m, in \u001b[0;36mGtFxDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     46\u001b[0m value_classes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_audio_sample_value_classes(index)\n\u001b[0;32m     47\u001b[0m parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_audio_sample_parameters(index)\n\u001b[1;32m---> 48\u001b[0m signal, sr \u001b[39m=\u001b[39m torchaudio\u001b[39m.\u001b[39;49mload(audio_sample_path)\n\u001b[0;32m     49\u001b[0m signal \u001b[39m=\u001b[39m signal\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m     50\u001b[0m signal \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_resample_if_necessary(signal, sr)\n",
            "File \u001b[1;32mc:\\Users\\roger\\miniconda3\\envs\\torchaudio\\lib\\site-packages\\torchaudio\\backend\\soundfile_backend.py:221\u001b[0m, in \u001b[0;36mload\u001b[1;34m(filepath, frame_offset, num_frames, normalize, channels_first, format)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39m@_requires_soundfile\u001b[39m\n\u001b[0;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[0;32m    141\u001b[0m     filepath: \u001b[39mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[39mformat\u001b[39m: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, \u001b[39mint\u001b[39m]:\n\u001b[0;32m    148\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load audio data from file.\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \n\u001b[0;32m    150\u001b[0m \u001b[39m    Note:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[39m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[0;32m    220\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 221\u001b[0m     \u001b[39mwith\u001b[39;00m soundfile\u001b[39m.\u001b[39;49mSoundFile(filepath, \u001b[39m\"\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m file_:\n\u001b[0;32m    222\u001b[0m         \u001b[39mif\u001b[39;00m file_\u001b[39m.\u001b[39mformat \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWAV\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m normalize:\n\u001b[0;32m    223\u001b[0m             dtype \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m\n",
            "File \u001b[1;32mc:\\Users\\roger\\miniconda3\\envs\\torchaudio\\lib\\site-packages\\soundfile.py:658\u001b[0m, in \u001b[0;36mSoundFile.__init__\u001b[1;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[0;32m    655\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mode \u001b[39m=\u001b[39m mode\n\u001b[0;32m    656\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info \u001b[39m=\u001b[39m _create_info_struct(file, mode, samplerate, channels,\n\u001b[0;32m    657\u001b[0m                                  \u001b[39mformat\u001b[39m, subtype, endian)\n\u001b[1;32m--> 658\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_file \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_open(file, mode_int, closefd)\n\u001b[0;32m    659\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mset\u001b[39m(mode)\u001b[39m.\u001b[39missuperset(\u001b[39m'\u001b[39m\u001b[39mr+\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseekable():\n\u001b[0;32m    660\u001b[0m     \u001b[39m# Move write position to 0 (like in Python file objects)\u001b[39;00m\n\u001b[0;32m    661\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\roger\\miniconda3\\envs\\torchaudio\\lib\\site-packages\\soundfile.py:1205\u001b[0m, in \u001b[0;36mSoundFile._open\u001b[1;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[0;32m   1203\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1204\u001b[0m             file \u001b[39m=\u001b[39m file\u001b[39m.\u001b[39mencode(_sys\u001b[39m.\u001b[39mgetfilesystemencoding())\n\u001b[1;32m-> 1205\u001b[0m     file_ptr \u001b[39m=\u001b[39m openfunction(file, mode_int, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_info)\n\u001b[0;32m   1206\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(file, \u001b[39mint\u001b[39m):\n\u001b[0;32m   1207\u001b[0m     file_ptr \u001b[39m=\u001b[39m _snd\u001b[39m.\u001b[39msf_open_fd(file, mode_int, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info, closefd)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from src.util import plot_violin\n",
        "import numpy as np\n",
        "\n",
        "WEIGHTS_DIR = \"_weights/\"\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 15\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device {device}\")\n",
        "\n",
        "error = [[], [], [], [], []]\n",
        "\n",
        "for fx in range(5):\n",
        "\n",
        "    WEIGHTS_PATH = os.path.join(WEIGHTS_DIR, EXPERIMENT_NAME + \"_\" + str(fx))\n",
        "\n",
        "    if not os.path.exists('%s' % WEIGHTS_DIR):\n",
        "        os.makedirs('%s' % WEIGHTS_DIR)\n",
        "\n",
        "    fxData = load_train_data(fx)\n",
        "    # fxData, _ = torch.utils.data.random_split(fxData, lengths=[0.01, 0.99])\n",
        "    \n",
        "    train_dataloader, test_dataloader = split_data(fxData)\n",
        "    val_dataloader = load_evaluation_data(fx)\n",
        "    \n",
        "    # construct model and assign it to device\n",
        "    cnn = model.Extractor().to(device)\n",
        "    \n",
        "    if fx == 0:\n",
        "        signal, _, _, _, _ = fxData[0]\n",
        "        print(f\"There are {len(fxData)} samples in the dataset.\")\n",
        "        print(f\"Shape of signal: {signal.shape}\")\n",
        "\n",
        "        print(\"input feature:\")\n",
        "        log_writer.add_figure(\"Input Feature\", plot_spectrogram(signal[0], title=\"MFCC\"))\n",
        "        log_writer.add_graph(cnn, signal.unsqueeze_(0))\n",
        "\n",
        "    # initialise loss funtion + optimiser\n",
        "    loss_fn = nn.MSELoss(reduction='mean')\n",
        "    optimiser = torch.optim.Adam(cnn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # train model\n",
        "    train.train(cnn,\n",
        "                train_dataloader,\n",
        "                test_dataloader,\n",
        "                loss_fn,\n",
        "                optimiser,\n",
        "                device,\n",
        "                log_writer,\n",
        "                EPOCHS,\n",
        "                WEIGHTS_PATH,\n",
        "                effect=fx)\n",
        "\n",
        "    _, _, log = train.test(cnn, val_dataloader, device, effect=fx)\n",
        "    for _, data in enumerate(log):\n",
        "        error[fx].append(data[3])\n",
        "\n",
        "    arr = np.array(error[fx])\n",
        "    np.save(EVU_DIR + EXPERIMENT_NAME + \"_\" + str(fx) + \"_evaluation.npy\", arr)\n",
        "\n",
        "log_writer.add_figure(\"Error Box\", \n",
        "                      plot_violin(error, title=\"Error\", labels=EFFECT_MAP, ylabel=\"parameter value\", outlier=True))\n",
        "\n",
        "log_writer.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "torchaudio-tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
