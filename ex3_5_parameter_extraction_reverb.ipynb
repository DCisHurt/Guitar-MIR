{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Distortion Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMiPTdSA8C2F",
        "outputId": "a297ba92-a5c6-4015-d92d-1e9487daf896"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import os"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gIdrZtW_JYf",
        "outputId": "c1bb6969-15a9-44eb-8cdd-8d79c89208f2"
      },
      "outputs": [],
      "source": [
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050*3\n",
        "\n",
        "mfcc = torchaudio.transforms.MFCC(\n",
        "    sample_rate = SAMPLE_RATE, \n",
        "    n_mfcc = 64,\n",
        "    melkwargs = {\n",
        "        \"n_fft\": 1024,\n",
        "        \"hop_length\": 1024,\n",
        "        \"n_mels\": 64,\n",
        "        \"center\": False})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Functions for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.gtfxdataset import GtFxDataset\n",
        "from src.util import plot_spectrogram\n",
        "from src.extrector import train\n",
        "from src.extrector import model\n",
        "from torch import nn\n",
        "\n",
        "AUDIO_DIR = \"_assets/DATASET/GT-FX-C53/\"\n",
        "ANNOTATIONS_FILE = os.path.join(AUDIO_DIR, \"train.csv\")\n",
        "EVU_ANNOTATIONS_FILE = os.path.join(AUDIO_DIR, \"evaluation.csv\")\n",
        "EFFECT_MAP = [\"distortion\", \"chorus\", \"tremolo\", \"delay\", \"reverb\"]\n",
        "\n",
        "def load_train_data(effect):\n",
        "    \n",
        "    fxData = GtFxDataset(ANNOTATIONS_FILE,\n",
        "                        AUDIO_DIR,\n",
        "                        mfcc,\n",
        "                        SAMPLE_RATE,\n",
        "                        NUM_SAMPLES,\n",
        "                        device,\n",
        "                        effect=EFFECT_MAP[effect])\n",
        "    return fxData\n",
        "\n",
        "def load_evaluation_data(effect):\n",
        "\n",
        "    evuData = GtFxDataset(EVU_ANNOTATIONS_FILE,\n",
        "                        AUDIO_DIR,\n",
        "                        mfcc,\n",
        "                        SAMPLE_RATE,\n",
        "                        NUM_SAMPLES,\n",
        "                        device,\n",
        "                        effect=EFFECT_MAP[effect])\n",
        "\n",
        "    BATCH_SIZE = round(len(evuData) / 1500)\n",
        "    val_dataloader = train.create_data_loader(evuData, BATCH_SIZE)\n",
        "    return val_dataloader\n",
        "\n",
        "def split_data(data):\n",
        "\n",
        "    BATCH_SIZE = round(len(data) / 1500)\n",
        "\n",
        "    split_ratio = [0.9, 0.1]\n",
        "    train_set, test_set = torch.utils.data.random_split(data, lengths=split_ratio)\n",
        "\n",
        "    train_dataloader = train.create_data_loader(train_set, BATCH_SIZE)\n",
        "    test_dataloader = train.create_data_loader(test_set, BATCH_SIZE)\n",
        "\n",
        "    return train_dataloader, test_dataloader   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Add Tensorboard to record data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "EXPERIMENT_NAME = \"c53_parameter\"\n",
        "LOG_DIR = \"_log/\" + EXPERIMENT_NAME\n",
        "EVU_DIR = \"_log/Evaluation/\"\n",
        "\n",
        "if not os.path.exists('%s' % LOG_DIR):\n",
        "    os.makedirs('%s' % LOG_DIR)\n",
        "\n",
        "if not os.path.exists('%s' % EVU_DIR):\n",
        "    os.makedirs('%s' % EVU_DIR)\n",
        "\n",
        "log_writer = SummaryWriter(LOG_DIR)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8O9r5WI2zuI",
        "outputId": "ed2a4ebd-f644-4e35-8fda-86170924dfe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cpu\n",
            "Epoch 1\n",
            "loss: 0.101386  [  0/170046]\n",
            "loss: 0.244178  [2520/170046]\n",
            "loss: 0.287179  [5040/170046]\n",
            "loss: 0.070572  [7560/170046]\n",
            "loss: 0.049818  [10080/170046]\n",
            "loss: 0.043875  [12600/170046]\n",
            "loss: 0.038516  [15120/170046]\n",
            "loss: 0.030149  [17640/170046]\n",
            "loss: 0.028433  [20160/170046]\n",
            "loss: 0.023954  [22680/170046]\n",
            "loss: 0.024047  [25200/170046]\n",
            "loss: 0.019714  [27720/170046]\n",
            "loss: 0.020643  [30240/170046]\n",
            "loss: 0.029026  [32760/170046]\n",
            "loss: 0.026084  [35280/170046]\n",
            "loss: 0.017839  [37800/170046]\n",
            "loss: 0.023692  [40320/170046]\n",
            "loss: 0.013714  [42840/170046]\n",
            "loss: 0.021294  [45360/170046]\n",
            "loss: 0.016387  [47880/170046]\n",
            "loss: 0.020015  [50400/170046]\n",
            "loss: 0.019754  [52920/170046]\n",
            "loss: 0.014443  [55440/170046]\n",
            "loss: 0.014945  [57960/170046]\n",
            "loss: 0.016373  [60480/170046]\n",
            "loss: 0.016577  [63000/170046]\n",
            "loss: 0.018557  [65520/170046]\n",
            "loss: 0.016276  [68040/170046]\n",
            "loss: 0.012428  [70560/170046]\n",
            "loss: 0.016906  [73080/170046]\n",
            "loss: 0.013597  [75600/170046]\n",
            "loss: 0.009675  [78120/170046]\n",
            "loss: 0.013209  [80640/170046]\n",
            "loss: 0.014126  [83160/170046]\n",
            "loss: 0.015363  [85680/170046]\n",
            "loss: 0.018770  [88200/170046]\n",
            "loss: 0.015104  [90720/170046]\n",
            "loss: 0.011573  [93240/170046]\n",
            "loss: 0.010742  [95760/170046]\n",
            "loss: 0.014000  [98280/170046]\n",
            "loss: 0.008759  [100800/170046]\n",
            "loss: 0.013887  [103320/170046]\n",
            "loss: 0.017948  [105840/170046]\n",
            "loss: 0.009596  [108360/170046]\n",
            "loss: 0.014047  [110880/170046]\n",
            "loss: 0.009947  [113400/170046]\n",
            "loss: 0.011118  [115920/170046]\n",
            "loss: 0.015441  [118440/170046]\n",
            "loss: 0.013348  [120960/170046]\n",
            "loss: 0.010662  [123480/170046]\n",
            "loss: 0.011571  [126000/170046]\n",
            "loss: 0.012265  [128520/170046]\n",
            "loss: 0.010820  [131040/170046]\n",
            "loss: 0.006931  [133560/170046]\n",
            "loss: 0.014807  [136080/170046]\n",
            "loss: 0.016593  [138600/170046]\n",
            "loss: 0.009409  [141120/170046]\n",
            "loss: 0.012268  [143640/170046]\n",
            "loss: 0.012676  [146160/170046]\n",
            "loss: 0.010508  [148680/170046]\n",
            "loss: 0.009596  [151200/170046]\n",
            "loss: 0.013965  [153720/170046]\n",
            "loss: 0.009195  [156240/170046]\n",
            "loss: 0.009686  [158760/170046]\n",
            "loss: 0.018672  [161280/170046]\n",
            "loss: 0.008127  [163800/170046]\n",
            "loss: 0.012100  [166320/170046]\n",
            "loss: 0.006916  [168840/170046]\n",
            "reverb decay: avg MSE: 0.011552, avg abs error: 0.0785\n",
            "learning rate: 0.001000 -> 0.000910\n",
            "---------------------------\n",
            "\n",
            "Epoch 2\n",
            "loss: 0.009566  [  0/170046]\n",
            "loss: 0.007514  [2520/170046]\n",
            "loss: 0.009012  [5040/170046]\n",
            "loss: 0.008376  [7560/170046]\n",
            "loss: 0.007622  [10080/170046]\n",
            "loss: 0.008199  [12600/170046]\n",
            "loss: 0.007262  [15120/170046]\n",
            "loss: 0.007239  [17640/170046]\n",
            "loss: 0.006459  [20160/170046]\n",
            "loss: 0.010174  [22680/170046]\n",
            "loss: 0.005830  [25200/170046]\n",
            "loss: 0.009356  [27720/170046]\n",
            "loss: 0.007201  [30240/170046]\n",
            "loss: 0.009361  [32760/170046]\n",
            "loss: 0.013008  [35280/170046]\n",
            "loss: 0.006831  [37800/170046]\n",
            "loss: 0.012404  [40320/170046]\n",
            "loss: 0.011354  [42840/170046]\n",
            "loss: 0.012529  [45360/170046]\n",
            "loss: 0.008228  [47880/170046]\n",
            "loss: 0.006572  [50400/170046]\n",
            "loss: 0.006962  [52920/170046]\n",
            "loss: 0.011933  [55440/170046]\n",
            "loss: 0.009787  [57960/170046]\n",
            "loss: 0.009065  [60480/170046]\n",
            "loss: 0.005723  [63000/170046]\n",
            "loss: 0.009874  [65520/170046]\n",
            "loss: 0.007768  [68040/170046]\n",
            "loss: 0.006258  [70560/170046]\n",
            "loss: 0.007065  [73080/170046]\n",
            "loss: 0.006342  [75600/170046]\n",
            "loss: 0.008269  [78120/170046]\n",
            "loss: 0.004743  [80640/170046]\n",
            "loss: 0.010117  [83160/170046]\n",
            "loss: 0.006359  [85680/170046]\n",
            "loss: 0.007942  [88200/170046]\n",
            "loss: 0.011703  [90720/170046]\n",
            "loss: 0.009256  [93240/170046]\n",
            "loss: 0.009541  [95760/170046]\n",
            "loss: 0.009543  [98280/170046]\n",
            "loss: 0.010801  [100800/170046]\n",
            "loss: 0.007247  [103320/170046]\n",
            "loss: 0.011643  [105840/170046]\n",
            "loss: 0.005366  [108360/170046]\n",
            "loss: 0.008788  [110880/170046]\n",
            "loss: 0.006582  [113400/170046]\n",
            "loss: 0.008097  [115920/170046]\n",
            "loss: 0.006989  [118440/170046]\n",
            "loss: 0.006027  [120960/170046]\n",
            "loss: 0.008335  [123480/170046]\n",
            "loss: 0.009365  [126000/170046]\n",
            "loss: 0.005170  [128520/170046]\n",
            "loss: 0.013331  [131040/170046]\n",
            "loss: 0.008741  [133560/170046]\n",
            "loss: 0.008732  [136080/170046]\n",
            "loss: 0.006475  [138600/170046]\n",
            "loss: 0.009223  [141120/170046]\n",
            "loss: 0.005065  [143640/170046]\n",
            "loss: 0.007724  [146160/170046]\n",
            "loss: 0.005843  [148680/170046]\n",
            "loss: 0.006593  [151200/170046]\n",
            "loss: 0.008467  [153720/170046]\n",
            "loss: 0.006174  [156240/170046]\n",
            "loss: 0.007461  [158760/170046]\n",
            "loss: 0.007111  [161280/170046]\n",
            "loss: 0.005493  [163800/170046]\n",
            "loss: 0.007042  [166320/170046]\n",
            "loss: 0.013729  [168840/170046]\n",
            "reverb decay: avg MSE: 0.006255, avg abs error: 0.0552\n",
            "learning rate: 0.000910 -> 0.000820\n",
            "---------------------------\n",
            "\n",
            "Epoch 3\n",
            "loss: 0.006911  [  0/170046]\n",
            "loss: 0.005963  [2520/170046]\n",
            "loss: 0.004759  [5040/170046]\n",
            "loss: 0.005084  [7560/170046]\n",
            "loss: 0.005588  [10080/170046]\n",
            "loss: 0.007690  [12600/170046]\n",
            "loss: 0.006727  [15120/170046]\n",
            "loss: 0.006383  [17640/170046]\n",
            "loss: 0.005789  [20160/170046]\n",
            "loss: 0.011061  [22680/170046]\n",
            "loss: 0.004506  [25200/170046]\n",
            "loss: 0.008995  [27720/170046]\n",
            "loss: 0.006959  [30240/170046]\n",
            "loss: 0.006362  [32760/170046]\n",
            "loss: 0.005912  [35280/170046]\n",
            "loss: 0.007639  [37800/170046]\n",
            "loss: 0.006766  [40320/170046]\n",
            "loss: 0.011888  [42840/170046]\n",
            "loss: 0.005257  [45360/170046]\n",
            "loss: 0.005570  [47880/170046]\n",
            "loss: 0.006305  [50400/170046]\n",
            "loss: 0.005782  [52920/170046]\n",
            "loss: 0.006890  [55440/170046]\n",
            "loss: 0.003375  [57960/170046]\n",
            "loss: 0.009743  [60480/170046]\n",
            "loss: 0.005724  [63000/170046]\n",
            "loss: 0.006745  [65520/170046]\n",
            "loss: 0.006946  [68040/170046]\n",
            "loss: 0.005751  [70560/170046]\n",
            "loss: 0.006230  [73080/170046]\n",
            "loss: 0.005920  [75600/170046]\n",
            "loss: 0.005939  [78120/170046]\n",
            "loss: 0.004981  [80640/170046]\n",
            "loss: 0.005605  [83160/170046]\n",
            "loss: 0.004669  [85680/170046]\n",
            "loss: 0.010056  [88200/170046]\n",
            "loss: 0.005794  [90720/170046]\n",
            "loss: 0.007434  [93240/170046]\n",
            "loss: 0.006209  [95760/170046]\n",
            "loss: 0.006553  [98280/170046]\n",
            "loss: 0.004767  [100800/170046]\n",
            "loss: 0.006982  [103320/170046]\n",
            "loss: 0.005910  [105840/170046]\n",
            "loss: 0.005810  [108360/170046]\n",
            "loss: 0.005646  [110880/170046]\n",
            "loss: 0.004538  [113400/170046]\n",
            "loss: 0.004536  [115920/170046]\n",
            "loss: 0.007458  [118440/170046]\n",
            "loss: 0.003696  [120960/170046]\n",
            "loss: 0.004839  [123480/170046]\n",
            "loss: 0.009373  [126000/170046]\n",
            "loss: 0.006707  [128520/170046]\n",
            "loss: 0.003916  [131040/170046]\n",
            "loss: 0.005891  [133560/170046]\n",
            "loss: 0.004538  [136080/170046]\n",
            "loss: 0.005174  [138600/170046]\n",
            "loss: 0.004639  [141120/170046]\n",
            "loss: 0.005761  [143640/170046]\n",
            "loss: 0.010020  [146160/170046]\n",
            "loss: 0.004005  [148680/170046]\n",
            "loss: 0.005836  [151200/170046]\n",
            "loss: 0.007276  [153720/170046]\n",
            "loss: 0.004967  [156240/170046]\n",
            "loss: 0.004358  [158760/170046]\n",
            "loss: 0.004745  [161280/170046]\n",
            "loss: 0.004575  [163800/170046]\n",
            "loss: 0.003468  [166320/170046]\n",
            "loss: 0.005147  [168840/170046]\n",
            "reverb decay: avg MSE: 0.006683, avg abs error: 0.0623\n",
            "learning rate: 0.000820 -> 0.000730\n",
            "---------------------------\n",
            "\n",
            "Epoch 4\n",
            "loss: 0.005141  [  0/170046]\n",
            "loss: 0.004378  [2520/170046]\n",
            "loss: 0.004502  [5040/170046]\n",
            "loss: 0.004995  [7560/170046]\n",
            "loss: 0.008406  [10080/170046]\n",
            "loss: 0.004290  [12600/170046]\n",
            "loss: 0.007665  [15120/170046]\n",
            "loss: 0.005644  [17640/170046]\n",
            "loss: 0.004350  [20160/170046]\n",
            "loss: 0.003336  [22680/170046]\n",
            "loss: 0.005425  [25200/170046]\n",
            "loss: 0.006670  [27720/170046]\n",
            "loss: 0.007254  [30240/170046]\n",
            "loss: 0.006680  [32760/170046]\n",
            "loss: 0.008126  [35280/170046]\n",
            "loss: 0.005168  [37800/170046]\n",
            "loss: 0.003587  [40320/170046]\n",
            "loss: 0.005507  [42840/170046]\n",
            "loss: 0.004908  [45360/170046]\n",
            "loss: 0.005312  [47880/170046]\n",
            "loss: 0.006952  [50400/170046]\n",
            "loss: 0.005837  [52920/170046]\n",
            "loss: 0.005479  [55440/170046]\n",
            "loss: 0.005177  [57960/170046]\n",
            "loss: 0.004186  [60480/170046]\n",
            "loss: 0.006679  [63000/170046]\n",
            "loss: 0.005012  [65520/170046]\n",
            "loss: 0.003612  [68040/170046]\n",
            "loss: 0.006308  [70560/170046]\n",
            "loss: 0.007613  [73080/170046]\n",
            "loss: 0.004071  [75600/170046]\n",
            "loss: 0.003974  [78120/170046]\n",
            "loss: 0.005049  [80640/170046]\n",
            "loss: 0.007198  [83160/170046]\n",
            "loss: 0.006544  [85680/170046]\n",
            "loss: 0.005805  [88200/170046]\n",
            "loss: 0.005411  [90720/170046]\n",
            "loss: 0.003209  [93240/170046]\n",
            "loss: 0.005526  [95760/170046]\n",
            "loss: 0.004934  [98280/170046]\n",
            "loss: 0.005833  [100800/170046]\n",
            "loss: 0.005678  [103320/170046]\n",
            "loss: 0.004452  [105840/170046]\n",
            "loss: 0.003715  [108360/170046]\n",
            "loss: 0.006928  [110880/170046]\n",
            "loss: 0.005505  [113400/170046]\n",
            "loss: 0.005970  [115920/170046]\n",
            "loss: 0.003580  [118440/170046]\n",
            "loss: 0.005392  [120960/170046]\n",
            "loss: 0.003417  [123480/170046]\n",
            "loss: 0.003581  [126000/170046]\n",
            "loss: 0.004602  [128520/170046]\n",
            "loss: 0.003475  [131040/170046]\n",
            "loss: 0.002858  [133560/170046]\n",
            "loss: 0.003681  [136080/170046]\n",
            "loss: 0.005276  [138600/170046]\n",
            "loss: 0.005156  [141120/170046]\n",
            "loss: 0.005626  [143640/170046]\n",
            "loss: 0.004993  [146160/170046]\n",
            "loss: 0.006379  [148680/170046]\n",
            "loss: 0.004773  [151200/170046]\n",
            "loss: 0.005593  [153720/170046]\n",
            "loss: 0.003880  [156240/170046]\n",
            "loss: 0.005346  [158760/170046]\n",
            "loss: 0.005386  [161280/170046]\n",
            "loss: 0.004288  [163800/170046]\n",
            "loss: 0.006855  [166320/170046]\n",
            "loss: 0.005313  [168840/170046]\n",
            "reverb decay: avg MSE: 0.010232, avg abs error: 0.0805\n",
            "learning rate: 0.000730 -> 0.000640\n",
            "---------------------------\n",
            "\n",
            "Epoch 5\n",
            "loss: 0.008417  [  0/170046]\n",
            "loss: 0.003282  [2520/170046]\n",
            "loss: 0.004172  [5040/170046]\n",
            "loss: 0.004026  [7560/170046]\n",
            "loss: 0.004925  [10080/170046]\n",
            "loss: 0.004649  [12600/170046]\n",
            "loss: 0.004723  [15120/170046]\n",
            "loss: 0.006109  [17640/170046]\n",
            "loss: 0.004880  [20160/170046]\n",
            "loss: 0.003718  [22680/170046]\n",
            "loss: 0.006469  [25200/170046]\n",
            "loss: 0.009046  [27720/170046]\n",
            "loss: 0.003894  [30240/170046]\n",
            "loss: 0.004622  [32760/170046]\n",
            "loss: 0.004744  [35280/170046]\n",
            "loss: 0.003986  [37800/170046]\n",
            "loss: 0.005656  [40320/170046]\n",
            "loss: 0.003881  [42840/170046]\n",
            "loss: 0.005290  [45360/170046]\n",
            "loss: 0.005189  [47880/170046]\n",
            "loss: 0.004065  [50400/170046]\n",
            "loss: 0.005380  [52920/170046]\n",
            "loss: 0.004660  [55440/170046]\n",
            "loss: 0.004871  [57960/170046]\n",
            "loss: 0.003153  [60480/170046]\n",
            "loss: 0.006484  [63000/170046]\n",
            "loss: 0.004627  [65520/170046]\n",
            "loss: 0.004344  [68040/170046]\n",
            "loss: 0.003568  [70560/170046]\n",
            "loss: 0.003769  [73080/170046]\n",
            "loss: 0.004764  [75600/170046]\n",
            "loss: 0.005903  [78120/170046]\n",
            "loss: 0.006517  [80640/170046]\n",
            "loss: 0.005375  [83160/170046]\n",
            "loss: 0.004209  [85680/170046]\n",
            "loss: 0.004764  [88200/170046]\n",
            "loss: 0.003472  [90720/170046]\n",
            "loss: 0.004733  [93240/170046]\n",
            "loss: 0.003795  [95760/170046]\n",
            "loss: 0.003438  [98280/170046]\n",
            "loss: 0.004107  [100800/170046]\n",
            "loss: 0.003286  [103320/170046]\n",
            "loss: 0.004557  [105840/170046]\n",
            "loss: 0.004035  [108360/170046]\n",
            "loss: 0.003681  [110880/170046]\n",
            "loss: 0.003515  [113400/170046]\n",
            "loss: 0.002764  [115920/170046]\n",
            "loss: 0.004568  [118440/170046]\n",
            "loss: 0.004529  [120960/170046]\n",
            "loss: 0.003724  [123480/170046]\n",
            "loss: 0.003011  [126000/170046]\n",
            "loss: 0.003192  [128520/170046]\n",
            "loss: 0.004472  [131040/170046]\n",
            "loss: 0.004143  [133560/170046]\n",
            "loss: 0.002583  [136080/170046]\n",
            "loss: 0.003455  [138600/170046]\n",
            "loss: 0.003887  [141120/170046]\n",
            "loss: 0.005456  [143640/170046]\n",
            "loss: 0.003526  [146160/170046]\n",
            "loss: 0.002967  [148680/170046]\n",
            "loss: 0.004957  [151200/170046]\n",
            "loss: 0.005718  [153720/170046]\n",
            "loss: 0.003776  [156240/170046]\n",
            "loss: 0.005065  [158760/170046]\n",
            "loss: 0.004648  [161280/170046]\n",
            "loss: 0.005238  [163800/170046]\n",
            "loss: 0.004406  [166320/170046]\n",
            "loss: 0.005184  [168840/170046]\n",
            "reverb decay: avg MSE: 0.004203, avg abs error: 0.0441\n",
            "learning rate: 0.000640 -> 0.000550\n",
            "---------------------------\n",
            "\n",
            "Epoch 6\n",
            "loss: 0.005760  [  0/170046]\n",
            "loss: 0.004192  [2520/170046]\n",
            "loss: 0.003284  [5040/170046]\n",
            "loss: 0.003131  [7560/170046]\n",
            "loss: 0.005451  [10080/170046]\n",
            "loss: 0.003348  [12600/170046]\n",
            "loss: 0.003678  [15120/170046]\n",
            "loss: 0.002406  [17640/170046]\n",
            "loss: 0.002528  [20160/170046]\n",
            "loss: 0.003397  [22680/170046]\n",
            "loss: 0.003484  [25200/170046]\n",
            "loss: 0.005046  [27720/170046]\n",
            "loss: 0.004863  [30240/170046]\n",
            "loss: 0.003738  [32760/170046]\n",
            "loss: 0.003626  [35280/170046]\n",
            "loss: 0.003410  [37800/170046]\n",
            "loss: 0.003382  [40320/170046]\n",
            "loss: 0.005386  [42840/170046]\n",
            "loss: 0.004321  [45360/170046]\n",
            "loss: 0.003208  [47880/170046]\n",
            "loss: 0.003626  [50400/170046]\n",
            "loss: 0.003106  [52920/170046]\n",
            "loss: 0.005172  [55440/170046]\n",
            "loss: 0.004144  [57960/170046]\n",
            "loss: 0.003842  [60480/170046]\n",
            "loss: 0.003261  [63000/170046]\n",
            "loss: 0.004869  [65520/170046]\n",
            "loss: 0.002959  [68040/170046]\n",
            "loss: 0.004166  [70560/170046]\n",
            "loss: 0.003837  [73080/170046]\n",
            "loss: 0.005308  [75600/170046]\n",
            "loss: 0.003930  [78120/170046]\n",
            "loss: 0.003439  [80640/170046]\n",
            "loss: 0.004678  [83160/170046]\n",
            "loss: 0.003607  [85680/170046]\n",
            "loss: 0.003836  [88200/170046]\n",
            "loss: 0.003556  [90720/170046]\n",
            "loss: 0.003907  [93240/170046]\n",
            "loss: 0.003776  [95760/170046]\n",
            "loss: 0.004215  [98280/170046]\n",
            "loss: 0.002920  [100800/170046]\n",
            "loss: 0.002833  [103320/170046]\n",
            "loss: 0.003967  [105840/170046]\n",
            "loss: 0.004311  [108360/170046]\n",
            "loss: 0.002803  [110880/170046]\n",
            "loss: 0.004381  [113400/170046]\n",
            "loss: 0.004273  [115920/170046]\n",
            "loss: 0.003943  [118440/170046]\n",
            "loss: 0.003042  [120960/170046]\n",
            "loss: 0.003110  [123480/170046]\n",
            "loss: 0.002962  [126000/170046]\n",
            "loss: 0.003682  [128520/170046]\n",
            "loss: 0.004014  [131040/170046]\n",
            "loss: 0.004120  [133560/170046]\n",
            "loss: 0.003560  [136080/170046]\n",
            "loss: 0.003260  [138600/170046]\n",
            "loss: 0.004779  [141120/170046]\n",
            "loss: 0.003606  [143640/170046]\n",
            "loss: 0.003545  [146160/170046]\n",
            "loss: 0.004443  [148680/170046]\n",
            "loss: 0.004969  [151200/170046]\n",
            "loss: 0.003386  [153720/170046]\n",
            "loss: 0.004886  [156240/170046]\n",
            "loss: 0.005213  [158760/170046]\n",
            "loss: 0.005189  [161280/170046]\n",
            "loss: 0.003560  [163800/170046]\n",
            "loss: 0.003348  [166320/170046]\n",
            "loss: 0.004173  [168840/170046]\n",
            "reverb decay: avg MSE: 0.004746, avg abs error: 0.0497\n",
            "learning rate: 0.000550 -> 0.000460\n",
            "---------------------------\n",
            "\n",
            "Epoch 7\n",
            "loss: 0.002908  [  0/170046]\n",
            "loss: 0.003246  [2520/170046]\n",
            "loss: 0.002427  [5040/170046]\n",
            "loss: 0.003715  [7560/170046]\n",
            "loss: 0.004403  [10080/170046]\n",
            "loss: 0.002983  [12600/170046]\n",
            "loss: 0.003640  [15120/170046]\n",
            "loss: 0.002662  [17640/170046]\n",
            "loss: 0.002865  [20160/170046]\n",
            "loss: 0.002169  [22680/170046]\n",
            "loss: 0.002947  [25200/170046]\n",
            "loss: 0.006115  [27720/170046]\n",
            "loss: 0.003002  [30240/170046]\n",
            "loss: 0.002230  [32760/170046]\n",
            "loss: 0.002949  [35280/170046]\n",
            "loss: 0.002604  [37800/170046]\n",
            "loss: 0.002718  [40320/170046]\n",
            "loss: 0.002123  [42840/170046]\n",
            "loss: 0.003606  [45360/170046]\n",
            "loss: 0.003446  [47880/170046]\n",
            "loss: 0.002908  [50400/170046]\n",
            "loss: 0.003357  [52920/170046]\n",
            "loss: 0.002346  [55440/170046]\n",
            "loss: 0.003528  [57960/170046]\n",
            "loss: 0.002299  [60480/170046]\n",
            "loss: 0.003458  [63000/170046]\n",
            "loss: 0.003351  [65520/170046]\n",
            "loss: 0.002286  [68040/170046]\n",
            "loss: 0.004565  [70560/170046]\n",
            "loss: 0.004820  [73080/170046]\n",
            "loss: 0.002154  [75600/170046]\n",
            "loss: 0.003435  [78120/170046]\n",
            "loss: 0.003189  [80640/170046]\n",
            "loss: 0.003760  [83160/170046]\n",
            "loss: 0.002754  [85680/170046]\n",
            "loss: 0.004005  [88200/170046]\n",
            "loss: 0.003771  [90720/170046]\n",
            "loss: 0.003763  [93240/170046]\n",
            "loss: 0.005684  [95760/170046]\n",
            "loss: 0.004040  [98280/170046]\n",
            "loss: 0.005224  [100800/170046]\n",
            "loss: 0.004759  [103320/170046]\n",
            "loss: 0.003331  [105840/170046]\n",
            "loss: 0.004297  [108360/170046]\n",
            "loss: 0.002455  [110880/170046]\n",
            "loss: 0.002287  [113400/170046]\n",
            "loss: 0.004286  [115920/170046]\n",
            "loss: 0.003820  [118440/170046]\n",
            "loss: 0.003768  [120960/170046]\n",
            "loss: 0.002623  [123480/170046]\n",
            "loss: 0.003366  [126000/170046]\n",
            "loss: 0.003512  [128520/170046]\n",
            "loss: 0.002248  [131040/170046]\n",
            "loss: 0.003328  [133560/170046]\n",
            "loss: 0.004869  [136080/170046]\n",
            "loss: 0.003418  [138600/170046]\n",
            "loss: 0.002900  [141120/170046]\n",
            "loss: 0.003252  [143640/170046]\n",
            "loss: 0.003237  [146160/170046]\n",
            "loss: 0.002499  [148680/170046]\n",
            "loss: 0.002979  [151200/170046]\n",
            "loss: 0.002979  [153720/170046]\n",
            "loss: 0.004605  [156240/170046]\n",
            "loss: 0.004320  [158760/170046]\n",
            "loss: 0.002216  [161280/170046]\n",
            "loss: 0.003601  [163800/170046]\n",
            "loss: 0.004347  [166320/170046]\n",
            "loss: 0.003098  [168840/170046]\n",
            "reverb decay: avg MSE: 0.003475, avg abs error: 0.0409\n",
            "learning rate: 0.000460 -> 0.000370\n",
            "---------------------------\n",
            "\n",
            "Epoch 8\n",
            "loss: 0.002226  [  0/170046]\n",
            "loss: 0.003059  [2520/170046]\n",
            "loss: 0.001578  [5040/170046]\n",
            "loss: 0.004021  [7560/170046]\n",
            "loss: 0.002783  [10080/170046]\n",
            "loss: 0.003563  [12600/170046]\n",
            "loss: 0.004017  [15120/170046]\n",
            "loss: 0.002896  [17640/170046]\n",
            "loss: 0.002091  [20160/170046]\n",
            "loss: 0.003761  [22680/170046]\n",
            "loss: 0.002215  [25200/170046]\n",
            "loss: 0.001920  [27720/170046]\n",
            "loss: 0.003829  [30240/170046]\n",
            "loss: 0.004111  [32760/170046]\n",
            "loss: 0.003542  [35280/170046]\n",
            "loss: 0.002329  [37800/170046]\n",
            "loss: 0.002001  [40320/170046]\n",
            "loss: 0.003989  [42840/170046]\n",
            "loss: 0.004015  [45360/170046]\n",
            "loss: 0.004196  [47880/170046]\n",
            "loss: 0.002370  [50400/170046]\n",
            "loss: 0.002671  [52920/170046]\n",
            "loss: 0.003612  [55440/170046]\n",
            "loss: 0.001905  [57960/170046]\n",
            "loss: 0.003460  [60480/170046]\n",
            "loss: 0.002450  [63000/170046]\n",
            "loss: 0.004314  [65520/170046]\n",
            "loss: 0.003474  [68040/170046]\n",
            "loss: 0.003495  [70560/170046]\n",
            "loss: 0.001211  [73080/170046]\n",
            "loss: 0.003965  [75600/170046]\n",
            "loss: 0.003209  [78120/170046]\n",
            "loss: 0.002693  [80640/170046]\n",
            "loss: 0.002977  [83160/170046]\n",
            "loss: 0.004081  [85680/170046]\n",
            "loss: 0.002368  [88200/170046]\n",
            "loss: 0.002644  [90720/170046]\n",
            "loss: 0.002035  [93240/170046]\n",
            "loss: 0.003413  [95760/170046]\n",
            "loss: 0.005769  [98280/170046]\n",
            "loss: 0.003004  [100800/170046]\n",
            "loss: 0.002073  [103320/170046]\n",
            "loss: 0.003213  [105840/170046]\n",
            "loss: 0.001805  [108360/170046]\n",
            "loss: 0.003129  [110880/170046]\n",
            "loss: 0.002519  [113400/170046]\n",
            "loss: 0.001905  [115920/170046]\n",
            "loss: 0.002616  [118440/170046]\n",
            "loss: 0.004395  [120960/170046]\n",
            "loss: 0.002536  [123480/170046]\n",
            "loss: 0.002244  [126000/170046]\n",
            "loss: 0.002121  [128520/170046]\n",
            "loss: 0.001633  [131040/170046]\n",
            "loss: 0.003678  [133560/170046]\n",
            "loss: 0.002944  [136080/170046]\n",
            "loss: 0.003442  [138600/170046]\n",
            "loss: 0.003728  [141120/170046]\n",
            "loss: 0.001810  [143640/170046]\n",
            "loss: 0.002645  [146160/170046]\n",
            "loss: 0.003201  [148680/170046]\n",
            "loss: 0.003549  [151200/170046]\n",
            "loss: 0.002483  [153720/170046]\n",
            "loss: 0.003455  [156240/170046]\n",
            "loss: 0.002317  [158760/170046]\n",
            "loss: 0.001803  [161280/170046]\n",
            "loss: 0.003217  [163800/170046]\n",
            "loss: 0.002829  [166320/170046]\n",
            "loss: 0.002180  [168840/170046]\n",
            "reverb decay: avg MSE: 0.003870, avg abs error: 0.0427\n",
            "learning rate: 0.000370 -> 0.000280\n",
            "---------------------------\n",
            "\n",
            "Epoch 9\n",
            "loss: 0.003473  [  0/170046]\n",
            "loss: 0.002119  [2520/170046]\n",
            "loss: 0.002343  [5040/170046]\n",
            "loss: 0.003081  [7560/170046]\n",
            "loss: 0.002075  [10080/170046]\n",
            "loss: 0.001647  [12600/170046]\n",
            "loss: 0.001598  [15120/170046]\n",
            "loss: 0.002207  [17640/170046]\n",
            "loss: 0.001667  [20160/170046]\n",
            "loss: 0.003743  [22680/170046]\n",
            "loss: 0.002301  [25200/170046]\n",
            "loss: 0.002951  [27720/170046]\n",
            "loss: 0.002939  [30240/170046]\n",
            "loss: 0.004210  [32760/170046]\n",
            "loss: 0.001913  [35280/170046]\n",
            "loss: 0.002367  [37800/170046]\n",
            "loss: 0.002489  [40320/170046]\n",
            "loss: 0.003366  [42840/170046]\n",
            "loss: 0.003654  [45360/170046]\n",
            "loss: 0.002489  [47880/170046]\n",
            "loss: 0.003048  [50400/170046]\n",
            "loss: 0.002542  [52920/170046]\n",
            "loss: 0.003193  [55440/170046]\n",
            "loss: 0.003028  [57960/170046]\n",
            "loss: 0.003632  [60480/170046]\n",
            "loss: 0.002156  [63000/170046]\n",
            "loss: 0.003755  [65520/170046]\n",
            "loss: 0.002279  [68040/170046]\n",
            "loss: 0.003117  [70560/170046]\n",
            "loss: 0.002523  [73080/170046]\n",
            "loss: 0.002217  [75600/170046]\n",
            "loss: 0.002589  [78120/170046]\n",
            "loss: 0.002524  [80640/170046]\n",
            "loss: 0.001909  [83160/170046]\n",
            "loss: 0.002124  [85680/170046]\n",
            "loss: 0.001917  [88200/170046]\n",
            "loss: 0.002853  [90720/170046]\n",
            "loss: 0.002202  [93240/170046]\n",
            "loss: 0.001830  [95760/170046]\n",
            "loss: 0.003183  [98280/170046]\n",
            "loss: 0.002807  [100800/170046]\n",
            "loss: 0.002897  [103320/170046]\n",
            "loss: 0.002701  [105840/170046]\n",
            "loss: 0.002509  [108360/170046]\n",
            "loss: 0.001998  [110880/170046]\n",
            "loss: 0.002321  [113400/170046]\n",
            "loss: 0.001955  [115920/170046]\n",
            "loss: 0.001983  [118440/170046]\n",
            "loss: 0.002633  [120960/170046]\n",
            "loss: 0.002074  [123480/170046]\n",
            "loss: 0.003053  [126000/170046]\n",
            "loss: 0.002563  [128520/170046]\n",
            "loss: 0.001808  [131040/170046]\n",
            "loss: 0.002526  [133560/170046]\n",
            "loss: 0.002411  [136080/170046]\n",
            "loss: 0.002179  [138600/170046]\n",
            "loss: 0.003384  [141120/170046]\n",
            "loss: 0.001934  [143640/170046]\n",
            "loss: 0.003438  [146160/170046]\n",
            "loss: 0.001821  [148680/170046]\n",
            "loss: 0.002796  [151200/170046]\n",
            "loss: 0.002152  [153720/170046]\n",
            "loss: 0.002594  [156240/170046]\n",
            "loss: 0.002734  [158760/170046]\n",
            "loss: 0.002824  [161280/170046]\n",
            "loss: 0.002295  [163800/170046]\n",
            "loss: 0.002347  [166320/170046]\n",
            "loss: 0.003686  [168840/170046]\n",
            "reverb decay: avg MSE: 0.003151, avg abs error: 0.0388\n",
            "learning rate: 0.000280 -> 0.000190\n",
            "---------------------------\n",
            "\n",
            "Epoch 10\n",
            "loss: 0.001594  [  0/170046]\n",
            "loss: 0.002311  [2520/170046]\n",
            "loss: 0.002497  [5040/170046]\n",
            "loss: 0.001776  [7560/170046]\n",
            "loss: 0.002493  [10080/170046]\n",
            "loss: 0.001897  [12600/170046]\n",
            "loss: 0.002870  [15120/170046]\n",
            "loss: 0.003661  [17640/170046]\n",
            "loss: 0.002098  [20160/170046]\n",
            "loss: 0.001991  [22680/170046]\n",
            "loss: 0.002211  [25200/170046]\n",
            "loss: 0.001933  [27720/170046]\n",
            "loss: 0.001683  [30240/170046]\n",
            "loss: 0.003460  [32760/170046]\n",
            "loss: 0.002613  [35280/170046]\n",
            "loss: 0.001946  [37800/170046]\n",
            "loss: 0.002216  [40320/170046]\n",
            "loss: 0.002706  [42840/170046]\n",
            "loss: 0.001793  [45360/170046]\n",
            "loss: 0.001836  [47880/170046]\n",
            "loss: 0.001555  [50400/170046]\n",
            "loss: 0.001819  [52920/170046]\n",
            "loss: 0.002893  [55440/170046]\n",
            "loss: 0.002071  [57960/170046]\n",
            "loss: 0.001369  [60480/170046]\n",
            "loss: 0.002631  [63000/170046]\n",
            "loss: 0.001676  [65520/170046]\n",
            "loss: 0.002486  [68040/170046]\n",
            "loss: 0.002566  [70560/170046]\n",
            "loss: 0.002091  [73080/170046]\n",
            "loss: 0.002008  [75600/170046]\n",
            "loss: 0.002060  [78120/170046]\n",
            "loss: 0.001737  [80640/170046]\n",
            "loss: 0.002145  [83160/170046]\n",
            "loss: 0.002143  [85680/170046]\n",
            "loss: 0.002939  [88200/170046]\n",
            "loss: 0.002901  [90720/170046]\n",
            "loss: 0.002169  [93240/170046]\n",
            "loss: 0.002716  [95760/170046]\n",
            "loss: 0.001663  [98280/170046]\n",
            "loss: 0.001903  [100800/170046]\n",
            "loss: 0.002417  [103320/170046]\n",
            "loss: 0.002345  [105840/170046]\n",
            "loss: 0.002469  [108360/170046]\n",
            "loss: 0.003259  [110880/170046]\n",
            "loss: 0.002023  [113400/170046]\n",
            "loss: 0.001775  [115920/170046]\n",
            "loss: 0.002396  [118440/170046]\n",
            "loss: 0.002746  [120960/170046]\n",
            "loss: 0.003049  [123480/170046]\n",
            "loss: 0.002372  [126000/170046]\n",
            "loss: 0.002133  [128520/170046]\n",
            "loss: 0.002252  [131040/170046]\n",
            "loss: 0.002116  [133560/170046]\n",
            "loss: 0.002708  [136080/170046]\n",
            "loss: 0.003003  [138600/170046]\n",
            "loss: 0.002954  [141120/170046]\n",
            "loss: 0.003048  [143640/170046]\n",
            "loss: 0.002553  [146160/170046]\n",
            "loss: 0.002229  [148680/170046]\n",
            "loss: 0.003131  [151200/170046]\n",
            "loss: 0.002164  [153720/170046]\n",
            "loss: 0.002519  [156240/170046]\n",
            "loss: 0.003096  [158760/170046]\n",
            "loss: 0.002478  [161280/170046]\n",
            "loss: 0.002670  [163800/170046]\n",
            "loss: 0.002416  [166320/170046]\n",
            "loss: 0.001459  [168840/170046]\n",
            "reverb decay: avg MSE: 0.003107, avg abs error: 0.0383\n",
            "learning rate: 0.000190 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 11\n",
            "loss: 0.001581  [  0/170046]\n",
            "loss: 0.002061  [2520/170046]\n",
            "loss: 0.001912  [5040/170046]\n",
            "loss: 0.001274  [7560/170046]\n",
            "loss: 0.002427  [10080/170046]\n",
            "loss: 0.001774  [12600/170046]\n",
            "loss: 0.002145  [15120/170046]\n",
            "loss: 0.002170  [17640/170046]\n",
            "loss: 0.003021  [20160/170046]\n",
            "loss: 0.001959  [22680/170046]\n",
            "loss: 0.001647  [25200/170046]\n",
            "loss: 0.002355  [27720/170046]\n",
            "loss: 0.002670  [30240/170046]\n",
            "loss: 0.002233  [32760/170046]\n",
            "loss: 0.001609  [35280/170046]\n",
            "loss: 0.001611  [37800/170046]\n",
            "loss: 0.001689  [40320/170046]\n",
            "loss: 0.002217  [42840/170046]\n",
            "loss: 0.001521  [45360/170046]\n",
            "loss: 0.001315  [47880/170046]\n",
            "loss: 0.001923  [50400/170046]\n",
            "loss: 0.002358  [52920/170046]\n",
            "loss: 0.002088  [55440/170046]\n",
            "loss: 0.001768  [57960/170046]\n",
            "loss: 0.001475  [60480/170046]\n",
            "loss: 0.003078  [63000/170046]\n",
            "loss: 0.001876  [65520/170046]\n",
            "loss: 0.002073  [68040/170046]\n",
            "loss: 0.002183  [70560/170046]\n",
            "loss: 0.001371  [73080/170046]\n",
            "loss: 0.002710  [75600/170046]\n",
            "loss: 0.001737  [78120/170046]\n",
            "loss: 0.002245  [80640/170046]\n",
            "loss: 0.001122  [83160/170046]\n",
            "loss: 0.001994  [85680/170046]\n",
            "loss: 0.002217  [88200/170046]\n",
            "loss: 0.002441  [90720/170046]\n",
            "loss: 0.001040  [93240/170046]\n",
            "loss: 0.001883  [95760/170046]\n",
            "loss: 0.002045  [98280/170046]\n",
            "loss: 0.001651  [100800/170046]\n",
            "loss: 0.002329  [103320/170046]\n",
            "loss: 0.001664  [105840/170046]\n",
            "loss: 0.002231  [108360/170046]\n",
            "loss: 0.002184  [110880/170046]\n",
            "loss: 0.001861  [113400/170046]\n",
            "loss: 0.001953  [115920/170046]\n",
            "loss: 0.001615  [118440/170046]\n",
            "loss: 0.003185  [120960/170046]\n",
            "loss: 0.002359  [123480/170046]\n",
            "loss: 0.001699  [126000/170046]\n",
            "loss: 0.002501  [128520/170046]\n",
            "loss: 0.001528  [131040/170046]\n",
            "loss: 0.001187  [133560/170046]\n",
            "loss: 0.001925  [136080/170046]\n",
            "loss: 0.001083  [138600/170046]\n",
            "loss: 0.001334  [141120/170046]\n",
            "loss: 0.001944  [143640/170046]\n",
            "loss: 0.002131  [146160/170046]\n",
            "loss: 0.001765  [148680/170046]\n",
            "loss: 0.002191  [151200/170046]\n",
            "loss: 0.002154  [153720/170046]\n",
            "loss: 0.002336  [156240/170046]\n",
            "loss: 0.002021  [158760/170046]\n",
            "loss: 0.002216  [161280/170046]\n",
            "loss: 0.001354  [163800/170046]\n",
            "loss: 0.001947  [166320/170046]\n",
            "loss: 0.001844  [168840/170046]\n",
            "reverb decay: avg MSE: 0.003282, avg abs error: 0.0409\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 12\n",
            "loss: 0.001781  [  0/170046]\n",
            "loss: 0.002003  [2520/170046]\n",
            "loss: 0.001803  [5040/170046]\n",
            "loss: 0.002022  [7560/170046]\n",
            "loss: 0.001284  [10080/170046]\n",
            "loss: 0.002596  [12600/170046]\n",
            "loss: 0.001822  [15120/170046]\n",
            "loss: 0.001951  [17640/170046]\n",
            "loss: 0.002390  [20160/170046]\n",
            "loss: 0.001828  [22680/170046]\n",
            "loss: 0.002239  [25200/170046]\n",
            "loss: 0.002307  [27720/170046]\n",
            "loss: 0.001831  [30240/170046]\n",
            "loss: 0.001161  [32760/170046]\n",
            "loss: 0.001817  [35280/170046]\n",
            "loss: 0.001793  [37800/170046]\n",
            "loss: 0.001574  [40320/170046]\n",
            "loss: 0.001653  [42840/170046]\n",
            "loss: 0.001624  [45360/170046]\n",
            "loss: 0.001400  [47880/170046]\n",
            "loss: 0.002045  [50400/170046]\n",
            "loss: 0.001561  [52920/170046]\n",
            "loss: 0.001490  [55440/170046]\n",
            "loss: 0.001631  [57960/170046]\n",
            "loss: 0.001711  [60480/170046]\n",
            "loss: 0.002582  [63000/170046]\n",
            "loss: 0.001533  [65520/170046]\n",
            "loss: 0.001231  [68040/170046]\n",
            "loss: 0.001906  [70560/170046]\n",
            "loss: 0.002106  [73080/170046]\n",
            "loss: 0.001595  [75600/170046]\n",
            "loss: 0.001708  [78120/170046]\n",
            "loss: 0.001890  [80640/170046]\n",
            "loss: 0.001841  [83160/170046]\n",
            "loss: 0.001674  [85680/170046]\n",
            "loss: 0.003079  [88200/170046]\n",
            "loss: 0.001600  [90720/170046]\n",
            "loss: 0.001620  [93240/170046]\n",
            "loss: 0.001573  [95760/170046]\n",
            "loss: 0.002010  [98280/170046]\n",
            "loss: 0.002794  [100800/170046]\n",
            "loss: 0.002753  [103320/170046]\n",
            "loss: 0.001551  [105840/170046]\n",
            "loss: 0.001861  [108360/170046]\n",
            "loss: 0.001532  [110880/170046]\n",
            "loss: 0.001979  [113400/170046]\n",
            "loss: 0.001690  [115920/170046]\n",
            "loss: 0.001990  [118440/170046]\n",
            "loss: 0.001812  [120960/170046]\n",
            "loss: 0.001905  [123480/170046]\n",
            "loss: 0.001613  [126000/170046]\n",
            "loss: 0.002069  [128520/170046]\n",
            "loss: 0.001593  [131040/170046]\n",
            "loss: 0.001227  [133560/170046]\n",
            "loss: 0.002445  [136080/170046]\n",
            "loss: 0.001569  [138600/170046]\n",
            "loss: 0.002072  [141120/170046]\n",
            "loss: 0.001532  [143640/170046]\n",
            "loss: 0.001966  [146160/170046]\n",
            "loss: 0.002132  [148680/170046]\n",
            "loss: 0.001893  [151200/170046]\n",
            "loss: 0.001446  [153720/170046]\n",
            "loss: 0.001375  [156240/170046]\n",
            "loss: 0.003465  [158760/170046]\n",
            "loss: 0.002316  [161280/170046]\n",
            "loss: 0.001583  [163800/170046]\n",
            "loss: 0.001839  [166320/170046]\n",
            "loss: 0.001668  [168840/170046]\n",
            "reverb decay: avg MSE: 0.002940, avg abs error: 0.0371\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 13\n",
            "loss: 0.001565  [  0/170046]\n",
            "loss: 0.002140  [2520/170046]\n",
            "loss: 0.001073  [5040/170046]\n",
            "loss: 0.001307  [7560/170046]\n",
            "loss: 0.001671  [10080/170046]\n",
            "loss: 0.001918  [12600/170046]\n",
            "loss: 0.002144  [15120/170046]\n",
            "loss: 0.002075  [17640/170046]\n",
            "loss: 0.002904  [20160/170046]\n",
            "loss: 0.002378  [22680/170046]\n",
            "loss: 0.001499  [25200/170046]\n",
            "loss: 0.001517  [27720/170046]\n",
            "loss: 0.001983  [30240/170046]\n",
            "loss: 0.002029  [32760/170046]\n",
            "loss: 0.001967  [35280/170046]\n",
            "loss: 0.001225  [37800/170046]\n",
            "loss: 0.001608  [40320/170046]\n",
            "loss: 0.001728  [42840/170046]\n",
            "loss: 0.001936  [45360/170046]\n",
            "loss: 0.001359  [47880/170046]\n",
            "loss: 0.001959  [50400/170046]\n",
            "loss: 0.001932  [52920/170046]\n",
            "loss: 0.001367  [55440/170046]\n",
            "loss: 0.001778  [57960/170046]\n",
            "loss: 0.001768  [60480/170046]\n",
            "loss: 0.002899  [63000/170046]\n",
            "loss: 0.001424  [65520/170046]\n",
            "loss: 0.002013  [68040/170046]\n",
            "loss: 0.001450  [70560/170046]\n",
            "loss: 0.003222  [73080/170046]\n",
            "loss: 0.002110  [75600/170046]\n",
            "loss: 0.001962  [78120/170046]\n",
            "loss: 0.001823  [80640/170046]\n",
            "loss: 0.002176  [83160/170046]\n",
            "loss: 0.002254  [85680/170046]\n",
            "loss: 0.001485  [88200/170046]\n",
            "loss: 0.002712  [90720/170046]\n",
            "loss: 0.001733  [93240/170046]\n",
            "loss: 0.001864  [95760/170046]\n",
            "loss: 0.002329  [98280/170046]\n",
            "loss: 0.001739  [100800/170046]\n",
            "loss: 0.002003  [103320/170046]\n",
            "loss: 0.001339  [105840/170046]\n",
            "loss: 0.002439  [108360/170046]\n",
            "loss: 0.001555  [110880/170046]\n",
            "loss: 0.001802  [113400/170046]\n",
            "loss: 0.003006  [115920/170046]\n",
            "loss: 0.002034  [118440/170046]\n",
            "loss: 0.001839  [120960/170046]\n",
            "loss: 0.001787  [123480/170046]\n",
            "loss: 0.001426  [126000/170046]\n",
            "loss: 0.002669  [128520/170046]\n",
            "loss: 0.001786  [131040/170046]\n",
            "loss: 0.002015  [133560/170046]\n",
            "loss: 0.002115  [136080/170046]\n",
            "loss: 0.001729  [138600/170046]\n",
            "loss: 0.002467  [141120/170046]\n",
            "loss: 0.001571  [143640/170046]\n",
            "loss: 0.001581  [146160/170046]\n",
            "loss: 0.001995  [148680/170046]\n",
            "loss: 0.001478  [151200/170046]\n",
            "loss: 0.001940  [153720/170046]\n",
            "loss: 0.001481  [156240/170046]\n",
            "loss: 0.002005  [158760/170046]\n",
            "loss: 0.001530  [161280/170046]\n",
            "loss: 0.002390  [163800/170046]\n",
            "loss: 0.002078  [166320/170046]\n",
            "loss: 0.002045  [168840/170046]\n",
            "reverb decay: avg MSE: 0.002824, avg abs error: 0.0359\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 14\n",
            "loss: 0.001257  [  0/170046]\n",
            "loss: 0.002501  [2520/170046]\n",
            "loss: 0.001746  [5040/170046]\n",
            "loss: 0.001450  [7560/170046]\n",
            "loss: 0.002378  [10080/170046]\n",
            "loss: 0.001534  [12600/170046]\n",
            "loss: 0.001614  [15120/170046]\n",
            "loss: 0.001925  [17640/170046]\n",
            "loss: 0.001809  [20160/170046]\n",
            "loss: 0.002195  [22680/170046]\n",
            "loss: 0.001966  [25200/170046]\n",
            "loss: 0.001834  [27720/170046]\n",
            "loss: 0.001649  [30240/170046]\n",
            "loss: 0.001535  [32760/170046]\n",
            "loss: 0.001450  [35280/170046]\n",
            "loss: 0.002538  [37800/170046]\n",
            "loss: 0.001709  [40320/170046]\n",
            "loss: 0.002040  [42840/170046]\n",
            "loss: 0.001638  [45360/170046]\n",
            "loss: 0.001818  [47880/170046]\n",
            "loss: 0.002201  [50400/170046]\n",
            "loss: 0.001366  [52920/170046]\n",
            "loss: 0.001754  [55440/170046]\n",
            "loss: 0.001963  [57960/170046]\n",
            "loss: 0.001470  [60480/170046]\n",
            "loss: 0.000973  [63000/170046]\n",
            "loss: 0.002278  [65520/170046]\n",
            "loss: 0.001716  [68040/170046]\n",
            "loss: 0.002926  [70560/170046]\n",
            "loss: 0.002196  [73080/170046]\n",
            "loss: 0.001615  [75600/170046]\n",
            "loss: 0.001841  [78120/170046]\n",
            "loss: 0.001676  [80640/170046]\n",
            "loss: 0.001411  [83160/170046]\n",
            "loss: 0.002357  [85680/170046]\n",
            "loss: 0.001660  [88200/170046]\n",
            "loss: 0.002130  [90720/170046]\n",
            "loss: 0.001697  [93240/170046]\n",
            "loss: 0.002227  [95760/170046]\n",
            "loss: 0.002493  [98280/170046]\n",
            "loss: 0.001819  [100800/170046]\n",
            "loss: 0.002152  [103320/170046]\n",
            "loss: 0.001616  [105840/170046]\n",
            "loss: 0.002203  [108360/170046]\n",
            "loss: 0.002794  [110880/170046]\n",
            "loss: 0.002506  [113400/170046]\n",
            "loss: 0.001193  [115920/170046]\n",
            "loss: 0.001729  [118440/170046]\n",
            "loss: 0.001328  [120960/170046]\n",
            "loss: 0.001933  [123480/170046]\n",
            "loss: 0.001397  [126000/170046]\n",
            "loss: 0.002904  [128520/170046]\n",
            "loss: 0.001395  [131040/170046]\n",
            "loss: 0.001420  [133560/170046]\n",
            "loss: 0.001684  [136080/170046]\n",
            "loss: 0.001430  [138600/170046]\n",
            "loss: 0.002067  [141120/170046]\n",
            "loss: 0.002240  [143640/170046]\n",
            "loss: 0.002231  [146160/170046]\n",
            "loss: 0.002500  [148680/170046]\n",
            "loss: 0.001549  [151200/170046]\n",
            "loss: 0.001452  [153720/170046]\n",
            "loss: 0.002395  [156240/170046]\n",
            "loss: 0.001733  [158760/170046]\n",
            "loss: 0.001616  [161280/170046]\n",
            "loss: 0.002003  [163800/170046]\n",
            "loss: 0.002174  [166320/170046]\n",
            "loss: 0.001353  [168840/170046]\n",
            "reverb decay: avg MSE: 0.003026, avg abs error: 0.0385\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 15\n",
            "loss: 0.001845  [  0/170046]\n",
            "loss: 0.002429  [2520/170046]\n",
            "loss: 0.001593  [5040/170046]\n",
            "loss: 0.001342  [7560/170046]\n",
            "loss: 0.001609  [10080/170046]\n",
            "loss: 0.002349  [12600/170046]\n",
            "loss: 0.002224  [15120/170046]\n",
            "loss: 0.002113  [17640/170046]\n",
            "loss: 0.001213  [20160/170046]\n",
            "loss: 0.001951  [22680/170046]\n",
            "loss: 0.001818  [25200/170046]\n",
            "loss: 0.001964  [27720/170046]\n",
            "loss: 0.001829  [30240/170046]\n",
            "loss: 0.001279  [32760/170046]\n",
            "loss: 0.002238  [35280/170046]\n",
            "loss: 0.002288  [37800/170046]\n",
            "loss: 0.002476  [40320/170046]\n",
            "loss: 0.002547  [42840/170046]\n",
            "loss: 0.001718  [45360/170046]\n",
            "loss: 0.002311  [47880/170046]\n",
            "loss: 0.001411  [50400/170046]\n",
            "loss: 0.001399  [52920/170046]\n",
            "loss: 0.001359  [55440/170046]\n",
            "loss: 0.002017  [57960/170046]\n",
            "loss: 0.001486  [60480/170046]\n",
            "loss: 0.001996  [63000/170046]\n",
            "loss: 0.002071  [65520/170046]\n",
            "loss: 0.001596  [68040/170046]\n",
            "loss: 0.001560  [70560/170046]\n",
            "loss: 0.001554  [73080/170046]\n",
            "loss: 0.002308  [75600/170046]\n",
            "loss: 0.001572  [78120/170046]\n",
            "loss: 0.002134  [80640/170046]\n",
            "loss: 0.002095  [83160/170046]\n",
            "loss: 0.001845  [85680/170046]\n",
            "loss: 0.002077  [88200/170046]\n",
            "loss: 0.002533  [90720/170046]\n",
            "loss: 0.002061  [93240/170046]\n",
            "loss: 0.001749  [95760/170046]\n",
            "loss: 0.001559  [98280/170046]\n",
            "loss: 0.001857  [100800/170046]\n",
            "loss: 0.002291  [103320/170046]\n",
            "loss: 0.002042  [105840/170046]\n",
            "loss: 0.001542  [108360/170046]\n",
            "loss: 0.002315  [110880/170046]\n",
            "loss: 0.001719  [113400/170046]\n",
            "loss: 0.002298  [115920/170046]\n",
            "loss: 0.001791  [118440/170046]\n",
            "loss: 0.002402  [120960/170046]\n",
            "loss: 0.002030  [123480/170046]\n",
            "loss: 0.001700  [126000/170046]\n",
            "loss: 0.002204  [128520/170046]\n",
            "loss: 0.001705  [131040/170046]\n",
            "loss: 0.001888  [133560/170046]\n",
            "loss: 0.001600  [136080/170046]\n",
            "loss: 0.002115  [138600/170046]\n",
            "loss: 0.001271  [141120/170046]\n",
            "loss: 0.001185  [143640/170046]\n",
            "loss: 0.001225  [146160/170046]\n",
            "loss: 0.001932  [148680/170046]\n",
            "loss: 0.001461  [151200/170046]\n",
            "loss: 0.001716  [153720/170046]\n",
            "loss: 0.001265  [156240/170046]\n",
            "loss: 0.001241  [158760/170046]\n",
            "loss: 0.001222  [161280/170046]\n",
            "loss: 0.001875  [163800/170046]\n",
            "loss: 0.001878  [166320/170046]\n",
            "loss: 0.001510  [168840/170046]\n",
            "reverb decay: avg MSE: 0.002853, avg abs error: 0.0365\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Finished training\n",
            "reverb decay: avg MSE: 0.002895, avg abs error: 0.0369\n"
          ]
        }
      ],
      "source": [
        "from src.util import plot_violin\n",
        "import numpy as np\n",
        "\n",
        "WEIGHTS_DIR = \"_weights/\"\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 15\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device {device}\")\n",
        "\n",
        "error = []\n",
        "\n",
        "fx = EFFECT_MAP.index(\"reverb\")\n",
        "\n",
        "WEIGHTS_PATH = os.path.join(WEIGHTS_DIR, EXPERIMENT_NAME + \"_\" + str(fx))\n",
        "\n",
        "if not os.path.exists('%s' % WEIGHTS_DIR):\n",
        "    os.makedirs('%s' % WEIGHTS_DIR)\n",
        "\n",
        "fxData = load_train_data(fx)\n",
        "# fxData, _ = torch.utils.data.random_split(fxData, lengths=[0.01, 0.99])\n",
        "\n",
        "train_dataloader, test_dataloader = split_data(fxData)\n",
        "val_dataloader = load_evaluation_data(fx)\n",
        "\n",
        "# construct model and assign it to device\n",
        "cnn = model.Extractor().to(device)\n",
        "\n",
        "# if fx == 0:\n",
        "#     signal, _, _, _, _ = fxData[0]\n",
        "#     print(f\"There are {len(fxData)} samples in the dataset.\")\n",
        "#     print(f\"Shape of signal: {signal.shape}\")\n",
        "\n",
        "#     print(\"input feature:\")\n",
        "#     log_writer.add_figure(\"Input Feature\", plot_spectrogram(signal[0], title=\"MFCC\"))\n",
        "#     log_writer.add_graph(cnn, signal.unsqueeze_(0))\n",
        "\n",
        "# initialise loss funtion + optimiser\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "optimiser = torch.optim.Adam(cnn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# train model\n",
        "train.train(cnn,\n",
        "            train_dataloader,\n",
        "            test_dataloader,\n",
        "            loss_fn,\n",
        "            optimiser,\n",
        "            device,\n",
        "            log_writer,\n",
        "            EPOCHS,\n",
        "            WEIGHTS_PATH,\n",
        "            effect=fx)\n",
        "\n",
        "_, _, log = train.test(cnn, val_dataloader, device, effect=fx)\n",
        "for _, data in enumerate(log):\n",
        "    error.append(data[3])\n",
        "\n",
        "arr = np.array(error)\n",
        "np.save(EVU_DIR + EXPERIMENT_NAME + \"_\" + str(fx) + \"_evaluation.npy\", arr)\n",
        "\n",
        "# log_writer.add_figure(\"Error Box\", \n",
        "#                       plot_violin(error, title=\"Error\", labels=EFFECT_MAP, ylabel=\"parameter value\", outlier=True))\n",
        "\n",
        "log_writer.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "torchaudio-tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
