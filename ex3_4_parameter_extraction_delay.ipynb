{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Distortion Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMiPTdSA8C2F",
        "outputId": "a297ba92-a5c6-4015-d92d-1e9487daf896"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import os"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gIdrZtW_JYf",
        "outputId": "c1bb6969-15a9-44eb-8cdd-8d79c89208f2"
      },
      "outputs": [],
      "source": [
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050*3\n",
        "\n",
        "mfcc = torchaudio.transforms.MFCC(\n",
        "    sample_rate = SAMPLE_RATE, \n",
        "    n_mfcc = 64,\n",
        "    melkwargs = {\n",
        "        \"n_fft\": 1024,\n",
        "        \"hop_length\": 1024,\n",
        "        \"n_mels\": 64,\n",
        "        \"center\": False})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Functions for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.gtfxdataset import GtFxDataset\n",
        "from src.util import plot_spectrogram\n",
        "from src.extrector import train\n",
        "from src.extrector import model\n",
        "from torch import nn\n",
        "\n",
        "AUDIO_DIR = \"_assets/DATASET/GT-FX-C53/\"\n",
        "ANNOTATIONS_FILE = os.path.join(AUDIO_DIR, \"train.csv\")\n",
        "EVU_ANNOTATIONS_FILE = os.path.join(AUDIO_DIR, \"evaluation.csv\")\n",
        "EFFECT_MAP = [\"distortion\", \"chorus\", \"tremolo\", \"delay\", \"reverb\"]\n",
        "\n",
        "def load_train_data(effect):\n",
        "    \n",
        "    fxData = GtFxDataset(ANNOTATIONS_FILE,\n",
        "                        AUDIO_DIR,\n",
        "                        mfcc,\n",
        "                        SAMPLE_RATE,\n",
        "                        NUM_SAMPLES,\n",
        "                        device,\n",
        "                        effect=EFFECT_MAP[effect])\n",
        "    return fxData\n",
        "\n",
        "def load_evaluation_data(effect):\n",
        "\n",
        "    evuData = GtFxDataset(EVU_ANNOTATIONS_FILE,\n",
        "                        AUDIO_DIR,\n",
        "                        mfcc,\n",
        "                        SAMPLE_RATE,\n",
        "                        NUM_SAMPLES,\n",
        "                        device,\n",
        "                        effect=EFFECT_MAP[effect])\n",
        "\n",
        "    BATCH_SIZE = round(len(evuData) / 1500)\n",
        "    val_dataloader = train.create_data_loader(evuData, BATCH_SIZE)\n",
        "    return val_dataloader\n",
        "\n",
        "def split_data(data):\n",
        "\n",
        "    BATCH_SIZE = round(len(data) / 1500)\n",
        "\n",
        "    split_ratio = [0.9, 0.1]\n",
        "    train_set, test_set = torch.utils.data.random_split(data, lengths=split_ratio)\n",
        "\n",
        "    train_dataloader = train.create_data_loader(train_set, BATCH_SIZE)\n",
        "    test_dataloader = train.create_data_loader(test_set, BATCH_SIZE)\n",
        "\n",
        "    return train_dataloader, test_dataloader   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Add Tensorboard to record data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "EXPERIMENT_NAME = \"c53_parameter\"\n",
        "LOG_DIR = \"_log/\" + EXPERIMENT_NAME\n",
        "EVU_DIR = \"_log/Evaluation/\"\n",
        "\n",
        "if not os.path.exists('%s' % LOG_DIR):\n",
        "    os.makedirs('%s' % LOG_DIR)\n",
        "\n",
        "if not os.path.exists('%s' % EVU_DIR):\n",
        "    os.makedirs('%s' % EVU_DIR)\n",
        "\n",
        "log_writer = SummaryWriter(LOG_DIR)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8O9r5WI2zuI",
        "outputId": "ed2a4ebd-f644-4e35-8fda-86170924dfe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cpu\n",
            "Epoch 1\n",
            "loss: 0.107253  [  0/170046]\n",
            "loss: 0.067041  [2520/170046]\n",
            "loss: 0.014602  [5040/170046]\n",
            "loss: 0.008162  [7560/170046]\n",
            "loss: 0.009882  [10080/170046]\n",
            "loss: 0.006568  [12600/170046]\n",
            "loss: 0.005346  [15120/170046]\n",
            "loss: 0.004322  [17640/170046]\n",
            "loss: 0.002773  [20160/170046]\n",
            "loss: 0.003772  [22680/170046]\n",
            "loss: 0.002471  [25200/170046]\n",
            "loss: 0.001926  [27720/170046]\n",
            "loss: 0.001518  [30240/170046]\n",
            "loss: 0.002792  [32760/170046]\n",
            "loss: 0.001901  [35280/170046]\n",
            "loss: 0.002103  [37800/170046]\n",
            "loss: 0.001599  [40320/170046]\n",
            "loss: 0.002337  [42840/170046]\n",
            "loss: 0.001759  [45360/170046]\n",
            "loss: 0.002523  [47880/170046]\n",
            "loss: 0.002142  [50400/170046]\n",
            "loss: 0.000862  [52920/170046]\n",
            "loss: 0.000976  [55440/170046]\n",
            "loss: 0.001354  [57960/170046]\n",
            "loss: 0.001600  [60480/170046]\n",
            "loss: 0.000714  [63000/170046]\n",
            "loss: 0.001272  [65520/170046]\n",
            "loss: 0.002288  [68040/170046]\n",
            "loss: 0.000820  [70560/170046]\n",
            "loss: 0.000849  [73080/170046]\n",
            "loss: 0.002398  [75600/170046]\n",
            "loss: 0.000991  [78120/170046]\n",
            "loss: 0.001963  [80640/170046]\n",
            "loss: 0.002126  [83160/170046]\n",
            "loss: 0.001011  [85680/170046]\n",
            "loss: 0.001181  [88200/170046]\n",
            "loss: 0.001030  [90720/170046]\n",
            "loss: 0.000946  [93240/170046]\n",
            "loss: 0.001469  [95760/170046]\n",
            "loss: 0.000919  [98280/170046]\n",
            "loss: 0.001242  [100800/170046]\n",
            "loss: 0.003450  [103320/170046]\n",
            "loss: 0.001462  [105840/170046]\n",
            "loss: 0.000770  [108360/170046]\n",
            "loss: 0.000668  [110880/170046]\n",
            "loss: 0.000946  [113400/170046]\n",
            "loss: 0.001458  [115920/170046]\n",
            "loss: 0.000854  [118440/170046]\n",
            "loss: 0.001048  [120960/170046]\n",
            "loss: 0.000657  [123480/170046]\n",
            "loss: 0.000770  [126000/170046]\n",
            "loss: 0.001197  [128520/170046]\n",
            "loss: 0.000858  [131040/170046]\n",
            "loss: 0.001893  [133560/170046]\n",
            "loss: 0.002939  [136080/170046]\n",
            "loss: 0.001098  [138600/170046]\n",
            "loss: 0.000957  [141120/170046]\n",
            "loss: 0.000989  [143640/170046]\n",
            "loss: 0.001061  [146160/170046]\n",
            "loss: 0.000899  [148680/170046]\n",
            "loss: 0.000789  [151200/170046]\n",
            "loss: 0.001218  [153720/170046]\n",
            "loss: 0.001647  [156240/170046]\n",
            "loss: 0.001334  [158760/170046]\n",
            "loss: 0.000873  [161280/170046]\n",
            "loss: 0.000598  [163800/170046]\n",
            "loss: 0.001196  [166320/170046]\n",
            "loss: 0.000988  [168840/170046]\n",
            "delay time: avg MSE: 0.000834, avg abs error: 0.0224\n",
            "learning rate: 0.001000 -> 0.000910\n",
            "---------------------------\n",
            "\n",
            "Epoch 2\n",
            "loss: 0.000923  [  0/170046]\n",
            "loss: 0.001060  [2520/170046]\n",
            "loss: 0.000940  [5040/170046]\n",
            "loss: 0.000820  [7560/170046]\n",
            "loss: 0.000724  [10080/170046]\n",
            "loss: 0.000722  [12600/170046]\n",
            "loss: 0.000698  [15120/170046]\n",
            "loss: 0.000902  [17640/170046]\n",
            "loss: 0.000808  [20160/170046]\n",
            "loss: 0.000525  [22680/170046]\n",
            "loss: 0.000474  [25200/170046]\n",
            "loss: 0.001021  [27720/170046]\n",
            "loss: 0.001330  [30240/170046]\n",
            "loss: 0.000736  [32760/170046]\n",
            "loss: 0.001158  [35280/170046]\n",
            "loss: 0.000417  [37800/170046]\n",
            "loss: 0.001553  [40320/170046]\n",
            "loss: 0.000537  [42840/170046]\n",
            "loss: 0.000744  [45360/170046]\n",
            "loss: 0.000648  [47880/170046]\n",
            "loss: 0.001064  [50400/170046]\n",
            "loss: 0.000578  [52920/170046]\n",
            "loss: 0.000811  [55440/170046]\n",
            "loss: 0.000691  [57960/170046]\n",
            "loss: 0.000548  [60480/170046]\n",
            "loss: 0.000808  [63000/170046]\n",
            "loss: 0.001014  [65520/170046]\n",
            "loss: 0.001063  [68040/170046]\n",
            "loss: 0.000517  [70560/170046]\n",
            "loss: 0.000700  [73080/170046]\n",
            "loss: 0.000492  [75600/170046]\n",
            "loss: 0.000679  [78120/170046]\n",
            "loss: 0.001361  [80640/170046]\n",
            "loss: 0.000570  [83160/170046]\n",
            "loss: 0.000637  [85680/170046]\n",
            "loss: 0.000518  [88200/170046]\n",
            "loss: 0.002098  [90720/170046]\n",
            "loss: 0.000438  [93240/170046]\n",
            "loss: 0.000944  [95760/170046]\n",
            "loss: 0.000725  [98280/170046]\n",
            "loss: 0.000556  [100800/170046]\n",
            "loss: 0.000974  [103320/170046]\n",
            "loss: 0.000609  [105840/170046]\n",
            "loss: 0.000547  [108360/170046]\n",
            "loss: 0.000432  [110880/170046]\n",
            "loss: 0.000986  [113400/170046]\n",
            "loss: 0.001157  [115920/170046]\n",
            "loss: 0.000526  [118440/170046]\n",
            "loss: 0.000758  [120960/170046]\n",
            "loss: 0.000493  [123480/170046]\n",
            "loss: 0.000551  [126000/170046]\n",
            "loss: 0.000544  [128520/170046]\n",
            "loss: 0.000711  [131040/170046]\n",
            "loss: 0.000490  [133560/170046]\n",
            "loss: 0.000544  [136080/170046]\n",
            "loss: 0.000649  [138600/170046]\n",
            "loss: 0.000324  [141120/170046]\n",
            "loss: 0.000696  [143640/170046]\n",
            "loss: 0.000461  [146160/170046]\n",
            "loss: 0.000338  [148680/170046]\n",
            "loss: 0.000699  [151200/170046]\n",
            "loss: 0.000460  [153720/170046]\n",
            "loss: 0.000371  [156240/170046]\n",
            "loss: 0.000782  [158760/170046]\n",
            "loss: 0.000650  [161280/170046]\n",
            "loss: 0.000396  [163800/170046]\n",
            "loss: 0.000677  [166320/170046]\n",
            "loss: 0.000603  [168840/170046]\n",
            "delay time: avg MSE: 0.000586, avg abs error: 0.0179\n",
            "learning rate: 0.000910 -> 0.000820\n",
            "---------------------------\n",
            "\n",
            "Epoch 3\n",
            "loss: 0.000584  [  0/170046]\n",
            "loss: 0.000612  [2520/170046]\n",
            "loss: 0.000486  [5040/170046]\n",
            "loss: 0.000401  [7560/170046]\n",
            "loss: 0.000657  [10080/170046]\n",
            "loss: 0.000468  [12600/170046]\n",
            "loss: 0.000713  [15120/170046]\n",
            "loss: 0.000817  [17640/170046]\n",
            "loss: 0.000261  [20160/170046]\n",
            "loss: 0.000456  [22680/170046]\n",
            "loss: 0.000502  [25200/170046]\n",
            "loss: 0.000359  [27720/170046]\n",
            "loss: 0.000678  [30240/170046]\n",
            "loss: 0.000382  [32760/170046]\n",
            "loss: 0.000386  [35280/170046]\n",
            "loss: 0.000689  [37800/170046]\n",
            "loss: 0.001648  [40320/170046]\n",
            "loss: 0.000434  [42840/170046]\n",
            "loss: 0.000396  [45360/170046]\n",
            "loss: 0.000523  [47880/170046]\n",
            "loss: 0.000692  [50400/170046]\n",
            "loss: 0.000507  [52920/170046]\n",
            "loss: 0.000371  [55440/170046]\n",
            "loss: 0.000723  [57960/170046]\n",
            "loss: 0.000686  [60480/170046]\n",
            "loss: 0.000396  [63000/170046]\n",
            "loss: 0.000454  [65520/170046]\n",
            "loss: 0.000611  [68040/170046]\n",
            "loss: 0.000686  [70560/170046]\n",
            "loss: 0.000371  [73080/170046]\n",
            "loss: 0.000401  [75600/170046]\n",
            "loss: 0.000336  [78120/170046]\n",
            "loss: 0.000674  [80640/170046]\n",
            "loss: 0.000773  [83160/170046]\n",
            "loss: 0.000424  [85680/170046]\n",
            "loss: 0.000270  [88200/170046]\n",
            "loss: 0.000335  [90720/170046]\n",
            "loss: 0.000551  [93240/170046]\n",
            "loss: 0.000543  [95760/170046]\n",
            "loss: 0.000271  [98280/170046]\n",
            "loss: 0.000663  [100800/170046]\n",
            "loss: 0.000447  [103320/170046]\n",
            "loss: 0.000445  [105840/170046]\n",
            "loss: 0.000300  [108360/170046]\n",
            "loss: 0.000273  [110880/170046]\n",
            "loss: 0.000512  [113400/170046]\n",
            "loss: 0.000428  [115920/170046]\n",
            "loss: 0.000316  [118440/170046]\n",
            "loss: 0.000498  [120960/170046]\n",
            "loss: 0.000592  [123480/170046]\n",
            "loss: 0.000439  [126000/170046]\n",
            "loss: 0.000234  [128520/170046]\n",
            "loss: 0.000287  [131040/170046]\n",
            "loss: 0.000555  [133560/170046]\n",
            "loss: 0.000283  [136080/170046]\n",
            "loss: 0.000408  [138600/170046]\n",
            "loss: 0.000486  [141120/170046]\n",
            "loss: 0.000400  [143640/170046]\n",
            "loss: 0.000356  [146160/170046]\n",
            "loss: 0.000360  [148680/170046]\n",
            "loss: 0.000365  [151200/170046]\n",
            "loss: 0.000254  [153720/170046]\n",
            "loss: 0.000462  [156240/170046]\n",
            "loss: 0.000299  [158760/170046]\n",
            "loss: 0.000647  [161280/170046]\n",
            "loss: 0.000639  [163800/170046]\n",
            "loss: 0.000322  [166320/170046]\n",
            "loss: 0.000435  [168840/170046]\n",
            "delay time: avg MSE: 0.000375, avg abs error: 0.0141\n",
            "learning rate: 0.000820 -> 0.000730\n",
            "---------------------------\n",
            "\n",
            "Epoch 4\n",
            "loss: 0.000570  [  0/170046]\n",
            "loss: 0.000265  [2520/170046]\n",
            "loss: 0.000233  [5040/170046]\n",
            "loss: 0.000335  [7560/170046]\n",
            "loss: 0.000348  [10080/170046]\n",
            "loss: 0.000428  [12600/170046]\n",
            "loss: 0.000421  [15120/170046]\n",
            "loss: 0.000307  [17640/170046]\n",
            "loss: 0.000477  [20160/170046]\n",
            "loss: 0.000300  [22680/170046]\n",
            "loss: 0.000435  [25200/170046]\n",
            "loss: 0.000587  [27720/170046]\n",
            "loss: 0.000368  [30240/170046]\n",
            "loss: 0.000425  [32760/170046]\n",
            "loss: 0.000566  [35280/170046]\n",
            "loss: 0.000254  [37800/170046]\n",
            "loss: 0.000422  [40320/170046]\n",
            "loss: 0.000307  [42840/170046]\n",
            "loss: 0.000441  [45360/170046]\n",
            "loss: 0.000805  [47880/170046]\n",
            "loss: 0.000454  [50400/170046]\n",
            "loss: 0.000599  [52920/170046]\n",
            "loss: 0.000395  [55440/170046]\n",
            "loss: 0.000435  [57960/170046]\n",
            "loss: 0.000598  [60480/170046]\n",
            "loss: 0.000458  [63000/170046]\n",
            "loss: 0.000441  [65520/170046]\n",
            "loss: 0.000334  [68040/170046]\n",
            "loss: 0.000291  [70560/170046]\n",
            "loss: 0.000386  [73080/170046]\n",
            "loss: 0.000219  [75600/170046]\n",
            "loss: 0.000354  [78120/170046]\n",
            "loss: 0.000241  [80640/170046]\n",
            "loss: 0.000632  [83160/170046]\n",
            "loss: 0.000352  [85680/170046]\n",
            "loss: 0.000859  [88200/170046]\n",
            "loss: 0.000361  [90720/170046]\n",
            "loss: 0.000922  [93240/170046]\n",
            "loss: 0.000402  [95760/170046]\n",
            "loss: 0.000241  [98280/170046]\n",
            "loss: 0.000222  [100800/170046]\n",
            "loss: 0.000477  [103320/170046]\n",
            "loss: 0.000567  [105840/170046]\n",
            "loss: 0.000362  [108360/170046]\n",
            "loss: 0.000281  [110880/170046]\n",
            "loss: 0.000543  [113400/170046]\n",
            "loss: 0.000337  [115920/170046]\n",
            "loss: 0.001598  [118440/170046]\n",
            "loss: 0.000424  [120960/170046]\n",
            "loss: 0.000409  [123480/170046]\n",
            "loss: 0.000406  [126000/170046]\n",
            "loss: 0.000555  [128520/170046]\n",
            "loss: 0.000359  [131040/170046]\n",
            "loss: 0.000343  [133560/170046]\n",
            "loss: 0.000334  [136080/170046]\n",
            "loss: 0.000368  [138600/170046]\n",
            "loss: 0.000353  [141120/170046]\n",
            "loss: 0.000305  [143640/170046]\n",
            "loss: 0.000325  [146160/170046]\n",
            "loss: 0.000271  [148680/170046]\n",
            "loss: 0.000336  [151200/170046]\n",
            "loss: 0.000245  [153720/170046]\n",
            "loss: 0.000405  [156240/170046]\n",
            "loss: 0.000551  [158760/170046]\n",
            "loss: 0.000386  [161280/170046]\n",
            "loss: 0.000682  [163800/170046]\n",
            "loss: 0.000242  [166320/170046]\n",
            "loss: 0.000362  [168840/170046]\n",
            "delay time: avg MSE: 0.000320, avg abs error: 0.0124\n",
            "learning rate: 0.000730 -> 0.000640\n",
            "---------------------------\n",
            "\n",
            "Epoch 5\n",
            "loss: 0.000220  [  0/170046]\n",
            "loss: 0.000321  [2520/170046]\n",
            "loss: 0.000246  [5040/170046]\n",
            "loss: 0.000296  [7560/170046]\n",
            "loss: 0.000229  [10080/170046]\n",
            "loss: 0.000194  [12600/170046]\n",
            "loss: 0.000223  [15120/170046]\n",
            "loss: 0.000272  [17640/170046]\n",
            "loss: 0.000409  [20160/170046]\n",
            "loss: 0.000418  [22680/170046]\n",
            "loss: 0.000499  [25200/170046]\n",
            "loss: 0.000291  [27720/170046]\n",
            "loss: 0.000377  [30240/170046]\n",
            "loss: 0.000303  [32760/170046]\n",
            "loss: 0.000220  [35280/170046]\n",
            "loss: 0.000309  [37800/170046]\n",
            "loss: 0.000302  [40320/170046]\n",
            "loss: 0.000518  [42840/170046]\n",
            "loss: 0.000254  [45360/170046]\n",
            "loss: 0.000429  [47880/170046]\n",
            "loss: 0.000238  [50400/170046]\n",
            "loss: 0.000225  [52920/170046]\n",
            "loss: 0.000391  [55440/170046]\n",
            "loss: 0.000221  [57960/170046]\n",
            "loss: 0.000356  [60480/170046]\n",
            "loss: 0.000442  [63000/170046]\n",
            "loss: 0.000387  [65520/170046]\n",
            "loss: 0.000337  [68040/170046]\n",
            "loss: 0.000353  [70560/170046]\n",
            "loss: 0.000212  [73080/170046]\n",
            "loss: 0.000216  [75600/170046]\n",
            "loss: 0.000329  [78120/170046]\n",
            "loss: 0.000169  [80640/170046]\n",
            "loss: 0.000324  [83160/170046]\n",
            "loss: 0.000378  [85680/170046]\n",
            "loss: 0.000408  [88200/170046]\n",
            "loss: 0.000480  [90720/170046]\n",
            "loss: 0.000191  [93240/170046]\n",
            "loss: 0.000227  [95760/170046]\n",
            "loss: 0.000253  [98280/170046]\n",
            "loss: 0.000394  [100800/170046]\n",
            "loss: 0.000357  [103320/170046]\n",
            "loss: 0.000220  [105840/170046]\n",
            "loss: 0.000258  [108360/170046]\n",
            "loss: 0.000283  [110880/170046]\n",
            "loss: 0.000502  [113400/170046]\n",
            "loss: 0.000323  [115920/170046]\n",
            "loss: 0.000391  [118440/170046]\n",
            "loss: 0.000281  [120960/170046]\n",
            "loss: 0.000217  [123480/170046]\n",
            "loss: 0.000323  [126000/170046]\n",
            "loss: 0.000676  [128520/170046]\n",
            "loss: 0.000197  [131040/170046]\n",
            "loss: 0.000227  [133560/170046]\n",
            "loss: 0.000373  [136080/170046]\n",
            "loss: 0.000287  [138600/170046]\n",
            "loss: 0.000294  [141120/170046]\n",
            "loss: 0.000489  [143640/170046]\n",
            "loss: 0.000703  [146160/170046]\n",
            "loss: 0.000394  [148680/170046]\n",
            "loss: 0.000189  [151200/170046]\n",
            "loss: 0.000453  [153720/170046]\n",
            "loss: 0.000184  [156240/170046]\n",
            "loss: 0.000285  [158760/170046]\n",
            "loss: 0.000189  [161280/170046]\n",
            "loss: 0.000193  [163800/170046]\n",
            "loss: 0.000277  [166320/170046]\n",
            "loss: 0.000334  [168840/170046]\n",
            "delay time: avg MSE: 0.000528, avg abs error: 0.0177\n",
            "learning rate: 0.000640 -> 0.000550\n",
            "---------------------------\n",
            "\n",
            "Epoch 6\n",
            "loss: 0.000624  [  0/170046]\n",
            "loss: 0.000270  [2520/170046]\n",
            "loss: 0.000493  [5040/170046]\n",
            "loss: 0.000473  [7560/170046]\n",
            "loss: 0.000457  [10080/170046]\n",
            "loss: 0.000270  [12600/170046]\n",
            "loss: 0.000165  [15120/170046]\n",
            "loss: 0.000209  [17640/170046]\n",
            "loss: 0.000297  [20160/170046]\n",
            "loss: 0.000270  [22680/170046]\n",
            "loss: 0.000433  [25200/170046]\n",
            "loss: 0.000245  [27720/170046]\n",
            "loss: 0.000583  [30240/170046]\n",
            "loss: 0.000257  [32760/170046]\n",
            "loss: 0.000271  [35280/170046]\n",
            "loss: 0.000221  [37800/170046]\n",
            "loss: 0.000205  [40320/170046]\n",
            "loss: 0.000233  [42840/170046]\n",
            "loss: 0.000237  [45360/170046]\n",
            "loss: 0.000433  [47880/170046]\n",
            "loss: 0.000233  [50400/170046]\n",
            "loss: 0.000192  [52920/170046]\n",
            "loss: 0.000393  [55440/170046]\n",
            "loss: 0.000258  [57960/170046]\n",
            "loss: 0.000384  [60480/170046]\n",
            "loss: 0.000202  [63000/170046]\n",
            "loss: 0.000295  [65520/170046]\n",
            "loss: 0.000189  [68040/170046]\n",
            "loss: 0.000250  [70560/170046]\n",
            "loss: 0.000433  [73080/170046]\n",
            "loss: 0.000211  [75600/170046]\n",
            "loss: 0.000421  [78120/170046]\n",
            "loss: 0.000277  [80640/170046]\n",
            "loss: 0.000315  [83160/170046]\n",
            "loss: 0.000421  [85680/170046]\n",
            "loss: 0.000259  [88200/170046]\n",
            "loss: 0.000163  [90720/170046]\n",
            "loss: 0.000263  [93240/170046]\n",
            "loss: 0.000305  [95760/170046]\n",
            "loss: 0.000232  [98280/170046]\n",
            "loss: 0.000259  [100800/170046]\n",
            "loss: 0.000273  [103320/170046]\n",
            "loss: 0.000515  [105840/170046]\n",
            "loss: 0.000276  [108360/170046]\n",
            "loss: 0.000268  [110880/170046]\n",
            "loss: 0.000186  [113400/170046]\n",
            "loss: 0.000166  [115920/170046]\n",
            "loss: 0.000241  [118440/170046]\n",
            "loss: 0.000393  [120960/170046]\n",
            "loss: 0.000286  [123480/170046]\n",
            "loss: 0.000435  [126000/170046]\n",
            "loss: 0.000231  [128520/170046]\n",
            "loss: 0.000294  [131040/170046]\n",
            "loss: 0.000337  [133560/170046]\n",
            "loss: 0.000397  [136080/170046]\n",
            "loss: 0.000228  [138600/170046]\n",
            "loss: 0.000264  [141120/170046]\n",
            "loss: 0.000179  [143640/170046]\n",
            "loss: 0.000591  [146160/170046]\n",
            "loss: 0.000259  [148680/170046]\n",
            "loss: 0.000168  [151200/170046]\n",
            "loss: 0.000372  [153720/170046]\n",
            "loss: 0.000302  [156240/170046]\n",
            "loss: 0.000237  [158760/170046]\n",
            "loss: 0.000143  [161280/170046]\n",
            "loss: 0.000217  [163800/170046]\n",
            "loss: 0.000220  [166320/170046]\n",
            "loss: 0.000286  [168840/170046]\n",
            "delay time: avg MSE: 0.000249, avg abs error: 0.0116\n",
            "learning rate: 0.000550 -> 0.000460\n",
            "---------------------------\n",
            "\n",
            "Epoch 7\n",
            "loss: 0.000193  [  0/170046]\n",
            "loss: 0.000116  [2520/170046]\n",
            "loss: 0.000174  [5040/170046]\n",
            "loss: 0.000165  [7560/170046]\n",
            "loss: 0.000178  [10080/170046]\n",
            "loss: 0.000191  [12600/170046]\n",
            "loss: 0.000147  [15120/170046]\n",
            "loss: 0.000250  [17640/170046]\n",
            "loss: 0.000158  [20160/170046]\n",
            "loss: 0.000313  [22680/170046]\n",
            "loss: 0.000297  [25200/170046]\n",
            "loss: 0.000151  [27720/170046]\n",
            "loss: 0.000452  [30240/170046]\n",
            "loss: 0.000257  [32760/170046]\n",
            "loss: 0.000165  [35280/170046]\n",
            "loss: 0.000164  [37800/170046]\n",
            "loss: 0.000164  [40320/170046]\n",
            "loss: 0.000387  [42840/170046]\n",
            "loss: 0.000187  [45360/170046]\n",
            "loss: 0.000237  [47880/170046]\n",
            "loss: 0.000279  [50400/170046]\n",
            "loss: 0.000197  [52920/170046]\n",
            "loss: 0.000265  [55440/170046]\n",
            "loss: 0.000370  [57960/170046]\n",
            "loss: 0.000220  [60480/170046]\n",
            "loss: 0.000250  [63000/170046]\n",
            "loss: 0.000340  [65520/170046]\n",
            "loss: 0.000206  [68040/170046]\n",
            "loss: 0.000128  [70560/170046]\n",
            "loss: 0.000307  [73080/170046]\n",
            "loss: 0.000182  [75600/170046]\n",
            "loss: 0.000224  [78120/170046]\n",
            "loss: 0.000175  [80640/170046]\n",
            "loss: 0.000189  [83160/170046]\n",
            "loss: 0.000154  [85680/170046]\n",
            "loss: 0.000191  [88200/170046]\n",
            "loss: 0.000267  [90720/170046]\n",
            "loss: 0.000365  [93240/170046]\n",
            "loss: 0.000123  [95760/170046]\n",
            "loss: 0.000228  [98280/170046]\n",
            "loss: 0.000171  [100800/170046]\n",
            "loss: 0.000268  [103320/170046]\n",
            "loss: 0.000365  [105840/170046]\n",
            "loss: 0.000293  [108360/170046]\n",
            "loss: 0.000163  [110880/170046]\n",
            "loss: 0.000232  [113400/170046]\n",
            "loss: 0.000270  [115920/170046]\n",
            "loss: 0.000200  [118440/170046]\n",
            "loss: 0.000216  [120960/170046]\n",
            "loss: 0.000175  [123480/170046]\n",
            "loss: 0.000250  [126000/170046]\n",
            "loss: 0.000255  [128520/170046]\n",
            "loss: 0.000206  [131040/170046]\n",
            "loss: 0.000194  [133560/170046]\n",
            "loss: 0.000258  [136080/170046]\n",
            "loss: 0.000258  [138600/170046]\n",
            "loss: 0.000185  [141120/170046]\n",
            "loss: 0.000210  [143640/170046]\n",
            "loss: 0.000199  [146160/170046]\n",
            "loss: 0.000195  [148680/170046]\n",
            "loss: 0.000263  [151200/170046]\n",
            "loss: 0.000226  [153720/170046]\n",
            "loss: 0.000236  [156240/170046]\n",
            "loss: 0.000239  [158760/170046]\n",
            "loss: 0.000257  [161280/170046]\n",
            "loss: 0.000349  [163800/170046]\n",
            "loss: 0.000133  [166320/170046]\n",
            "loss: 0.000334  [168840/170046]\n",
            "delay time: avg MSE: 0.000317, avg abs error: 0.014\n",
            "learning rate: 0.000460 -> 0.000370\n",
            "---------------------------\n",
            "\n",
            "Epoch 8\n",
            "loss: 0.000329  [  0/170046]\n",
            "loss: 0.000249  [2520/170046]\n",
            "loss: 0.000161  [5040/170046]\n",
            "loss: 0.000140  [7560/170046]\n",
            "loss: 0.000256  [10080/170046]\n",
            "loss: 0.000194  [12600/170046]\n",
            "loss: 0.000228  [15120/170046]\n",
            "loss: 0.000143  [17640/170046]\n",
            "loss: 0.000185  [20160/170046]\n",
            "loss: 0.000192  [22680/170046]\n",
            "loss: 0.000183  [25200/170046]\n",
            "loss: 0.000136  [27720/170046]\n",
            "loss: 0.000164  [30240/170046]\n",
            "loss: 0.000257  [32760/170046]\n",
            "loss: 0.000173  [35280/170046]\n",
            "loss: 0.000358  [37800/170046]\n",
            "loss: 0.000235  [40320/170046]\n",
            "loss: 0.000184  [42840/170046]\n",
            "loss: 0.000127  [45360/170046]\n",
            "loss: 0.000124  [47880/170046]\n",
            "loss: 0.000227  [50400/170046]\n",
            "loss: 0.000183  [52920/170046]\n",
            "loss: 0.000155  [55440/170046]\n",
            "loss: 0.000200  [57960/170046]\n",
            "loss: 0.000261  [60480/170046]\n",
            "loss: 0.000146  [63000/170046]\n",
            "loss: 0.000175  [65520/170046]\n",
            "loss: 0.000132  [68040/170046]\n",
            "loss: 0.000148  [70560/170046]\n",
            "loss: 0.000172  [73080/170046]\n",
            "loss: 0.000234  [75600/170046]\n",
            "loss: 0.000217  [78120/170046]\n",
            "loss: 0.000259  [80640/170046]\n",
            "loss: 0.000195  [83160/170046]\n",
            "loss: 0.000179  [85680/170046]\n",
            "loss: 0.000151  [88200/170046]\n",
            "loss: 0.000164  [90720/170046]\n",
            "loss: 0.000248  [93240/170046]\n",
            "loss: 0.000163  [95760/170046]\n",
            "loss: 0.000223  [98280/170046]\n",
            "loss: 0.000164  [100800/170046]\n",
            "loss: 0.000183  [103320/170046]\n",
            "loss: 0.000221  [105840/170046]\n",
            "loss: 0.000155  [108360/170046]\n",
            "loss: 0.000155  [110880/170046]\n",
            "loss: 0.000175  [113400/170046]\n",
            "loss: 0.000178  [115920/170046]\n",
            "loss: 0.000220  [118440/170046]\n",
            "loss: 0.000155  [120960/170046]\n",
            "loss: 0.000136  [123480/170046]\n",
            "loss: 0.000154  [126000/170046]\n",
            "loss: 0.000162  [128520/170046]\n",
            "loss: 0.000152  [131040/170046]\n",
            "loss: 0.000156  [133560/170046]\n",
            "loss: 0.000168  [136080/170046]\n",
            "loss: 0.000154  [138600/170046]\n",
            "loss: 0.000191  [141120/170046]\n",
            "loss: 0.000236  [143640/170046]\n",
            "loss: 0.000122  [146160/170046]\n",
            "loss: 0.000144  [148680/170046]\n",
            "loss: 0.000227  [151200/170046]\n",
            "loss: 0.000315  [153720/170046]\n",
            "loss: 0.000208  [156240/170046]\n",
            "loss: 0.000180  [158760/170046]\n",
            "loss: 0.000212  [161280/170046]\n",
            "loss: 0.000167  [163800/170046]\n",
            "loss: 0.000170  [166320/170046]\n",
            "loss: 0.000146  [168840/170046]\n",
            "delay time: avg MSE: 0.000170, avg abs error: 0.0087\n",
            "learning rate: 0.000370 -> 0.000280\n",
            "---------------------------\n",
            "\n",
            "Epoch 9\n",
            "loss: 0.000188  [  0/170046]\n",
            "loss: 0.000217  [2520/170046]\n",
            "loss: 0.000105  [5040/170046]\n",
            "loss: 0.000139  [7560/170046]\n",
            "loss: 0.000168  [10080/170046]\n",
            "loss: 0.000126  [12600/170046]\n",
            "loss: 0.000174  [15120/170046]\n",
            "loss: 0.000109  [17640/170046]\n",
            "loss: 0.000168  [20160/170046]\n",
            "loss: 0.000174  [22680/170046]\n",
            "loss: 0.000189  [25200/170046]\n",
            "loss: 0.000145  [27720/170046]\n",
            "loss: 0.000115  [30240/170046]\n",
            "loss: 0.000180  [32760/170046]\n",
            "loss: 0.000165  [35280/170046]\n",
            "loss: 0.000122  [37800/170046]\n",
            "loss: 0.000128  [40320/170046]\n",
            "loss: 0.000142  [42840/170046]\n",
            "loss: 0.000179  [45360/170046]\n",
            "loss: 0.000092  [47880/170046]\n",
            "loss: 0.000108  [50400/170046]\n",
            "loss: 0.000183  [52920/170046]\n",
            "loss: 0.000216  [55440/170046]\n",
            "loss: 0.000150  [57960/170046]\n",
            "loss: 0.000179  [60480/170046]\n",
            "loss: 0.000168  [63000/170046]\n",
            "loss: 0.000140  [65520/170046]\n",
            "loss: 0.000177  [68040/170046]\n",
            "loss: 0.000123  [70560/170046]\n",
            "loss: 0.000134  [73080/170046]\n",
            "loss: 0.000203  [75600/170046]\n",
            "loss: 0.000110  [78120/170046]\n",
            "loss: 0.000131  [80640/170046]\n",
            "loss: 0.000092  [83160/170046]\n",
            "loss: 0.000166  [85680/170046]\n",
            "loss: 0.000179  [88200/170046]\n",
            "loss: 0.000106  [90720/170046]\n",
            "loss: 0.000177  [93240/170046]\n",
            "loss: 0.000223  [95760/170046]\n",
            "loss: 0.000221  [98280/170046]\n",
            "loss: 0.000148  [100800/170046]\n",
            "loss: 0.000172  [103320/170046]\n",
            "loss: 0.000119  [105840/170046]\n",
            "loss: 0.000132  [108360/170046]\n",
            "loss: 0.000124  [110880/170046]\n",
            "loss: 0.000198  [113400/170046]\n",
            "loss: 0.000181  [115920/170046]\n",
            "loss: 0.000210  [118440/170046]\n",
            "loss: 0.000188  [120960/170046]\n",
            "loss: 0.000151  [123480/170046]\n",
            "loss: 0.000171  [126000/170046]\n",
            "loss: 0.000141  [128520/170046]\n",
            "loss: 0.000123  [131040/170046]\n",
            "loss: 0.000203  [133560/170046]\n",
            "loss: 0.000137  [136080/170046]\n",
            "loss: 0.000120  [138600/170046]\n",
            "loss: 0.000169  [141120/170046]\n",
            "loss: 0.000230  [143640/170046]\n",
            "loss: 0.000140  [146160/170046]\n",
            "loss: 0.000138  [148680/170046]\n",
            "loss: 0.000174  [151200/170046]\n",
            "loss: 0.000144  [153720/170046]\n",
            "loss: 0.000134  [156240/170046]\n",
            "loss: 0.000096  [158760/170046]\n",
            "loss: 0.000122  [161280/170046]\n",
            "loss: 0.000098  [163800/170046]\n",
            "loss: 0.000151  [166320/170046]\n",
            "loss: 0.000280  [168840/170046]\n",
            "delay time: avg MSE: 0.000173, avg abs error: 0.0093\n",
            "learning rate: 0.000280 -> 0.000190\n",
            "---------------------------\n",
            "\n",
            "Epoch 10\n",
            "loss: 0.000195  [  0/170046]\n",
            "loss: 0.000123  [2520/170046]\n",
            "loss: 0.000139  [5040/170046]\n",
            "loss: 0.000172  [7560/170046]\n",
            "loss: 0.000196  [10080/170046]\n",
            "loss: 0.000097  [12600/170046]\n",
            "loss: 0.000102  [15120/170046]\n",
            "loss: 0.000154  [17640/170046]\n",
            "loss: 0.000094  [20160/170046]\n",
            "loss: 0.000101  [22680/170046]\n",
            "loss: 0.000172  [25200/170046]\n",
            "loss: 0.000174  [27720/170046]\n",
            "loss: 0.000133  [30240/170046]\n",
            "loss: 0.000138  [32760/170046]\n",
            "loss: 0.000138  [35280/170046]\n",
            "loss: 0.000158  [37800/170046]\n",
            "loss: 0.000126  [40320/170046]\n",
            "loss: 0.000136  [42840/170046]\n",
            "loss: 0.000104  [45360/170046]\n",
            "loss: 0.000088  [47880/170046]\n",
            "loss: 0.000122  [50400/170046]\n",
            "loss: 0.000223  [52920/170046]\n",
            "loss: 0.000142  [55440/170046]\n",
            "loss: 0.000097  [57960/170046]\n",
            "loss: 0.000111  [60480/170046]\n",
            "loss: 0.000145  [63000/170046]\n",
            "loss: 0.000192  [65520/170046]\n",
            "loss: 0.000129  [68040/170046]\n",
            "loss: 0.000135  [70560/170046]\n",
            "loss: 0.000134  [73080/170046]\n",
            "loss: 0.000117  [75600/170046]\n",
            "loss: 0.000114  [78120/170046]\n",
            "loss: 0.000119  [80640/170046]\n",
            "loss: 0.000097  [83160/170046]\n",
            "loss: 0.000123  [85680/170046]\n",
            "loss: 0.000268  [88200/170046]\n",
            "loss: 0.000103  [90720/170046]\n",
            "loss: 0.000159  [93240/170046]\n",
            "loss: 0.000149  [95760/170046]\n",
            "loss: 0.000111  [98280/170046]\n",
            "loss: 0.000115  [100800/170046]\n",
            "loss: 0.000079  [103320/170046]\n",
            "loss: 0.000101  [105840/170046]\n",
            "loss: 0.000127  [108360/170046]\n",
            "loss: 0.000268  [110880/170046]\n",
            "loss: 0.000140  [113400/170046]\n",
            "loss: 0.000120  [115920/170046]\n",
            "loss: 0.000120  [118440/170046]\n",
            "loss: 0.000119  [120960/170046]\n",
            "loss: 0.000160  [123480/170046]\n",
            "loss: 0.000076  [126000/170046]\n",
            "loss: 0.000150  [128520/170046]\n",
            "loss: 0.000137  [131040/170046]\n",
            "loss: 0.000105  [133560/170046]\n",
            "loss: 0.000156  [136080/170046]\n",
            "loss: 0.000170  [138600/170046]\n",
            "loss: 0.000119  [141120/170046]\n",
            "loss: 0.000077  [143640/170046]\n",
            "loss: 0.000108  [146160/170046]\n",
            "loss: 0.000116  [148680/170046]\n",
            "loss: 0.000162  [151200/170046]\n",
            "loss: 0.000094  [153720/170046]\n",
            "loss: 0.000152  [156240/170046]\n",
            "loss: 0.000163  [158760/170046]\n",
            "loss: 0.000235  [161280/170046]\n",
            "loss: 0.000140  [163800/170046]\n",
            "loss: 0.000151  [166320/170046]\n",
            "loss: 0.000201  [168840/170046]\n",
            "delay time: avg MSE: 0.000167, avg abs error: 0.0087\n",
            "learning rate: 0.000190 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 11\n",
            "loss: 0.000175  [  0/170046]\n",
            "loss: 0.000192  [2520/170046]\n",
            "loss: 0.000106  [5040/170046]\n",
            "loss: 0.000129  [7560/170046]\n",
            "loss: 0.000120  [10080/170046]\n",
            "loss: 0.000138  [12600/170046]\n",
            "loss: 0.000088  [15120/170046]\n",
            "loss: 0.000113  [17640/170046]\n",
            "loss: 0.000090  [20160/170046]\n",
            "loss: 0.000068  [22680/170046]\n",
            "loss: 0.000142  [25200/170046]\n",
            "loss: 0.000104  [27720/170046]\n",
            "loss: 0.000067  [30240/170046]\n",
            "loss: 0.000103  [32760/170046]\n",
            "loss: 0.000139  [35280/170046]\n",
            "loss: 0.000101  [37800/170046]\n",
            "loss: 0.000073  [40320/170046]\n",
            "loss: 0.000130  [42840/170046]\n",
            "loss: 0.000101  [45360/170046]\n",
            "loss: 0.000145  [47880/170046]\n",
            "loss: 0.000179  [50400/170046]\n",
            "loss: 0.000160  [52920/170046]\n",
            "loss: 0.000123  [55440/170046]\n",
            "loss: 0.000140  [57960/170046]\n",
            "loss: 0.000105  [60480/170046]\n",
            "loss: 0.000118  [63000/170046]\n",
            "loss: 0.000100  [65520/170046]\n",
            "loss: 0.000107  [68040/170046]\n",
            "loss: 0.000105  [70560/170046]\n",
            "loss: 0.000135  [73080/170046]\n",
            "loss: 0.000130  [75600/170046]\n",
            "loss: 0.000116  [78120/170046]\n",
            "loss: 0.000112  [80640/170046]\n",
            "loss: 0.000133  [83160/170046]\n",
            "loss: 0.000153  [85680/170046]\n",
            "loss: 0.000106  [88200/170046]\n",
            "loss: 0.000098  [90720/170046]\n",
            "loss: 0.000092  [93240/170046]\n",
            "loss: 0.000081  [95760/170046]\n",
            "loss: 0.000102  [98280/170046]\n",
            "loss: 0.000098  [100800/170046]\n",
            "loss: 0.000166  [103320/170046]\n",
            "loss: 0.000118  [105840/170046]\n",
            "loss: 0.000123  [108360/170046]\n",
            "loss: 0.000123  [110880/170046]\n",
            "loss: 0.000087  [113400/170046]\n",
            "loss: 0.000096  [115920/170046]\n",
            "loss: 0.000095  [118440/170046]\n",
            "loss: 0.000120  [120960/170046]\n",
            "loss: 0.000125  [123480/170046]\n",
            "loss: 0.000081  [126000/170046]\n",
            "loss: 0.000085  [128520/170046]\n",
            "loss: 0.000137  [131040/170046]\n",
            "loss: 0.000098  [133560/170046]\n",
            "loss: 0.000117  [136080/170046]\n",
            "loss: 0.000143  [138600/170046]\n",
            "loss: 0.000095  [141120/170046]\n",
            "loss: 0.000098  [143640/170046]\n",
            "loss: 0.000085  [146160/170046]\n",
            "loss: 0.000104  [148680/170046]\n",
            "loss: 0.000125  [151200/170046]\n",
            "loss: 0.000144  [153720/170046]\n",
            "loss: 0.000154  [156240/170046]\n",
            "loss: 0.000155  [158760/170046]\n",
            "loss: 0.000179  [161280/170046]\n",
            "loss: 0.000112  [163800/170046]\n",
            "loss: 0.000085  [166320/170046]\n",
            "loss: 0.000155  [168840/170046]\n",
            "delay time: avg MSE: 0.000125, avg abs error: 0.0074\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 12\n",
            "loss: 0.000102  [  0/170046]\n",
            "loss: 0.000135  [2520/170046]\n",
            "loss: 0.000097  [5040/170046]\n",
            "loss: 0.000095  [7560/170046]\n",
            "loss: 0.000144  [10080/170046]\n",
            "loss: 0.000122  [12600/170046]\n",
            "loss: 0.000116  [15120/170046]\n",
            "loss: 0.000091  [17640/170046]\n",
            "loss: 0.000094  [20160/170046]\n",
            "loss: 0.000112  [22680/170046]\n",
            "loss: 0.000165  [25200/170046]\n",
            "loss: 0.000101  [27720/170046]\n",
            "loss: 0.000136  [30240/170046]\n",
            "loss: 0.000100  [32760/170046]\n",
            "loss: 0.000085  [35280/170046]\n",
            "loss: 0.000078  [37800/170046]\n",
            "loss: 0.000105  [40320/170046]\n",
            "loss: 0.000131  [42840/170046]\n",
            "loss: 0.000169  [45360/170046]\n",
            "loss: 0.000099  [47880/170046]\n",
            "loss: 0.000127  [50400/170046]\n",
            "loss: 0.000128  [52920/170046]\n",
            "loss: 0.000080  [55440/170046]\n",
            "loss: 0.000140  [57960/170046]\n",
            "loss: 0.000089  [60480/170046]\n",
            "loss: 0.000116  [63000/170046]\n",
            "loss: 0.000108  [65520/170046]\n",
            "loss: 0.000103  [68040/170046]\n",
            "loss: 0.000140  [70560/170046]\n",
            "loss: 0.000129  [73080/170046]\n",
            "loss: 0.000121  [75600/170046]\n",
            "loss: 0.000165  [78120/170046]\n",
            "loss: 0.000120  [80640/170046]\n",
            "loss: 0.000103  [83160/170046]\n",
            "loss: 0.000054  [85680/170046]\n",
            "loss: 0.000108  [88200/170046]\n",
            "loss: 0.000138  [90720/170046]\n",
            "loss: 0.000093  [93240/170046]\n",
            "loss: 0.000059  [95760/170046]\n",
            "loss: 0.000108  [98280/170046]\n",
            "loss: 0.000091  [100800/170046]\n",
            "loss: 0.000191  [103320/170046]\n",
            "loss: 0.000105  [105840/170046]\n",
            "loss: 0.000082  [108360/170046]\n",
            "loss: 0.000124  [110880/170046]\n",
            "loss: 0.000152  [113400/170046]\n",
            "loss: 0.000143  [115920/170046]\n",
            "loss: 0.000157  [118440/170046]\n",
            "loss: 0.000090  [120960/170046]\n",
            "loss: 0.000168  [123480/170046]\n",
            "loss: 0.000149  [126000/170046]\n",
            "loss: 0.000095  [128520/170046]\n",
            "loss: 0.000071  [131040/170046]\n",
            "loss: 0.000112  [133560/170046]\n",
            "loss: 0.000065  [136080/170046]\n",
            "loss: 0.000086  [138600/170046]\n",
            "loss: 0.000170  [141120/170046]\n",
            "loss: 0.000100  [143640/170046]\n",
            "loss: 0.000084  [146160/170046]\n",
            "loss: 0.000103  [148680/170046]\n",
            "loss: 0.000081  [151200/170046]\n",
            "loss: 0.000108  [153720/170046]\n",
            "loss: 0.000122  [156240/170046]\n",
            "loss: 0.000106  [158760/170046]\n",
            "loss: 0.000117  [161280/170046]\n",
            "loss: 0.000131  [163800/170046]\n",
            "loss: 0.000167  [166320/170046]\n",
            "loss: 0.000152  [168840/170046]\n",
            "delay time: avg MSE: 0.000117, avg abs error: 0.007\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 13\n",
            "loss: 0.000133  [  0/170046]\n",
            "loss: 0.000112  [2520/170046]\n",
            "loss: 0.000119  [5040/170046]\n",
            "loss: 0.000131  [7560/170046]\n",
            "loss: 0.000077  [10080/170046]\n",
            "loss: 0.000081  [12600/170046]\n",
            "loss: 0.000146  [15120/170046]\n",
            "loss: 0.000105  [17640/170046]\n",
            "loss: 0.000095  [20160/170046]\n",
            "loss: 0.000069  [22680/170046]\n",
            "loss: 0.000110  [25200/170046]\n",
            "loss: 0.000131  [27720/170046]\n",
            "loss: 0.000103  [30240/170046]\n",
            "loss: 0.000110  [32760/170046]\n",
            "loss: 0.000083  [35280/170046]\n",
            "loss: 0.000139  [37800/170046]\n",
            "loss: 0.000086  [40320/170046]\n",
            "loss: 0.000088  [42840/170046]\n",
            "loss: 0.000153  [45360/170046]\n",
            "loss: 0.000096  [47880/170046]\n",
            "loss: 0.000101  [50400/170046]\n",
            "loss: 0.000104  [52920/170046]\n",
            "loss: 0.000108  [55440/170046]\n",
            "loss: 0.000096  [57960/170046]\n",
            "loss: 0.000133  [60480/170046]\n",
            "loss: 0.000213  [63000/170046]\n",
            "loss: 0.000132  [65520/170046]\n",
            "loss: 0.000092  [68040/170046]\n",
            "loss: 0.000150  [70560/170046]\n",
            "loss: 0.000088  [73080/170046]\n",
            "loss: 0.000116  [75600/170046]\n",
            "loss: 0.000106  [78120/170046]\n",
            "loss: 0.000126  [80640/170046]\n",
            "loss: 0.000078  [83160/170046]\n",
            "loss: 0.000077  [85680/170046]\n",
            "loss: 0.000139  [88200/170046]\n",
            "loss: 0.000173  [90720/170046]\n",
            "loss: 0.000089  [93240/170046]\n",
            "loss: 0.000120  [95760/170046]\n",
            "loss: 0.000122  [98280/170046]\n",
            "loss: 0.000143  [100800/170046]\n",
            "loss: 0.000106  [103320/170046]\n",
            "loss: 0.000086  [105840/170046]\n",
            "loss: 0.000075  [108360/170046]\n",
            "loss: 0.000090  [110880/170046]\n",
            "loss: 0.000081  [113400/170046]\n",
            "loss: 0.000101  [115920/170046]\n",
            "loss: 0.000133  [118440/170046]\n",
            "loss: 0.000120  [120960/170046]\n",
            "loss: 0.000101  [123480/170046]\n",
            "loss: 0.000114  [126000/170046]\n",
            "loss: 0.000155  [128520/170046]\n",
            "loss: 0.000156  [131040/170046]\n",
            "loss: 0.000079  [133560/170046]\n",
            "loss: 0.000086  [136080/170046]\n",
            "loss: 0.000094  [138600/170046]\n",
            "loss: 0.000102  [141120/170046]\n",
            "loss: 0.000112  [143640/170046]\n",
            "loss: 0.000084  [146160/170046]\n",
            "loss: 0.000099  [148680/170046]\n",
            "loss: 0.000113  [151200/170046]\n",
            "loss: 0.000110  [153720/170046]\n",
            "loss: 0.000111  [156240/170046]\n",
            "loss: 0.000108  [158760/170046]\n",
            "loss: 0.000099  [161280/170046]\n",
            "loss: 0.000129  [163800/170046]\n",
            "loss: 0.000083  [166320/170046]\n",
            "loss: 0.000133  [168840/170046]\n",
            "delay time: avg MSE: 0.000131, avg abs error: 0.0077\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 14\n",
            "loss: 0.000116  [  0/170046]\n",
            "loss: 0.000160  [2520/170046]\n",
            "loss: 0.000123  [5040/170046]\n",
            "loss: 0.000067  [7560/170046]\n",
            "loss: 0.000080  [10080/170046]\n",
            "loss: 0.000090  [12600/170046]\n",
            "loss: 0.000091  [15120/170046]\n",
            "loss: 0.000093  [17640/170046]\n",
            "loss: 0.000120  [20160/170046]\n",
            "loss: 0.000104  [22680/170046]\n",
            "loss: 0.000097  [25200/170046]\n",
            "loss: 0.000088  [27720/170046]\n",
            "loss: 0.000083  [30240/170046]\n",
            "loss: 0.000083  [32760/170046]\n",
            "loss: 0.000113  [35280/170046]\n",
            "loss: 0.000089  [37800/170046]\n",
            "loss: 0.000115  [40320/170046]\n",
            "loss: 0.000096  [42840/170046]\n",
            "loss: 0.000098  [45360/170046]\n",
            "loss: 0.000106  [47880/170046]\n",
            "loss: 0.000106  [50400/170046]\n",
            "loss: 0.000105  [52920/170046]\n",
            "loss: 0.000102  [55440/170046]\n",
            "loss: 0.000114  [57960/170046]\n",
            "loss: 0.000089  [60480/170046]\n",
            "loss: 0.000103  [63000/170046]\n",
            "loss: 0.000144  [65520/170046]\n",
            "loss: 0.000142  [68040/170046]\n",
            "loss: 0.000064  [70560/170046]\n",
            "loss: 0.000091  [73080/170046]\n",
            "loss: 0.000083  [75600/170046]\n",
            "loss: 0.000079  [78120/170046]\n",
            "loss: 0.000135  [80640/170046]\n",
            "loss: 0.000095  [83160/170046]\n",
            "loss: 0.000102  [85680/170046]\n",
            "loss: 0.000100  [88200/170046]\n",
            "loss: 0.000097  [90720/170046]\n",
            "loss: 0.000097  [93240/170046]\n",
            "loss: 0.000129  [95760/170046]\n",
            "loss: 0.000131  [98280/170046]\n",
            "loss: 0.000159  [100800/170046]\n",
            "loss: 0.000075  [103320/170046]\n",
            "loss: 0.000123  [105840/170046]\n",
            "loss: 0.000145  [108360/170046]\n",
            "loss: 0.000068  [110880/170046]\n",
            "loss: 0.000092  [113400/170046]\n",
            "loss: 0.000177  [115920/170046]\n",
            "loss: 0.000121  [118440/170046]\n",
            "loss: 0.000146  [120960/170046]\n",
            "loss: 0.000130  [123480/170046]\n",
            "loss: 0.000124  [126000/170046]\n",
            "loss: 0.000083  [128520/170046]\n",
            "loss: 0.000092  [131040/170046]\n",
            "loss: 0.000098  [133560/170046]\n",
            "loss: 0.000155  [136080/170046]\n",
            "loss: 0.000085  [138600/170046]\n",
            "loss: 0.000070  [141120/170046]\n",
            "loss: 0.000116  [143640/170046]\n",
            "loss: 0.000061  [146160/170046]\n",
            "loss: 0.000171  [148680/170046]\n",
            "loss: 0.000062  [151200/170046]\n",
            "loss: 0.000092  [153720/170046]\n",
            "loss: 0.000090  [156240/170046]\n",
            "loss: 0.000124  [158760/170046]\n",
            "loss: 0.000190  [161280/170046]\n",
            "loss: 0.000163  [163800/170046]\n",
            "loss: 0.000055  [166320/170046]\n",
            "loss: 0.000140  [168840/170046]\n",
            "delay time: avg MSE: 0.000125, avg abs error: 0.0075\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 15\n",
            "loss: 0.000131  [  0/170046]\n",
            "loss: 0.000080  [2520/170046]\n",
            "loss: 0.000098  [5040/170046]\n",
            "loss: 0.000141  [7560/170046]\n",
            "loss: 0.000103  [10080/170046]\n",
            "loss: 0.000099  [12600/170046]\n",
            "loss: 0.000092  [15120/170046]\n",
            "loss: 0.000106  [17640/170046]\n",
            "loss: 0.000104  [20160/170046]\n",
            "loss: 0.000063  [22680/170046]\n",
            "loss: 0.000087  [25200/170046]\n",
            "loss: 0.000074  [27720/170046]\n",
            "loss: 0.000113  [30240/170046]\n",
            "loss: 0.000108  [32760/170046]\n",
            "loss: 0.000095  [35280/170046]\n",
            "loss: 0.000176  [37800/170046]\n",
            "loss: 0.000141  [40320/170046]\n",
            "loss: 0.000181  [42840/170046]\n",
            "loss: 0.000094  [45360/170046]\n",
            "loss: 0.000094  [47880/170046]\n",
            "loss: 0.000076  [50400/170046]\n",
            "loss: 0.000100  [52920/170046]\n",
            "loss: 0.000095  [55440/170046]\n",
            "loss: 0.000061  [57960/170046]\n",
            "loss: 0.000116  [60480/170046]\n",
            "loss: 0.000146  [63000/170046]\n",
            "loss: 0.000099  [65520/170046]\n",
            "loss: 0.000070  [68040/170046]\n",
            "loss: 0.000073  [70560/170046]\n",
            "loss: 0.000158  [73080/170046]\n",
            "loss: 0.000079  [75600/170046]\n",
            "loss: 0.000121  [78120/170046]\n",
            "loss: 0.000105  [80640/170046]\n",
            "loss: 0.000084  [83160/170046]\n",
            "loss: 0.000070  [85680/170046]\n",
            "loss: 0.000087  [88200/170046]\n",
            "loss: 0.000065  [90720/170046]\n",
            "loss: 0.000118  [93240/170046]\n",
            "loss: 0.000108  [95760/170046]\n",
            "loss: 0.000117  [98280/170046]\n",
            "loss: 0.000101  [100800/170046]\n",
            "loss: 0.000091  [103320/170046]\n",
            "loss: 0.000104  [105840/170046]\n",
            "loss: 0.000091  [108360/170046]\n",
            "loss: 0.000107  [110880/170046]\n",
            "loss: 0.000096  [113400/170046]\n",
            "loss: 0.000094  [115920/170046]\n",
            "loss: 0.000114  [118440/170046]\n",
            "loss: 0.000134  [120960/170046]\n",
            "loss: 0.000100  [123480/170046]\n",
            "loss: 0.000081  [126000/170046]\n",
            "loss: 0.000069  [128520/170046]\n",
            "loss: 0.000148  [131040/170046]\n",
            "loss: 0.000111  [133560/170046]\n",
            "loss: 0.000057  [136080/170046]\n",
            "loss: 0.000097  [138600/170046]\n",
            "loss: 0.000119  [141120/170046]\n",
            "loss: 0.000204  [143640/170046]\n",
            "loss: 0.000079  [146160/170046]\n",
            "loss: 0.000153  [148680/170046]\n",
            "loss: 0.000080  [151200/170046]\n",
            "loss: 0.000082  [153720/170046]\n",
            "loss: 0.000069  [156240/170046]\n",
            "loss: 0.000072  [158760/170046]\n",
            "loss: 0.000099  [161280/170046]\n",
            "loss: 0.000068  [163800/170046]\n",
            "loss: 0.000091  [166320/170046]\n",
            "loss: 0.000078  [168840/170046]\n",
            "delay time: avg MSE: 0.000115, avg abs error: 0.007\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Finished training\n",
            "delay time: avg MSE: 0.000117, avg abs error: 0.007\n"
          ]
        }
      ],
      "source": [
        "from src.util import plot_violin\n",
        "import numpy as np\n",
        "\n",
        "WEIGHTS_DIR = \"_weights/\"\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 15\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device {device}\")\n",
        "\n",
        "error = []\n",
        "\n",
        "fx = EFFECT_MAP.index(\"delay\")\n",
        "\n",
        "WEIGHTS_PATH = os.path.join(WEIGHTS_DIR, EXPERIMENT_NAME + \"_\" + str(fx))\n",
        "\n",
        "if not os.path.exists('%s' % WEIGHTS_DIR):\n",
        "    os.makedirs('%s' % WEIGHTS_DIR)\n",
        "\n",
        "fxData = load_train_data(fx)\n",
        "# fxData, _ = torch.utils.data.random_split(fxData, lengths=[0.01, 0.99])\n",
        "\n",
        "train_dataloader, test_dataloader = split_data(fxData)\n",
        "val_dataloader = load_evaluation_data(fx)\n",
        "\n",
        "# construct model and assign it to device\n",
        "cnn = model.Extractor().to(device)\n",
        "\n",
        "# if fx == 0:\n",
        "#     signal, _, _, _, _ = fxData[0]\n",
        "#     print(f\"There are {len(fxData)} samples in the dataset.\")\n",
        "#     print(f\"Shape of signal: {signal.shape}\")\n",
        "\n",
        "#     print(\"input feature:\")\n",
        "#     log_writer.add_figure(\"Input Feature\", plot_spectrogram(signal[0], title=\"MFCC\"))\n",
        "#     log_writer.add_graph(cnn, signal.unsqueeze_(0))\n",
        "\n",
        "# initialise loss funtion + optimiser\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "optimiser = torch.optim.Adam(cnn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# train model\n",
        "train.train(cnn,\n",
        "            train_dataloader,\n",
        "            test_dataloader,\n",
        "            loss_fn,\n",
        "            optimiser,\n",
        "            device,\n",
        "            log_writer,\n",
        "            EPOCHS,\n",
        "            WEIGHTS_PATH,\n",
        "            effect=fx)\n",
        "\n",
        "_, _, log = train.test(cnn, val_dataloader, device, effect=fx)\n",
        "for _, data in enumerate(log):\n",
        "    error.append(data[3])\n",
        "\n",
        "arr = np.array(error)\n",
        "np.save(EVU_DIR + EXPERIMENT_NAME + \"_\" + str(fx) + \"_evaluation.npy\", arr)\n",
        "\n",
        "# log_writer.add_figure(\"Error Box\", \n",
        "#                       plot_violin(error, title=\"Error\", labels=EFFECT_MAP, ylabel=\"parameter value\", outlier=True))\n",
        "\n",
        "log_writer.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "torchaudio-tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
