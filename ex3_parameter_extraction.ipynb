{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Distortion Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMiPTdSA8C2F",
        "outputId": "a297ba92-a5c6-4015-d92d-1e9487daf896"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import os"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gIdrZtW_JYf",
        "outputId": "c1bb6969-15a9-44eb-8cdd-8d79c89208f2"
      },
      "outputs": [],
      "source": [
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050*3\n",
        "\n",
        "mfcc = torchaudio.transforms.MFCC(\n",
        "    sample_rate = SAMPLE_RATE, \n",
        "    n_mfcc = 64,\n",
        "    melkwargs = {\n",
        "        \"n_fft\": 1024,\n",
        "        \"hop_length\": 1024,\n",
        "        \"n_mels\": 64,\n",
        "        \"center\": False})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Functions for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.gtfxdataset import GtFxDataset\n",
        "from src.util import plot_spectrogram\n",
        "from src.extrector import train\n",
        "from src.extrector import model\n",
        "from torch import nn\n",
        "\n",
        "AUDIO_DIR = \"_assets/DATASET/GT-FX-C53/\"\n",
        "ANNOTATIONS_FILE = os.path.join(AUDIO_DIR, \"annotation.csv\")\n",
        "EFFECT_MAP = [\"distortion\", \"chorus\", \"tremolo\", \"delay\", \"reverb\"]\n",
        "\n",
        "def load_data(effect):\n",
        "    \n",
        "    fxData = GtFxDataset(ANNOTATIONS_FILE,\n",
        "                        AUDIO_DIR,\n",
        "                        mfcc,\n",
        "                        SAMPLE_RATE,\n",
        "                        NUM_SAMPLES,\n",
        "                        device,\n",
        "                        effect=EFFECT_MAP[effect])\n",
        "    return fxData\n",
        "\n",
        "def split_data(data):\n",
        "\n",
        "    BATCH_SIZE = round(len(data) / 1500)\n",
        "\n",
        "    split_ratio = [0.8, 0.1, 0.1]\n",
        "    train_set, test_set, val_set = torch.utils.data.random_split(data, lengths=split_ratio)\n",
        "\n",
        "    train_dataloader = train.create_data_loader(train_set, BATCH_SIZE)\n",
        "    test_dataloader = train.create_data_loader(test_set, BATCH_SIZE)\n",
        "    val_dataloader = train.create_data_loader(val_set, BATCH_SIZE)\n",
        "\n",
        "    return train_dataloader, test_dataloader, val_dataloader   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Add Tensorboard to record data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "EXPERIMENT_NAME = \"c53_parameter\"\n",
        "EVU_DIR = \"_log/Evaluation/\"\n",
        "\n",
        "if not os.path.exists('%s' % LOG_DIR):\n",
        "    os.makedirs('%s' % LOG_DIR)\n",
        "\n",
        "if not os.path.exists('%s' % EVU_DIR):\n",
        "    os.makedirs('%s' % EVU_DIR)\n",
        "\n",
        "log_writer = SummaryWriter(LOG_DIR)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8O9r5WI2zuI",
        "outputId": "ed2a4ebd-f644-4e35-8fda-86170924dfe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cpu\n",
            "There are 209844 samples in the dataset.\n",
            "Shape of signal: torch.Size([1, 64, 64])\n",
            "input feature:\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA+UAAAKoCAYAAAARcl6MAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABO2ElEQVR4nO3de3SddZ0v/s/e2Ul6S29Ak5a2UKBV7gLF0oq0cqlURTmMguIFhZklC7wgKmcYzhwro60HlwzjqeLAoANnDjDneAOPA7SKFLWipYKUqhSk0AsNpaX03lyf3x8u8jNTLrvfpPmm7eu1VtaCvZ93Pp+kT3byzpNkl4qiKAIAAADoc+XcCwAAAMD+SikHAACATJRyAAAAyEQpBwAAgEyUcgAAAMhEKQcAAIBMlHIAAADIRCkHAACATJRyAAAAyEQpB4B9wL/+679GqVSKUqkUDzzwwC73F0URRxxxRJRKpZgxY0bX7S9n/vPLgQceuMvr+PnPfx7nn39+HHzwwVFXVxfDhg2LadOmxY033hjbtm3rdmxLS0vMmzcvTj311BgxYkTU1dXFwQcfHOeff34sXLiwt998ANhrVXIvAAD0noaGhrjlllu6Fe+IiIULF8af/vSnaGho2CXz3ve+Nz772c92u622trbb/3/hC1+Ia6+9NqZNmxb/8A//EIcffnhs3749Fi1aFLNnz47ly5fHP/7jP0ZExPr16+Pss8+Oxx57LC6++OL4/Oc/HyNHjow1a9bEXXfdFWeccUYsWbIkjj/++N594wFgL6SUA8A+5IILLoj//b//d3zjG9+IoUOHdt1+yy23xNSpU2Pz5s27ZBobG+OUU0551df5f//v/41rr702Lrnkkrj55pujVCp13Tdr1qy46qqr4le/+lXXbR/5yEfid7/7Xdx3331x+umnd3td73//++PKK6+MESNG9OTNBIB9hh9fB4B9yAc+8IGIiLjjjju6btu0aVN873vfi4svvjjpdV577bUxYsSI+PrXv96tkL+soaEhZs6cGRERS5YsiXvuuScuueSSXQr5y04++eQYP3580i4AsK9RygFgHzJ06NB473vfG9/+9re7brvjjjuiXC7HBRdc8IqZoiiivb2920tRFBERsXbt2nj88cdj5syZMWjQoNedP3/+/IiIOPfcc3v+xgDAfkApB4B9zMUXXxy/+c1vYtmyZRER8e1vfzve9773veLvk0dEfPOb34za2tpuL7fccktERKxcuTIiIiZMmFDV7N09HgD2d36nHAD2MdOnT4/DDz88vv3tb8dHP/rRWLx4cXzta1971ePPP//8+PznP9/ttkMPPXQPbwkARCjlALDPKZVK8bGPfSy+/vWvx86dO2PSpEnx1re+9VWPP+igg2Ly5MmveN/Lv/u9YsWKqmb/5fFveMMbdnNzANj/+PF1ANgHffSjH43169fHt771rfjYxz6W/HpGjx4dxx57bMyfPz+2b9/+use//e1vj4iIH/7wh8kzAWB/opQDwD7o4IMPjs9//vNxzjnnxEUXXdSj1/X3f//3sXHjxvjUpz7V9Qfg/tLWrVu7/sDbiSeeGLNmzYpbbrkl7r///ld8fQ8//HDX754DwP7Oj68DwD7qK1/5Sq+8nve9733x93//9/EP//AP8cc//jEuueSSOPzww2P79u3x61//Ov75n/85Lrjggq6nRbvtttvi7LPPjlmzZsXFF18cs2bNihEjRsTatWvjRz/6Udxxxx2xZMkST4sGAKGUAwBVuPbaa+PMM8+M//k//2dcc801sX79+hg4cGAcffTRceWVV8bHP/7xrmMPPPDA+MUvfhE333xz3HHHHXH77bfH9u3bY9SoUXHKKafE3XffHccff3zGtwYA+o9S8Uo/hwYAAADscX6nHAAAADJRygEAACATpRwAAAAyUcoBAAAgE6UcAAAAMlHKAQAAIJN9/nnKOzs747nnnouGhoYolUq51wEAAGAfVxRFbNmyJcaMGRPl8mtfC9/nS/lzzz0X48aNy70GAAAA+5lVq1bF2LFjX/OYfb6UNzQ0RETEKW/526hU6jNvU53KwkeTciuveXP60MQfIhj/pd8kj3z+jjck5Ro/8ETyzKe+cUJSbtobn0qe2Vmk/ZbI+jM3Jc/c8r0JSbmGv1qRPDPV+J8NSM6ufNvO5Oz6f5+YlGtq2JI8s/0dzcnZVAPuGZWUW715WPLM8cM2JuW2n70+eWbLD8cn5Xa01ybPHP7ePyXltn7/0OSZnZ1pjydD3/t08syJC9PeR89uH5k8s3XW88lZXlvqY19ExGEjNiTlBtW0Js9cfe0RyVmAnNrbW+KhX36lq4++ln2+lL/8I+uVSn1UKulf/PelSintC6CaAelvX5FYylN3jYioGZT2TZKezCwPTHsf1Q6uS56ZWsp79L4d3Pfv21R1Q9Lft5VSR3I29fyrHdySPDMyvH9Tz92a9vRvYqbO7Mn515F4zte0pc9M3beSuGtEREdiKe/J+7ZuSFq2ttSDx80MHyv7i9THvoj0j+26Hny1ubd87Qbwaqr5FWp/6A0AAAAyUcoBAAAgE6UcAAAAMlHKAQAAIBOlHAAAADJRygEAACATpRwAAAAy2eefp/xlnbWl6KwkPhn3XqKjB0/lWdQUvbdIlYYM6MHzPSeqDGxPyj0/dXPyzIafH5icTTV01p+Scmu+f3TyzI6OxO/xbdmYPLPzp+kf04fVbUjKbXnr+uSZT9/+pqTcgIGtyTNPGfBsUm5He/rzRI+o25GUe3H+ockz62c+k5Qbu2h48swXEnPjGl5KnrnxLS8m5XbcNyF5ZsTqpNQbGp5Pnri2J/8uO4akBc9Iezsj0h/ne/J4kqqpYUtydmtb2nOcj6jbnjyzs3bf/toN2Hd1VvH85C9zpRwAAAAyUcoBAAAgE6UcAAAAMlHKAQAAIBOlHAAAADJRygEAACATpRwAAAAyUcoBAAAgk0ruBfpK3U8eiUqpNvcae1RnXZGcLWrSs6kGVNr7fGaltiMp9/RXpibPPKx1VVKu9oHRyTPbZqxNyh12wIbkmZ1FKSl30ICtyTN/2zw2OftUc9r7d+THJybPPOzCXyVnU2375Yik3KaWAT2YOjwpta21Lnli/U/TzoWdHTuTZ6bauHNQcnb9jw5Myg0ttyTPrC+nPVa/2Do4eWZ7kX7N4PCh65NyOx5qSJ5ZjsTz6Odp/54REXXltM9nw+teTJ45sNyalNvRmf6xXXffw8lZgJzKRVv1x+7BPQAAAIDXoJQDAABAJko5AAAAZKKUAwAAQCZKOQAAAGSilAMAAEAmSjkAAABkopQDAABAJko5AAAAZFLJvUBfKZ10VJRqBuReoyrF4qVpuUqRPrMH2VTlUt/PrKnpTMqVjtiSPLNx0OakXGeR/j2z+oeGJOVWb0t/SGhP3PeZLSOTZ9ZVOpKz4ya8kJQbeOmK5JmpDvjliOTsCzvTzoWjRzYnz1z4p4lJuZHDtybPrCmnfWxv2Dk4eeak32xLyj27Nf2x7+gD0/5dVm1NP4f+tPXApNy2tvrkmWs2DUvOvqlpTVKuXEo7hyIiVm8fnpQ7rGF98sz6cntyNtXAmrak3Itt6R9npZPTHk8Acit17IxYcldVx2a/Ur5mzZr40Ic+FAcccEAMGjQo3vSmN8WSJUu67i+KImbPnh1jxoyJgQMHxowZM2LZsmUZNwYAAIDekbWUb9y4Md7ylrdEbW1t3HPPPfH73/8+vva1r8Xw4cO7jrnuuuvi+uuvj3nz5sXixYujqakpzjrrrNiyJf3KJQAAAPQHWX98/X/8j/8R48aNi+985ztdtx166KFd/10URdxwww1xzTXXxHnnnRcREbfeems0NjbG7bffHh//+Mf7emUAAADoNVmvlN99990xefLkeN/73hejRo2KE044IW6++eau+1esWBHNzc0xc+bMrtvq6+tj+vTpsWjRold8nS0tLbF58+ZuLwAAANAfZS3lTz/9dNx4440xceLEuO++++LSSy+NT33qU3HbbbdFRERz85//oE1jY2O3XGNjY9d9/9ncuXNj2LBhXS/jxo3bs28EAAAAJMpayjs7O+PEE0+MOXPmxAknnBAf//jH42/+5m/ixhtv7HZcqVTq9v9FUexy28uuvvrq2LRpU9fLqlWr9tj+AAAA0BNZS/no0aPjqKOO6nbbkUceGStXroyIiKampoiIXa6Kr1u3bper5y+rr6+PoUOHdnsBAACA/ihrKX/LW94STzzxRLfbli9fHoccckhEREyYMCGamppiwYIFXfe3trbGwoULY9q0aX26KwAAAPS2rH99/TOf+UxMmzYt5syZE+eff3785je/iZtuuiluuummiPjzj61fccUVMWfOnJg4cWJMnDgx5syZE4MGDYoLL7ww5+oAAADQY1lL+cknnxw/+MEP4uqrr45rr702JkyYEDfccEN88IMf7Drmqquuih07dsRll10WGzdujClTpsT8+fOjoaFht2YVS34fRam2t9+EfqWoFOnhmh5k9wPj3vt4cnbl/EOTcuMbXkye2VC7My14xurkmQf84oCk3Lrtu/ex/JdOHJX+NyNe2DkkKdf6wOjkmQcPeikpt3ZHffLMl3YMTMqNqN+ePPPNhz6TnE3V2lnT5zN3dKR9ThlQ054888WWwUm5Q3vwePLcKVuSck/92wnJM084pAcf29NeSsp1/jT9D8OWz0jbd/mCQ5JnHjRwa1Ju06kbkmcu//aUpNykix9OnhmxtAdZgHyKoq3qY7OW8oiId73rXfGud73rVe8vlUoxe/bsmD17dt8tBQAAAH0g6++UAwAAwP5MKQcAAIBMlHIAAADIRCkHAACATJRyAAAAyEQpBwAAgEyUcgAAAMgk+/OU04vKRXq20tl7e1Spsygl5XrynaT29rR0+0/GJ88cP/jFpNyQSmvyzG3t9Um55TefkDzz2I5VSbn6mc8kz9y2aHhydkBNe1KuUk7/WKkvp80cO+il5Jkv7hiUlCuX0h9PWjtrknIbW9J2jYg4YugLSbme/HuOrNuelNvalvbxGRExvLYlKffCziHJMyO2JKWO+NAj6SN/fmB6NlHq56SIiIYHD0oLnvZs8sxNycl0ky5+OCk3OPX9ExHbTkv72AbYm7hSDgAAAJko5QAAAJCJUg4AAACZKOUAAACQiVIOAAAAmSjlAAAAkIlSDgAAAJko5QAAAJCJUg4AAACZVHIv0FdKxx8ZpZr63GtUpXhkWVqwXCTPLNWkZ1O1tKedfj05adtWD06bef7S9KG/Gpo2s9SRPLKceC68802PJc9c1zIkKTd00fDkmfXl9uRsS2famTRmwKbkmZ1FKSn3zNaRyTMHn/10Uq7Sg3+X53c2JOXGDdmYPHNj66Ck3Na29M8LrR01SbnVW4Ynzxw5cHtSrlLuTJ6ZasWdxyVnJ3W8kJzd+OOJSbkRZz6ZPHP7/QcnZ/cH205L//csnXB0L24C0HdKHS0Rv7urqmNdKQcAAIBMlHIAAADIRCkHAACATJRyAAAAyEQpBwAAgEyUcgAAAMhEKQcAAIBMlHIAAADIpJJ7gb6ys2lgVGoH5F6jKvWPJAZ78C2Wck2RlHv6K1OTZw5t25CUG5w8MeKIKx/qQTrN5taBSbnt7XXJM1s70z60jxu2Jnnmjo7apFxbkX7itiS+nRER9eX2pFzq+zYioqUjLXvwoE3JM+sX1yflnjz5peSZrfeOTMo11m9JnvnElsak3Pa29I+zZzakvZ0njlmdPHN7e9rHWaXcmTwz9ew7avTzyTMHVVqTs0eObk7KPfuLA5JnVkrbk3KHPpL+77K5Pe1zS325LXnm4ycl7vvTsckzd14/KDkLkFN7Wznid9Ud60o5AAAAZKKUAwAAQCZKOQAAAGSilAMAAEAmSjkAAABkopQDAABAJko5AAAAZKKUAwAAQCZKOQAAAGRSyb1AX6m/77dRKdXmXmOPKpWL5Gy53JmUaxvZnjxzy9aBSbkDkyemK91/cHL2oAEvJeV+uviY5JkTP/HrpNzmhwckz6yUO5Jy9aX0c+hPJ+9Mzo7/9eCkXF05fd8nNo1KylXOXJk8s35hU1Jux31pH58RETWR9lj02Inpj2ERzUmp9nsPS5447r2PJ+We/+m45Jkt7WmftscOeSl55uAHD0rKtXamf99/w1s2JmeHL057HDuwflvyzJda0z5entqW9r6NiBhRtz0pt6NtUPLMsQ+l5Vafsjp5Zn2kZwFyqinaqj7WlXIAAADIRCkHAACATJRyAAAAyEQpBwAAgEyUcgAAAMhEKQcAAIBMlHIAAADIRCkHAACATJRyAAAAyKSSe4G+su3cyVGpHZB7jaoM/u6v04LlInlmKTFbqu9InjnhA79Lzva14vQ1ydnnFzYl5SZ+IvE8iIiV//fYpFz9theSZzZvbUjKjRy4PXlmy/za5OyGlm1JuWe2jkyeWTlzZVJu672HJc9sbU37GB349hXpMxcckpzta4PPfrrPZ25vSz9vhyTuuyF5YsSwXxyQlNvUmv529uSLk+e2D0vKDaq0Js/89YpDk3JHfOiR5JlrEh8XWtvT37upj9eDHxyYPHPd19Mf/wByam/bGfHDu6o61pVyAAAAyEQpBwAAgEyUcgAAAMhEKQcAAIBMlHIAAADIRCkHAACATJRyAAAAyEQpBwAAgEwquRfoK4N/+HBUSrW519ijSuUiOVtT05mUa+3wfZ3X0zK9OSnX/MMjk2eOP3dpUq4leWJE+93DknLF6WuSZ9YnJyO2JeZynPFDzn46w9R0dWc9m3uFfm1v+/d8bmvax/bgHrydO+6bkJyN6SvSZqZPjCNiYw/Safam8yj18TYiYnC80Gt7APSl9qKt6mM1KgAAAMhEKQcAAIBMlHIAAADIRCkHAACATJRyAAAAyEQpBwAAgEyUcgAAAMhEKQcAAIBMlHIAAADIpJJ7gb6y8+yTolI7IPcaVRnw/36TlCuVi+SZ5cTspIsfTp65Nxn2iwOSs5tO3ZCUazr3D8kzczjo3U8k5ZZ/683JM2uHtSRnJ3zgd8lZXttT/3ZCUu6IDz2SPPNPXzslKXf4Zx9Knrm/GHz200m5mp+NSZ458G0rkrPsW3a+K/1zBEBO7W07I+69q6pjXSkHAACATJRyAAAAyEQpBwAAgEyUcgAAAMhEKQcAAIBMlHIAAADIRCkHAACATJRyAAAAyEQpBwAAgEwquRfoK7Xb2qNSac+9xh5VLhXJ2VIPsqme+rcTknJHfOiRXt7k9W06dUOfz1z13WOSs+Pe+3gvbrJnTbr0N7lXoJfl+Bg9/LMP9fnMvcnaHx6ZnB197h+Sch1vey55Jrysduu+/bUbsO8qtVf/+OVKOQAAAGSilAMAAEAmSjkAAABkopQDAABAJko5AAAAZKKUAwAAQCZKOQAAAGSilAMAAEAmldwL9JWan/8uakq1udfYs0pFcrTcg2yqkcO39fnMvcm49z6eewVgHzH63D/kXoFXULr/4ORscfqaXtyk/6p54Le5VwBIUhRtVR+b9Ur57Nmzo1QqdXtpamrqur8oipg9e3aMGTMmBg4cGDNmzIhly5Zl3BgAAAB6T/YfXz/66KNj7dq1XS9Lly7tuu+6666L66+/PubNmxeLFy+OpqamOOuss2LLli0ZNwYAAIDekb2UVyqVaGpq6no56KCDIuLPV8lvuOGGuOaaa+K8886LY445Jm699dbYvn173H777Zm3BgAAgJ7LXsqffPLJGDNmTEyYMCHe//73x9NPPx0REStWrIjm5uaYOXNm17H19fUxffr0WLRo0au+vpaWlti8eXO3FwAAAOiPspbyKVOmxG233Rb33Xdf3HzzzdHc3BzTpk2LDRs2RHNzc0RENDY2dss0NjZ23fdK5s6dG8OGDet6GTdu3B59GwAAACBV1lI+a9as+Ku/+qs49thj48wzz4wf//jHERFx6623dh1TKpW6ZYqi2OW2v3T11VfHpk2bul5WrVq1Z5YHAACAHsr+4+t/afDgwXHsscfGk08+2fVX2P/zVfF169btcvX8L9XX18fQoUO7vQAAAEB/1K9KeUtLS/zhD3+I0aNHx4QJE6KpqSkWLFjQdX9ra2ssXLgwpk2blnFLAAAA6B2VnMM/97nPxTnnnBPjx4+PdevWxZe+9KXYvHlzXHTRRVEqleKKK66IOXPmxMSJE2PixIkxZ86cGDRoUFx44YU51wYAAIBekbWUr169Oj7wgQ/E+vXr46CDDopTTjklHnrooTjkkEMiIuKqq66KHTt2xGWXXRYbN26MKVOmxPz586OhoWG3Z7WeeWJ01g7o7Tdhj6i7d3FSrlwukmeWS+nZVEMH7OzzmamWf/PNydlJl/2mFzcBYF9RnL4m9wq7ZcUdxyflJnzgd8kzW88+OTkLkFN7286In9xV1bFZS/mdd975mveXSqWYPXt2zJ49u28WAgAAgD7Ur36nHAAAAPYnSjkAAABkopQDAABAJko5AAAAZKKUAwAAQCZKOQAAAGSilAMAAEAmWZ+nvC/V/eS3USnV5l5jjyqVepItknLr7npj8sxxtS8l5VqSJ6Ybc9j65OzaHx6ZlBs//KXkmW0z1ibl6hc2Jc9smd6cnAWg/ztoxJakXPtPxifPrDtzcXIWIKdy0Vb9sXtwDwAAAOA1KOUAAACQiVIOAAAAmSjlAAAAkIlSDgAAAJko5QAAAJCJUg4AAACZKOUAAACQiVIOAAAAmVRyL9BXakYdFDXlutxrVKXj+XVJuXK5s5c3eX2jhmxNzg6oaUvKPf39o5NnHnzesqTckLOfTp45JDGX9t7pmbpye3K2pRf3AKD/Sf1c+ORtJybPfGPjzuQsQE5FZ2tElbXOlXIAAADIRCkHAACATJRyAAAAyEQpBwAAgEyUcgAAAMhEKQcAAIBMlHIAAADIRCkHAACATJRyAAAAyKSSe4G+0jn6wOisqc+9RnWeX5cUK5XSR5ZKRVKuoW5n8sxKqTMpN3zQjuSZvLatbXvJx8heqnXBIUm5urOe7eVNAPrOxI/8NjnbecLRvbgJQN/p7GiJqLLWuVIOAAAAmSjlAAAAkIlSDgAAAJko5QAAAJCJUg4AAACZKOUAAACQiVIOAAAAmSjlAAAAkEkl9wJ9pbyzNco1pdxrVKUjMVcqFb26RzUG1LQnZ8ulzqTckLqW5Jmp76Ejl6R/qNz39JFJufHvW5o8c/CDByXltp22Jnkmr6/urGdzrwCwVynvbM29AkCSckf1j1+ulAMAAEAmSjkAAABkopQDAABAJko5AAAAZKKUAwAAQCZKOQAAAGSilAMAAEAmSjkAAABkopQDAABAJpXcC/SVjif+FKVSbe419qhyqejzmfXl9uRsZ5SScgNq0mfuSMz94aT0meNjaXI21bbTXujzmakGLmxMzj770ojk7Kj3/DE5C0Df6PjDk7lXAEjSUbRVfawr5QAAAJCJUg4AAACZKOUAAACQiVIOAAAAmSjlAAAAkIlSDgAAAJko5QAAAJCJUg4AAACZKOUAAACQSSX3An2ldMKRUaqpz71GVYoly5Jy5VKRPrMoJeUq5Y7kme2dNX0+k/5nx/Tnk7O1Px7ai5sA0N+UTjo69woASUodLRGP3FXVsa6UAwAAQCZKOQAAAGSilAMAAEAmSjkAAABkopQDAABAJko5AAAAZKKUAwAAQCZKOQAAAGRSyb1AX2lvqIuo1Odeoyo1iblSqUieWRSlpFxND2a2J+Yq5c7kmanGPjQkObv6lK1JuRf/36TkmSPftTw5m2rdXW9Myo16zx+TZ45455PJWQD6v/ahe8fXbgD/WXt79T3JlXIAAADIRCkHAACATJRyAAAAyEQpBwAAgEyUcgAAAMhEKQcAAIBMlHIAAADIRCkHAACATJRyAAAAyKSSe4G+UpRLUZRLudfYo0qlIjnb0en7M6+lpbPvP1RGvmt5n89s/8n45OyoM//Yi5sA8FqW3zI5KTfpkod7eZPXt/HHE5OzDdfv21+7Afuu3ememhgAAABkopQDAABAJko5AAAAZKKUAwAAQCZKOQAAAGSilAMAAEAmSjkAAABkopQDAABAJpXcC/SVygOPRqVUm3uNPWo3np9+F51FWritsyZ96F7khWkv5V6hT6xYc2BydmKs7MVNAHgtky55OCnXuuCQ5Jkj6rcn5cbXP5c8c+VPn0zOAmRVtFV9aL+5Uj537twolUpxxRVXdN1WFEXMnj07xowZEwMHDowZM2bEsmXL8i0JAAAAvahflPLFixfHTTfdFMcdd1y326+77rq4/vrrY968ebF48eJoamqKs846K7Zs2ZJpUwAAAOg92Uv51q1b44Mf/GDcfPPNMWLEiK7bi6KIG264Ia655po477zz4phjjolbb701tm/fHrfffnvGjQEAAKB3ZC/ll19+ebzzne+MM888s9vtK1asiObm5pg5c2bXbfX19TF9+vRYtGjRq76+lpaW2Lx5c7cXAAAA6I+y/qG3O++8M37729/G4sWLd7mvubk5IiIaGxu73d7Y2BjPPvvsq77OuXPnxhe/+MXeXRQAAAD2gGxXyletWhWf/vSn49/+7d9iwIABr3pcqdT9r4IXRbHLbX/p6quvjk2bNnW9rFq1qtd2BgAAgN6U7Ur5kiVLYt26dXHSSSd13dbR0REPPvhgzJs3L5544omI+PMV89GjR3cds27dul2unv+l+vr6qK+v33OLAwAAQC/JdqX8jDPOiKVLl8ajjz7a9TJ58uT44Ac/GI8++mgcdthh0dTUFAsWLOjKtLa2xsKFC2PatGm51gYAAIBek+1KeUNDQxxzzDHdbhs8eHAccMABXbdfccUVMWfOnJg4cWJMnDgx5syZE4MGDYoLL7wwx8oAAADQq7L+obfXc9VVV8WOHTvisssui40bN8aUKVNi/vz50dDQsNuvq3T8kVGq2Tt+rL14ZFlSrlQqkme2t9ck5ba0p79PB9a0JWfZM0aM3Jp7BfZj7T8Zn5ytnLmyFzeB3ZN67uY4b+vOevU/lvt6tvVxLiKidMLRPUgD5FPqaIn43V1VHduvSvkDDzzQ7f9LpVLMnj07Zs+enWUfAAAA2JOyP085AAAA7K+UcgAAAMhEKQcAAIBMlHIAAADIRCkHAACATJRyAAAAyEQpBwAAgEyUcgAAAMikknuBvlL87g9RlGpzr1GV5Te+OSnXWHoxeWZHe9r3Z57bOix55qFDNyTl2jv7/ntJ4389ODm7csq2XtykOqX7D07KHXj68l7eBKpXOXNl7hUgiXN3zykeWZZ7BYAkRdFW9bGulAMAAEAmSjkAAABkopQDAABAJko5AAAAZKKUAwAAQCZKOQAAAGSilAMAAEAmSjkAAABkUsm9ALsqDW5Py5WK5JltO9NOhYFvX5E8s/2XI9JynTXJMzf9xxFJuZVTnkqemcP2trqk3MBe3gMAAHhtrpQDAABAJko5AAAAZKKUAwAAQCZKOQAAAGSilAMAAEAmSjkAAABkopQDAABAJko5AAAAZKKUAwAAQCaV3Auwq0pde1KuXCqSZ9avrE/OptrZkXb6peYiIoYP2JGUS3/Pptt8z+HJ2aFv/1MvbtJ/vfj/JiVnR75reS9uwl9a/s03J+UmXfabXt4EAKD/c6UcAAAAMlHKAQAAIBOlHAAAADJRygEAACATpRwAAAAyUcoBAAAgE6UcAAAAMlHKAQAAIBOlHAAAADKp5F6grxRTjomiMiD3GlWpVDqScqUezDzkC4t6kE6zpS3t36OtoyZ55vD6HUm5ul8ckDzzDy80JuUGF63JM/cX7R3p31d89v8cm5Q75PylyTP3JoMfPCg5O+m03/TiJuTWMv/QpNzhQ9cnz2wr0j+2n5+6OTlL/1NMOz73CgBJivadEb++q6pjXSkHAACATJRyAAAAyEQpBwAAgEyUcgAAAMhEKQcAAIBMlHIAAADIRCkHAACATJRyAAAAyKSSe4G+Uvr141Eq1eZeoyqVq45MypVLRS9vsmetfnF4Um5gfWvyzJEDtiXlNp26IXnmmEjP7g/G/3pwcnbgzo3J2RUbRyZn9ybP/eCopNyBLWkfKxER2348MSk34p1PJs9kzxlS15KUe6FlSPLMlvb0L09W3HloUm7C+x9LnsmeU1r0u9wrACQpFW1VH+tKOQAAAGSilAMAAEAmSjkAAABkkvxLW52dnfHUU0/FunXrorOzs9t9p512Wo8XAwAAgH1dUil/6KGH4sILL4xnn302iqL7HxcrlUrR0dHRK8sBAADAviyplF966aUxefLk+PGPfxyjR4+OUqnU23sBAADAPi+plD/55JPx3e9+N4444oje3gcAAAD2G0l/6G3KlCnx1FNP9fYuAAAAsF9JulL+yU9+Mj772c9Gc3NzHHvssVFbW9vt/uOOO65XlgMAAIB9Wan4z3+prQrl8q4X2EulUhRF0e/+0NvmzZtj2LBh8dbT/ntUKgNyr1OVFz+zLSk3dEBL8sz6mc8kZ/vaU//rhOTs8YesTsptO+2F5JnP/p9jk3KHnL80eebe5PDF6R+XL7YOSs5ufMuLSbn2n4xPnlk5c2VyFmB/1PG2E3OvAJCkvX1n/PzBa2PTpk0xdOjQ1zw26Ur5ihUrkhYDAAAA/n9JpfyQQw7p7T0AAABgv1N1Kb/77rtj1qxZUVtbG3ffffdrHvvud7+7x4sBAADAvq7qUn7uuedGc3NzjBo1Ks4999xXPa6//U45AAAA9FdVl/LOzs5X/G8AAAAgTdLzlAMAAAA9l1zKf/rTn8a73vWuOPzww+OII46Id73rXfGTn/ykN3cDAACAfVpSKZ83b16cffbZ0dDQEJ/+9KfjU5/6VAwdOjTe8Y53xLx583p7RwAAANgnlYqiKHY3dPDBB8fVV18dn/jEJ7rd/o1vfCO+/OUvx3PPPddrC/bU5s2bY9iwYXHGiIuiUqrLvU5V1v/bgUm5wXWtyTPrzno2Obs3qV/Y1Ocz2zvTfiCl4209+Dj66dik2HEj1qSPXD0pKXfG2OXJMx87cbcfvrqs+f7RSbk3NaW/j16Y9lJyNtVBi4Yn5XLsCi97+itTk7OH/e2vknJb7z0seeaQs59OynX+dFzyzPIZq5Kze5OaESNyrwCQpL1ojZ9uvDU2bdoUQ4cOfc1jk9rC5s2b4+yzz97l9pkzZ8bmzZtTXiUAAADsd5JK+bvf/e74wQ9+sMvtd911V5xzzjk9XgoAAAD2B1U/JdrXv/71rv8+8sgj48tf/nI88MADMXXqn3/E7KGHHopf/vKX8dnPfrb3twQAAIB9UNWl/B//8R+7/f+IESPi97//ffz+97/vum348OHx7W9/O/7bf/tvvbchAAAA7KOqLuUrVqzYk3sAAADAfif5ecqrMXTo0Hj66bS/SAoAAAD7uj1ayhOebQ0AAAD2G3u0lAMAAACvTikHAACATKr+Q297vVEHRNTU596iKjXlztwr7LNa2hNP+TNW9+4ie1rivg/8v0nJIw969xNJuceSJ/bMwectS8qVfzW0lzfZs16Y9lLuFfrEmu8fnZRLPQ9yafj5gUm5LW9d38ub7FmH/e2v+nxma+rnh4homX9oUq7+jGeSZ+43mtLOeYDsOloiNlZ36B69Ul4qlfbkqwcAAIC9mj/0BgAAAJns0VJ+zz33xMEHH7wnRwAAAMBeK+kXqK688sqqjz311FNTRgAAAMA+L6mUP/LII/Hb3/422tvb4w1veENERCxfvjxqamrixBNP7DrO75QDAADAq0sq5eecc040NDTErbfeGiNGjIiIiI0bN8bHPvaxeOtb3xqf/exne3VJAAAA2Bcl/U751772tZg7d25XIY+IGDFiRHzpS1+Kr33ta722HAAAAOzLkkr55s2b4/nnn9/l9nXr1sWWLVt6vBQAAADsD5JK+X/5L/8lPvaxj8V3v/vdWL16daxevTq++93vxiWXXBLnnXdeb+8IAAAA+6Sk3yn/1re+FZ/73OfiQx/6ULS1tf35FVUqcckll8RXv/rVql/PjTfeGDfeeGM888wzERFx9NFHx3//7/89Zs2aFRF/fp7zL37xi3HTTTfFxo0bY8qUKfGNb3wjjj766N3euW3koCgqA3Y7l0NNaXNSrlzyvPCvZ0d7bVJuYC/v0V+NfNfy5GzrgkOScnVnPZs8M4fnp6Z9fLJnHXzestwr9Iktb12fe4V9Vk8e/9hz2g4YnHsFgCTt7TVVH5t0pXzQoEHxzW9+MzZs2ND1l9hffPHF+OY3vxmDB1f/4Dl27Nj4yle+Eg8//HA8/PDDcfrpp8d73vOeWLbsz19cXXfddXH99dfHvHnzYvHixdHU1BRnnXWWH5EHAABgn5BUyl+2du3aWLt2bUyaNCkGDx4cRbF7V2rPOeeceMc73hGTJk2KSZMmxZe//OUYMmRIPPTQQ1EURdxwww1xzTXXxHnnnRfHHHNM3HrrrbF9+/a4/fbbe7I2AAAA9AtJpXzDhg1xxhlnxKRJk+Id73hHrF27NiIi/vqv/zr56dA6OjrizjvvjG3btsXUqVNjxYoV0dzcHDNnzuw6pr6+PqZPnx6LFi1KmgEAAAD9SVIp/8xnPhO1tbWxcuXKGDRoUNftF1xwQdx777279bqWLl0aQ4YMifr6+rj00kvjBz/4QRx11FHR3NwcERGNjY3djm9sbOy675W0tLTE5s2bu70AAABAf5T0h97mz58f9913X4wdO7bb7RMnToxnn929P9r0hje8IR599NF46aWX4nvf+15cdNFFsXDhwq77S6VSt+OLotjltr80d+7c+OIXv7hbOwAAAEAOSVfKt23b1u0K+cvWr18f9fX1u/W66urq4ogjjojJkyfH3Llz4/jjj49/+qd/iqampoiIXa6Kr1u3bper53/p6quvjk2bNnW9rFq1arf2AQAAgL6SVMpPO+20uO2227r+v1QqRWdnZ3z1q1+Nt73tbT1aqCiKaGlpiQkTJkRTU1MsWLCg677W1tZYuHBhTJs27VXz9fX1MXTo0G4vAAAA0B8l/fj6V7/61ZgxY0Y8/PDD0draGldddVUsW7YsXnzxxfjlL39Z9ev5u7/7u5g1a1aMGzcutmzZEnfeeWc88MADce+990apVIorrrgi5syZExMnToyJEyfGnDlzYtCgQXHhhRemrA0AAAD9SlIpP+qoo+Kxxx6LG2+8MWpqamLbtm1x3nnnxeWXXx6jR4+u+vU8//zz8eEPfzjWrl0bw4YNi+OOOy7uvffeOOussyIi4qqrroodO3bEZZddFhs3bowpU6bE/Pnzo6GhYbd3Lv9qaZRLtbudy6FSPjQpVy7t3lPS7Y/aOtOeBXDoA9Wf17vMnLE2KVe6/+DkmcXpa5KzqerO2r2/J7E/GvvQkKTc6lO29vImAHuH8i8ezb0CQJJy0Vb1sbtdytva2mLmzJnxz//8zz3+g2q33HLLa95fKpVi9uzZMXv27B7NAQAAgP5oty8b1tbWxuOPP/6afwEdAAAAeH1JP8v7kY985HWvcgMAAACvLel3yltbW+Nf/uVfYsGCBTF58uQYPHhwt/uvv/76XlkOAAAA9mVVl/LHHnssjjnmmCiXy/H444/HiSeeGBERy5cv73acH2sHAACA6lRdyk844YRYu3ZtjBo1Kp599tlYvHhxHHDAAXtyNwAAANinVf075cOHD48VK1ZERMQzzzwTnZ2de2wpAAAA2B9UfaX8r/7qr2L69OkxevToKJVKMXny5KipqXnFY59++uleWxAAAAD2VVWX8ptuuinOO++8eOqpp+JTn/pU/M3f/E00NDTsyd0AAABgn7Zbf3397LPPjoiIJUuWxKc//WmlfA8plYqkXDnScnubp/7XCcnZUbE5Kdc2Y23yzFTF6Wv6fCZ71upTtuZeAaDP9eTz9hEffqQXNwHon5KeEu073/lOb+8BAAAA+52q/9AbAAAA0LuUcgAAAMhEKQcAAIBMlHIAAADIRCkHAACATJRyAAAAyEQpBwAAgEyUcgAAAMikknsBdlUpd+ZeoV8bNmx7crY28X3buuCQ5Jl1Zz2blKt9YHTyzLYZa5OzsD+pX9iUnG2Z3tyLm9BbGn5+YFJuy1vX9/Imr690/8HJ2e1tdUm5icNeSJ45uNKSNrP4ffLMJ5OTAHsPV8oBAAAgE6UcAAAAMlHKAQAAIBOlHAAAADJRygEAACATpRwAAAAyUcoBAAAgE6UcAAAAMqnkXqCvFFOOiaIyIPcaVSnHhqRcqVQkz0xPpmv+4ZFJuREDdiTPrK9pTwuesTp55sTF9Um5x15MP19rFhySlKs769nkmala5h+anK2f+Uyv7cH+qWV6c+4V6GVb29Iec3OYNHRdn8/c0p7+uaWc+HXGb9cfnDxz0LSG5CxATkX7zohf31XVsa6UAwAAQCZKOQAAAGSilAMAAEAmSjkAAABkopQDAABAJko5AAAAZKKUAwAAQCZKOQAAAGSilAMAAEAmldwL9JVya0eUO9pzr1GdUpEUKyfmIiI6kpPpRjVsTcrVltO3TX0ftSVPjNjcXp+UG/j2FT2Yuveon/lM7hWA1/Ds/zk2KXfI+Ut7eZPqFKevyTI3xROTe/LZJVX6zOcScy9+d1DyzCEte8nXbgD/Sbmj+s7iSjkAAABkopQDAABAJko5AAAAZKKUAwAAQCZKOQAAAGSilAMAAEAmSjkAAABkopQDAABAJpXcC/SV4pE/RFGqzb1GVWrLo5NynUWplzfZswZW2pJylVJH8sz2oiYpV7r/4OSZz09dk5wFyO3N455Nyj3fy3uw9xr33seTs0Uv7gHQl4qi+q7jSjkAAABkopQDAABAJko5AAAAZKKUAwAAQCZKOQAAAGSilAMAAEAmSjkAAABkopQDAABAJko5AAAAZFLJvQC7KpeKPp/Z0ecT09XVpG/b3l6TlDtkyIvJM59JTgLk9/zUzblXAIB9mivlAAAAkIlSDgAAAJko5QAAAJCJUg4AAACZKOUAAACQiVIOAAAAmSjlAAAAkIlSDgAAAJko5QAAAJBJJfcCfaV9+psiKgNyr1GVgbE6KdcZpV7eZM/a0V6blBtauzN5ZmtHTVJuY+ug5JkRO3qQ7VsH/HJEcvaPG0Yl5Q569xPJM/c2jb8ampR7furmXt6kf9px34Tk7MC3r+jFTfY9tQ+MTs62zVjbi5vA7mk//aTcKwAkaW/fGbHwrqqOdaUcAAAAMlHKAQAAIBOlHAAAADJRygEAACATpRwAAAAyUcoBAAAgE6UcAAAAMlHKAQAAIJNK7gX6SmXho1Ep1eZeoyrl2Qel5aLUu4vsYc+9NDQpN3rQpuSZO9vTzoHi9DXJM1MNfjDtPIiI2NgyKCnXULsueWZdpSM5uzcZ/+vBydlt7TVJufU/mpQ888Bzlidn+9rAt6/IvcI+a0fiY1/EfvSFQgYrvjI1OTvhb3/Vi5v0X5X7l+ReASBN0Vb1oa6UAwAAQCZKOQAAAGSilAMAAEAmSjkAAABkopQDAABAJko5AAAAZKKUAwAAQCZKOQAAAGSilAMAAEAmldwL0D+suPO4pNzgga3JM8e+Z1lSruahhuSZm3fWJ+UOSp4YMfahIUm5+potyTOH1LakzSy3J88cP3RjWvAXByTP3NlRm5xNNbBmQ3L2kIFp2Q07ByfPhIiIne3pn+7THsHyGZP4OeK5U9Ifc1Mf5w+veSR55p+SkwD0N1mvlM+dOzdOPvnkaGhoiFGjRsW5554bTzzxRLdjiqKI2bNnx5gxY2LgwIExY8aMWLYsrcwBAABAf5K1lC9cuDAuv/zyeOihh2LBggXR3t4eM2fOjG3btnUdc91118X1118f8+bNi8WLF0dTU1OcddZZsWVL+ne0AQAAoD/I+uPr9957b7f//853vhOjRo2KJUuWxGmnnRZFUcQNN9wQ11xzTZx33nkREXHrrbdGY2Nj3H777fHxj388x9oAAADQK/rVH3rbtGlTRESMHDkyIiJWrFgRzc3NMXPmzK5j6uvrY/r06bFo0aJXfB0tLS2xefPmbi8AAADQH/WbUl4URVx55ZVx6qmnxjHHHBMREc3NzRER0djY2O3YxsbGrvv+s7lz58awYcO6XsaNG7dnFwcAAIBE/aaUf+ITn4jHHnss7rjjjl3uK5VK3f6/KIpdbnvZ1VdfHZs2bep6WbVq1R7ZFwAAAHqqXzwl2ic/+cm4++6748EHH4yxY8d23d7U1BQRf75iPnr06K7b161bt8vV85fV19dHfX3a014BAABAX8p6pbwoivjEJz4R3//+9+P++++PCRMmdLt/woQJ0dTUFAsWLOi6rbW1NRYuXBjTpk3r63UBAACgV2W9Un755ZfH7bffHnfddVc0NDR0/Z74sGHDYuDAgVEqleKKK66IOXPmxMSJE2PixIkxZ86cGDRoUFx44YU5VwcAAIAey1rKb7zxxoiImDFjRrfbv/Od78RHP/rRiIi46qqrYseOHXHZZZfFxo0bY8qUKTF//vxoaGjo4237TrlUpOWKtFxExJBBLUm5mnL6zBw2bxmUlDuoBzMbancm5dbtTD/H62vak3I7OuqSZz66auzrH/QK3nb48uSZtaXO5OxzO4Ym5V5qG5g884nJbYnJ1ckz2XN23Dfh9Q96BQPfvqKXN3l9Q85+us9n9kTNz8YkZyulTUm50v0HJ89cfcqa5Oz+oPFXaY+3ERHPT/UsOsC+L2spL6ookaVSKWbPnh2zZ8/e8wsBAABAH+o3f30dAAAA9jdKOQAAAGSilAMAAEAmSjkAAABkopQDAABAJko5AAAAZKKUAwAAQCZZn6e8LxVvPiaKyoDca1SlXFrd5zMH1bUl5V7aPrCXN9mzOjtKSbnlt0xOnlm7ZU1Srm3G2uSZOUyI3yXlnundNar21P8am5QbdeDm5JlDIz2bavM9hyflhs76Uy9vsu8Z+PYVuVfYZ42s356cXTllW2IyNbd3qX1gdHI29fPS1rb65JnF1OOTswA5Fe07I35zV1XHulIOAAAAmSjlAAAAkIlSDgAAAJko5QAAAJCJUg4AAACZKOUAAACQiVIOAAAAmSjlAAAAkIlSDgAAAJlUci/QV4q6chSVveN7EOVS0eczB9a2JeXWbBrRy5vsWcOHb0vKHTQ4LRcRsWLDyKTc2FibPHN/0f6T8cnZ0rq0j7Ohs/6UPDOHvW1fiIh4YdpLuVfo92p+NiYpV1dO+3wfEXHEkrQvG0fXPZM88yd1Y5OzADkV5eq7597RUgEAAGAfpJQDAABAJko5AAAAZKKUAwAAQCZKOQAAAGSilAMAAEAmSjkAAABkopQDAABAJpXcC/SV8i8ei3KpNvcaVRqZlCqXOpMn1pY70oI7a5Jn5nDQ4G1pwTNWJ88cG+lZXlvlzJXJ2cMjPQvse+oXNiVnW6Y39+Ime9aIuh3J2S1tA5Jyz2w9IHlmeeEjyVmAnMpFW/XH7sE9AAAAgNeglAMAAEAmSjkAAABkopQDAABAJko5AAAAZKKUAwAAQCZKOQAAAGSilAMAAEAmSjkAAABkUsm9QF+peeMRUVNTn3uNqpTjhT6fOaCmLSlXMzQt1xMdRSk5O2LA9qTcxuSJAOwNWqY39/nM9T+alJw98G3Lk3Krkyf2xNbkZM1R6e8jgJyKjpaIP1Z3rCvlAAAAkIlSDgAAAJko5QAAAJCJUg4AAACZKOUAAACQiVIOAAAAmSjlAAAAkIlSDgAAAJko5QAAAJBJJfcCfaW0cXOUynW51+i3KuXOpFzHjppe3mTPaqi0JOU29vIeAHDgOctzr9DvlV7clHsFgCSlztaqj3WlHAAAADJRygEAACATpRwAAAAyUcoBAAAgE6UcAAAAMlHKAQAAIBOlHAAAADJRygEAACCTSu4F+krnqBHRWVOfe42qlEtrE3NF8sy6ckdSbtJfP5w8M1VND97OwZWWXtwEANiTOhtH5l4BIElnR0vE89Ud60o5AAAAZKKUAwAAQCZKOQAAAGSilAMAAEAmSjkAAABkopQDAABAJko5AAAAZKKUAwAAQCZKOQAAAGRSyb1AX+lc+kR0lmpzr1Gl4X0+sbbc0eczU5WjSM7Wl9t7cRMAYE/q/N0fcq8AkKSzaKv6WFfKAQAAIBOlHAAAADJRygEAACATpRwAAAAyUcoBAAAgE6UcAAAAMlHKAQAAIBOlHAAAADJRygEAACCTSu4F+krNEROipqY+9xpVKZde6PuZUfT5zFSVckdytraUmq1JngkApKmZdHjuFQCSFB0tEU9Vd6wr5QAAAJCJUg4AAACZKOUAAACQiVIOAAAAmSjlAAAAkIlSDgAAAJko5QAAAJCJUg4AAACZVHIv0Fc6nloRpVJt7jWqNDQpVS4VyRMr5Y7kbF+rLXUmZ2uSszXJMwGANB3L/5R7BYAkHUVb1cdmvVL+4IMPxjnnnBNjxoyJUqkUP/zhD7vdXxRFzJ49O8aMGRMDBw6MGTNmxLJly/IsCwAAAL0saynftm1bHH/88TFv3rxXvP+6666L66+/PubNmxeLFy+OpqamOOuss2LLli19vCkAAAD0vqw/vj5r1qyYNWvWK95XFEXccMMNcc0118R5550XERG33nprNDY2xu233x4f//jH+3JVAAAA6HX99g+9rVixIpqbm2PmzJldt9XX18f06dNj0aJFr5praWmJzZs3d3sBAACA/qjflvLm5uaIiGhsbOx2e2NjY9d9r2Tu3LkxbNiwrpdx48bt0T0BAAAgVb8t5S8rlUrd/r8oil1u+0tXX311bNq0qetl1apVe3pFAAAASNJvnxKtqakpIv58xXz06NFdt69bt26Xq+d/qb6+Purr6/f4fgAAANBT/fZK+YQJE6KpqSkWLFjQdVtra2ssXLgwpk2blnEzAAAA6B1Zr5Rv3bo1nnrqqa7/X7FiRTz66KMxcuTIGD9+fFxxxRUxZ86cmDhxYkycODHmzJkTgwYNigsvvDDj1gAAANA7spbyhx9+ON72trd1/f+VV14ZEREXXXRR/Ou//mtcddVVsWPHjrjsssti48aNMWXKlJg/f340NDTs9qzOqcdGZ2VAr+2+J9WU/pRhZtHnM+OnY5NilfKr/6G/11Nb6kjKjf/18OSZK6dsS8qV7j84eWZLR9qHdt1ZzybPTPXU/zohOTvlsGeSs+1F2g8KbTp1Q/JMAHZP56lvyr0CQJLO9p0Rv7qrqmOzlvIZM2ZEUbx6GSyVSjF79uyYPXt23y0FAAAAfaTf/k45AAAA7OuUcgAAAMhEKQcAAIBMlHIAAADIRCkHAACATJRyAAAAyEQpBwAAgEyyPk95Xyr/ammUS7W516hSQ59PLMerP1/8njJm8KakXG2pI3lm6tu5dMPo5JnD4qmkXHH6muSZdcnJdAMXNibljpj+SPLMDcnJdCN+OTI5u/EtL/biJv3XQYuGJ+VemPZSr+4B7P3Kv3g09woAScpFW/XH7sE9AAAAgNeglAMAAEAmSjkAAABkopQDAABAJko5AAAAZKKUAwAAQCZKOQAAAGSilAMAAEAmSjkAAABkUsm9AP1DudTZ5zNH1m5LytWWOpJn1iS+ndNHP5U884mFjUm5w4e8kDxzVN2WpFxbUZM8c0jNE0m5lqW1yTN7IvVt7Siak2f+6v4JSbnjR6xJnplqW3t9cvbJk1/qvUVgL3DMkrRrHD15zC2XiqRcTz62xw7YmJRr6Uz/cnPJCa4fAfs+j3QAAACQiVIOAAAAmSjlAAAAkIlSDgAAAJko5QAAAJCJUg4AAACZKOUAAACQiVIOAAAAmSjlAAAAkEkl9wJ9pXLI2KiU63OvUaWNfT6xXCqScpv+44jkmUMqf0jOpiqXOpNyW9oHJM+cPOLZpNzOztrkmds765JyW9vTP0Z+0vzGpFz9zGeSZ9YvbErOrnppeFKulPixEhFx0LufSMo9mjyxJ1qyTGXPOOV3bcnZTe0Dk3JDatLPoR0d6Y9/lXLa4/zIyrbkmamP16t3jkieObIubd/WcvqXfouOT/vc0hOVQ8f3+UyAXtHZElFlDXClHAAAADJRygEAACATpRwAAAAyUcoBAAAgE6UcAAAAMlHKAQAAIBOlHAAAADJRygEAACCTSu4F+kxHZ0TRmXuLfqsm0t437x67NHlmbbk9Kbe9oz55Zk0USbkjBq5LnvnbzeOTco82H5w88+DzliXljvtt2vsnIuKkA1Ym5Yb8ri155p+27UzOThixISn3xobnk2ce/oe082hTx6DkmfcdMzQp9/bHNyfPrC+n/ZvWljqSZ27tGJCUK5fSPy+0FTVJuYZy+nmb6sX2IcnZNw5cm5TbWdQmz+yspF8zSH7MnZY8sge2JSfTHnEjIvr+/HvTI+nZx9/tazdgL9VZ/eOXK+UAAACQiVIOAAAAmSjlAAAAkIlSDgAAAJko5QAAAJCJUg4AAACZKOUAAACQiVIOAAAAmSjlAAAAkEkl9wJ9pX31cxGl2txrVKUcQ/p+ZqlIyo2t25A8c1PH4KRcWyn9tK0tdSTlBpVbkmdOHvZMUm7WAUuTZ7b9sSYp11qkv29rojMpN7gH79vjBq5KzrYVqe+jtFxExPPtw5Jyg8qtyTMvWb4iKbelY2DyzBcTP7ZbOtMfo39+3ICk3EmPpJ23ERHj69Me/3YW6W/nPUcPT86mSz8X8ngp9wL7rMZfDU3KHT3w98kzH01/mAfIqr1oq/pYV8oBAAAgE6UcAAAAMlHKAQAAIBOlHAAAADJRygEAACATpRwAAAAyUcoBAAAgE6UcAAAAMlHKAQAAIJNK7gX6SuXg0VEp1+deo0qbklLlUtHLe7y+4TXbk7PbO9P+PWpKnckzy4nZ1a0jk2eOrnspKXfbG8Ylz4T/3wG5F+jXlpyQ/r3pJXFQL27CXxr70JDk7MlDVyTlBpTbkmcOKrckZ1PVlTqScgNK6W9nbak9MZe2a0REZeyU5CxAVp0tEWuqO9SVcgAAAMhEKQcAAIBMlHIAAADIRCkHAACATJRyAAAAyEQpBwAAgEyUcgAAAMhEKQcAAIBMKrkX6Cvta9ZGlGpzr1GlIbkXqNrgcktytrbUkZQrlzqTZ9ZEWvbIgWuSZ94yaUJyNtW7f78hKTeuNi0XEfFSx6CkXE2pSJ5534tHJ2ePbUj7Nz2s7oXkmU2Vl5JyGzrSHxNWtR2QlDu8bl3yzJE1W5NyNZF+LtQmPi5s70z/vNAaNUm5ukh77IuIGJb4mDugB4+b6cl0taX0bOrVhrR/zT9L/xfdu7Qlfoi2Rfo/aPvq9M+/ADm1F21VH+tKOQAAAGSilAMAAEAmSjkAAABkopQDAABAJko5AAAAZKKUAwAAQCZKOQAAAGSilAMAAEAmSjkAAABkUsm9QF8pDx4U5VJd7jX6rXKpSMoNKLUlz6wpdSZnU6W+ne8ZvD555l+t2ZiUqyntbd8z297nEz/Y8GCfz+yZ1H/Tnrxv+/7fJaI2w8y9SU0PsoN6bYtqdRR9/1jdE52R9jjfGX3/dnYUabtGpO/bkfj+iYhoSzwXNnWmzywPHpycBcipXLRGbKvy2D27CgAAAPBqlHIAAADIRCkHAACATJRyAAAAyEQpBwAAgEyUcgAAAMhEKQcAAIBMlHIAAADIpJJ7gb7SuW17dJbacq9RldWn9P3MjYm5R+PEXt1jz2tISn0vRiVPvPzJ5Um5FzuGJM+8441jkrMA0H9sy70AQJLOovruuVdcKf/mN78ZEyZMiAEDBsRJJ50UP//5z3OvBAAAAD3W70v5v//7v8cVV1wR11xzTTzyyCPx1re+NWbNmhUrV67MvRoAAAD0SL8v5ddff31ccskl8dd//ddx5JFHxg033BDjxo2LG2+8MfdqAAAA0CP9upS3trbGkiVLYubMmd1unzlzZixatOgVMy0tLbF58+ZuLwAAANAf9etSvn79+ujo6IjGxsZutzc2NkZzc/MrZubOnRvDhg3rehk3blxfrAoAAAC7rV+X8peVSqVu/18UxS63vezqq6+OTZs2db2sWrWqL1YEAACA3davnxLtwAMPjJqaml2uiq9bt26Xq+cvq6+vj/r6+r5YDwAAAHqkX18pr6uri5NOOikWLFjQ7fYFCxbEtGnTMm0FAAAAvaNfXymPiLjyyivjwx/+cEyePDmmTp0aN910U6xcuTIuvfTSqvJFUURERHu0RRR7clN4Zdu3dCTldnS2J89sL9qSswAAQM+0x5+/Hn+5j76Wfl/KL7jggtiwYUNce+21sXbt2jjmmGPiP/7jP+KQQw6pKr9ly5aIiPhF/MeeXBNe1QMn5N4AAADIYcuWLTFs2LDXPKZUVFPd92KdnZ3x3HPPRUNDwy5/HG7z5s0xbty4WLVqVQwdOjTThuztnEf0lHOInnIO0RucR/SUc4ie2pfOoaIoYsuWLTFmzJgol1/7t8b7/ZXyniqXyzF27NjXPGbo0KF7/T86+TmP6CnnED3lHKI3OI/oKecQPbWvnEOvd4X8Zf36D70BAADAvkwpBwAAgEz261JeX18fX/jCFzyvOT3iPKKnnEP0lHOI3uA8oqecQ/TU/noO7fN/6A0AAAD6q/36SjkAAADkpJQDAABAJko5AAAAZKKUAwAAQCb7dSn/5je/GRMmTIgBAwbESSedFD//+c9zr0Q/9eCDD8Y555wTY8aMiVKpFD/84Q+73V8URcyePTvGjBkTAwcOjBkzZsSyZcvyLEu/NHfu3Dj55JOjoaEhRo0aFeeee2488cQT3Y5xHvF6brzxxjjuuONi6NChMXTo0Jg6dWrcc889Xfc7h9gdc+fOjVKpFFdccUXXbc4hXs/s2bOjVCp1e2lqauq63zlENdasWRMf+tCH4oADDohBgwbFm970pliyZEnX/fvbebTflvJ///d/jyuuuCKuueaaeOSRR+Ktb31rzJo1K1auXJl7Nfqhbdu2xfHHHx/z5s17xfuvu+66uP7662PevHmxePHiaGpqirPOOiu2bNnSx5vSXy1cuDAuv/zyeOihh2LBggXR3t4eM2fOjG3btnUd4zzi9YwdOza+8pWvxMMPPxwPP/xwnH766fGe97yn6wsV5xDVWrx4cdx0001x3HHHdbvdOUQ1jj766Fi7dm3Xy9KlS7vucw7xejZu3Bhvectbora2Nu655574/e9/H1/72tdi+PDhXcfsd+dRsZ9685vfXFx66aXdbnvjG99Y/O3f/m2mjdhbRETxgx/8oOv/Ozs7i6ampuIrX/lK1207d+4shg0bVnzrW9/KsCF7g3Xr1hURUSxcuLAoCucR6UaMGFH8y7/8i3OIqm3ZsqWYOHFisWDBgmL69OnFpz/96aIoPA5RnS984QvF8ccf/4r3OYeoxn/9r/+1OPXUU1/1/v3xPNovr5S3trbGkiVLYubMmd1unzlzZixatCjTVuytVqxYEc3Nzd3Op/r6+pg+fbrziVe1adOmiIgYOXJkRDiP2H0dHR1x5513xrZt22Lq1KnOIap2+eWXxzvf+c4488wzu93uHKJaTz75ZIwZMyYmTJgQ73//++Ppp5+OCOcQ1bn77rtj8uTJ8b73vS9GjRoVJ5xwQtx8881d9++P59F+WcrXr18fHR0d0djY2O32xsbGaG5uzrQVe6uXzxnnE9UqiiKuvPLKOPXUU+OYY46JCOcR1Vu6dGkMGTIk6uvr49JLL40f/OAHcdRRRzmHqMqdd94Zv/3tb2Pu3Lm73OccohpTpkyJ2267Le677764+eabo7m5OaZNmxYbNmxwDlGVp59+Om688caYOHFi3HfffXHppZfGpz71qbjtttsiYv98LKrkXiCnUqnU7f+LotjlNqiW84lqfeITn4jHHnssfvGLX+xyn/OI1/OGN7whHn300XjppZfie9/7Xlx00UWxcOHCrvudQ7yaVatWxac//emYP39+DBgw4FWPcw7xWmbNmtX138cee2xMnTo1Dj/88Lj11lvjlFNOiQjnEK+ts7MzJk+eHHPmzImIiBNOOCGWLVsWN954Y3zkIx/pOm5/Oo/2yyvlBx54YNTU1OzynZZ169bt8h0ZeD0v/8VR5xPV+OQnPxl33313/OxnP4uxY8d23e48olp1dXVxxBFHxOTJk2Pu3Llx/PHHxz/90z85h3hdS5YsiXXr1sVJJ50UlUolKpVKLFy4ML7+9a9HpVLpOk+cQ+yOwYMHx7HHHhtPPvmkxyGqMnr06DjqqKO63XbkkUd2/cHt/fE82i9LeV1dXZx00kmxYMGCbrcvWLAgpk2blmkr9lYTJkyIpqambudTa2trLFy40PlEl6Io4hOf+ER8//vfj/vvvz8mTJjQ7X7nEamKooiWlhbnEK/rjDPOiKVLl8ajjz7a9TJ58uT44Ac/GI8++mgcdthhziF2W0tLS/zhD3+I0aNHexyiKm95y1t2eVrY5cuXxyGHHBIR++fXRPvtj69feeWV8eEPfzgmT54cU6dOjZtuuilWrlwZl156ae7V6Ie2bt0aTz31VNf/r1ixIh599NEYOXJkjB8/Pq644oqYM2dOTJw4MSZOnBhz5syJQYMGxYUXXphxa/qTyy+/PG6//fa46667oqGhoeu7v8OGDYuBAwd2PVew84jX8nd/93cxa9asGDduXGzZsiXuvPPOeOCBB+Lee+91DvG6Ghoauv6OxcsGDx4cBxxwQNftziFez+c+97k455xzYvz48bFu3br40pe+FJs3b46LLrrI4xBV+cxnPhPTpk2LOXPmxPnnnx+/+c1v4qabboqbbropImL/PI9y/dn3/uAb3/hGccghhxR1dXXFiSee2PXURPCf/exnPysiYpeXiy66qCiKPz91wxe+8IWiqampqK+vL0477bRi6dKleZemX3ml8yciiu985ztdxziPeD0XX3xx1+etgw46qDjjjDOK+fPnd93vHGJ3/eVTohWFc4jXd8EFFxSjR48uamtrizFjxhTnnXdesWzZsq77nUNU40c/+lFxzDHHFPX19cUb3/jG4qabbup2//52HpWKoigyfT8AAAAA9mv75e+UAwAAQH+glAMAAEAmSjkAAABkopQDAABAJko5AAAAZKKUAwAAQCZKOQAAAGSilAMAAEAmSjkAAABkopQDAABAJko5AAAAZKKUAwAAQCb/H8sCojyvBthdAAAAAElFTkSuQmCC",
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "loss: 0.058751  [  0/167876]\n",
            "loss: 0.017773  [2800/167876]\n",
            "loss: 0.009626  [5600/167876]\n",
            "loss: 0.005877  [8400/167876]\n",
            "loss: 0.003118  [11200/167876]\n",
            "loss: 0.003669  [14000/167876]\n",
            "loss: 0.004042  [16800/167876]\n",
            "loss: 0.003146  [19600/167876]\n",
            "loss: 0.003771  [22400/167876]\n",
            "loss: 0.002436  [25200/167876]\n",
            "loss: 0.004353  [28000/167876]\n",
            "loss: 0.003582  [30800/167876]\n",
            "loss: 0.003744  [33600/167876]\n",
            "loss: 0.003211  [36400/167876]\n",
            "loss: 0.003767  [39200/167876]\n",
            "loss: 0.004367  [42000/167876]\n",
            "loss: 0.002261  [44800/167876]\n",
            "loss: 0.002265  [47600/167876]\n",
            "loss: 0.001838  [50400/167876]\n",
            "loss: 0.009375  [53200/167876]\n",
            "loss: 0.003145  [56000/167876]\n",
            "loss: 0.002096  [58800/167876]\n",
            "loss: 0.002964  [61600/167876]\n",
            "loss: 0.001953  [64400/167876]\n",
            "loss: 0.002099  [67200/167876]\n",
            "loss: 0.002126  [70000/167876]\n",
            "loss: 0.001289  [72800/167876]\n",
            "loss: 0.002909  [75600/167876]\n",
            "loss: 0.001374  [78400/167876]\n",
            "loss: 0.001871  [81200/167876]\n",
            "loss: 0.001507  [84000/167876]\n",
            "loss: 0.002105  [86800/167876]\n",
            "loss: 0.002817  [89600/167876]\n",
            "loss: 0.001658  [92400/167876]\n",
            "loss: 0.001626  [95200/167876]\n",
            "loss: 0.001898  [98000/167876]\n",
            "loss: 0.002074  [100800/167876]\n",
            "loss: 0.002190  [103600/167876]\n",
            "loss: 0.001535  [106400/167876]\n",
            "loss: 0.001871  [109200/167876]\n",
            "loss: 0.001896  [112000/167876]\n",
            "loss: 0.001344  [114800/167876]\n",
            "loss: 0.001858  [117600/167876]\n",
            "loss: 0.001841  [120400/167876]\n",
            "loss: 0.003622  [123200/167876]\n",
            "loss: 0.001567  [126000/167876]\n",
            "loss: 0.002426  [128800/167876]\n",
            "loss: 0.004589  [131600/167876]\n",
            "loss: 0.002044  [134400/167876]\n",
            "loss: 0.001284  [137200/167876]\n",
            "loss: 0.001861  [140000/167876]\n",
            "loss: 0.001933  [142800/167876]\n",
            "loss: 0.003972  [145600/167876]\n",
            "loss: 0.002069  [148400/167876]\n",
            "loss: 0.001158  [151200/167876]\n",
            "loss: 0.001593  [154000/167876]\n",
            "loss: 0.001236  [156800/167876]\n",
            "loss: 0.002148  [159600/167876]\n",
            "loss: 0.001821  [162400/167876]\n",
            "loss: 0.003483  [165200/167876]\n",
            "distortion gain: avg MSE: 0.001597, avg abs error: 0.0309\n",
            "learning rate: 0.001000 -> 0.000910\n",
            "---------------------------\n",
            "\n",
            "Epoch 2\n",
            "loss: 0.001327  [  0/167876]\n",
            "loss: 0.001309  [2800/167876]\n",
            "loss: 0.001563  [5600/167876]\n",
            "loss: 0.001016  [8400/167876]\n",
            "loss: 0.001397  [11200/167876]\n",
            "loss: 0.002101  [14000/167876]\n",
            "loss: 0.001220  [16800/167876]\n",
            "loss: 0.001202  [19600/167876]\n",
            "loss: 0.001335  [22400/167876]\n",
            "loss: 0.001173  [25200/167876]\n",
            "loss: 0.002088  [28000/167876]\n",
            "loss: 0.001291  [30800/167876]\n",
            "loss: 0.001681  [33600/167876]\n",
            "loss: 0.001299  [36400/167876]\n",
            "loss: 0.001343  [39200/167876]\n",
            "loss: 0.002199  [42000/167876]\n",
            "loss: 0.002277  [44800/167876]\n",
            "loss: 0.001193  [47600/167876]\n",
            "loss: 0.001762  [50400/167876]\n",
            "loss: 0.001663  [53200/167876]\n",
            "loss: 0.001936  [56000/167876]\n",
            "loss: 0.001009  [58800/167876]\n",
            "loss: 0.001097  [61600/167876]\n",
            "loss: 0.001856  [64400/167876]\n",
            "loss: 0.001029  [67200/167876]\n",
            "loss: 0.002233  [70000/167876]\n",
            "loss: 0.000882  [72800/167876]\n",
            "loss: 0.001456  [75600/167876]\n",
            "loss: 0.001223  [78400/167876]\n",
            "loss: 0.001414  [81200/167876]\n",
            "loss: 0.001547  [84000/167876]\n",
            "loss: 0.001310  [86800/167876]\n",
            "loss: 0.001372  [89600/167876]\n",
            "loss: 0.001000  [92400/167876]\n",
            "loss: 0.001185  [95200/167876]\n",
            "loss: 0.003560  [98000/167876]\n",
            "loss: 0.001051  [100800/167876]\n",
            "loss: 0.001965  [103600/167876]\n",
            "loss: 0.001243  [106400/167876]\n",
            "loss: 0.001202  [109200/167876]\n",
            "loss: 0.001183  [112000/167876]\n",
            "loss: 0.001196  [114800/167876]\n",
            "loss: 0.001465  [117600/167876]\n",
            "loss: 0.001167  [120400/167876]\n",
            "loss: 0.001328  [123200/167876]\n",
            "loss: 0.000952  [126000/167876]\n",
            "loss: 0.001614  [128800/167876]\n",
            "loss: 0.001690  [131600/167876]\n",
            "loss: 0.000935  [134400/167876]\n",
            "loss: 0.002227  [137200/167876]\n",
            "loss: 0.001145  [140000/167876]\n",
            "loss: 0.001157  [142800/167876]\n",
            "loss: 0.001323  [145600/167876]\n",
            "loss: 0.001720  [148400/167876]\n",
            "loss: 0.001325  [151200/167876]\n",
            "loss: 0.002382  [154000/167876]\n",
            "loss: 0.001094  [156800/167876]\n",
            "loss: 0.001793  [159600/167876]\n",
            "loss: 0.001047  [162400/167876]\n",
            "loss: 0.001282  [165200/167876]\n",
            "distortion gain: avg MSE: 0.002399, avg abs error: 0.0408\n",
            "learning rate: 0.000910 -> 0.000820\n",
            "---------------------------\n",
            "\n",
            "Epoch 3\n",
            "loss: 0.005358  [  0/167876]\n",
            "loss: 0.000996  [2800/167876]\n",
            "loss: 0.001027  [5600/167876]\n",
            "loss: 0.000833  [8400/167876]\n",
            "loss: 0.000817  [11200/167876]\n",
            "loss: 0.001114  [14000/167876]\n",
            "loss: 0.001342  [16800/167876]\n",
            "loss: 0.001269  [19600/167876]\n",
            "loss: 0.000725  [22400/167876]\n",
            "loss: 0.000833  [25200/167876]\n",
            "loss: 0.001370  [28000/167876]\n",
            "loss: 0.001037  [30800/167876]\n",
            "loss: 0.000950  [33600/167876]\n",
            "loss: 0.001694  [36400/167876]\n",
            "loss: 0.001496  [39200/167876]\n",
            "loss: 0.000961  [42000/167876]\n",
            "loss: 0.000819  [44800/167876]\n",
            "loss: 0.000981  [47600/167876]\n",
            "loss: 0.001145  [50400/167876]\n",
            "loss: 0.001114  [53200/167876]\n",
            "loss: 0.001129  [56000/167876]\n",
            "loss: 0.000937  [58800/167876]\n",
            "loss: 0.000770  [61600/167876]\n",
            "loss: 0.000975  [64400/167876]\n",
            "loss: 0.000913  [67200/167876]\n",
            "loss: 0.001948  [70000/167876]\n",
            "loss: 0.001103  [72800/167876]\n",
            "loss: 0.000616  [75600/167876]\n",
            "loss: 0.001138  [78400/167876]\n",
            "loss: 0.000805  [81200/167876]\n",
            "loss: 0.001346  [84000/167876]\n",
            "loss: 0.002433  [86800/167876]\n",
            "loss: 0.001388  [89600/167876]\n",
            "loss: 0.001201  [92400/167876]\n",
            "loss: 0.001818  [95200/167876]\n",
            "loss: 0.001099  [98000/167876]\n",
            "loss: 0.000878  [100800/167876]\n",
            "loss: 0.001011  [103600/167876]\n",
            "loss: 0.000716  [106400/167876]\n",
            "loss: 0.001579  [109200/167876]\n",
            "loss: 0.000856  [112000/167876]\n",
            "loss: 0.000782  [114800/167876]\n",
            "loss: 0.000947  [117600/167876]\n",
            "loss: 0.001148  [120400/167876]\n",
            "loss: 0.001047  [123200/167876]\n",
            "loss: 0.001090  [126000/167876]\n",
            "loss: 0.001008  [128800/167876]\n",
            "loss: 0.001141  [131600/167876]\n",
            "loss: 0.001341  [134400/167876]\n",
            "loss: 0.001107  [137200/167876]\n",
            "loss: 0.000857  [140000/167876]\n",
            "loss: 0.000993  [142800/167876]\n",
            "loss: 0.000918  [145600/167876]\n",
            "loss: 0.001030  [148400/167876]\n",
            "loss: 0.000793  [151200/167876]\n",
            "loss: 0.001132  [154000/167876]\n",
            "loss: 0.001499  [156800/167876]\n",
            "loss: 0.000844  [159600/167876]\n",
            "loss: 0.001013  [162400/167876]\n",
            "loss: 0.001036  [165200/167876]\n",
            "distortion gain: avg MSE: 0.001660, avg abs error: 0.0338\n",
            "learning rate: 0.000820 -> 0.000730\n",
            "---------------------------\n",
            "\n",
            "Epoch 4\n",
            "loss: 0.000923  [  0/167876]\n",
            "loss: 0.000753  [2800/167876]\n",
            "loss: 0.000746  [5600/167876]\n",
            "loss: 0.000869  [8400/167876]\n",
            "loss: 0.001057  [11200/167876]\n",
            "loss: 0.000854  [14000/167876]\n",
            "loss: 0.001225  [16800/167876]\n",
            "loss: 0.000800  [19600/167876]\n",
            "loss: 0.000700  [22400/167876]\n",
            "loss: 0.000851  [25200/167876]\n",
            "loss: 0.000764  [28000/167876]\n",
            "loss: 0.000784  [30800/167876]\n",
            "loss: 0.000810  [33600/167876]\n",
            "loss: 0.000755  [36400/167876]\n",
            "loss: 0.001782  [39200/167876]\n",
            "loss: 0.001334  [42000/167876]\n",
            "loss: 0.000881  [44800/167876]\n",
            "loss: 0.001003  [47600/167876]\n",
            "loss: 0.001020  [50400/167876]\n",
            "loss: 0.000844  [53200/167876]\n",
            "loss: 0.000802  [56000/167876]\n",
            "loss: 0.000838  [58800/167876]\n",
            "loss: 0.000864  [61600/167876]\n",
            "loss: 0.000881  [64400/167876]\n",
            "loss: 0.000873  [67200/167876]\n",
            "loss: 0.000887  [70000/167876]\n",
            "loss: 0.000704  [72800/167876]\n",
            "loss: 0.000898  [75600/167876]\n",
            "loss: 0.000759  [78400/167876]\n",
            "loss: 0.001012  [81200/167876]\n",
            "loss: 0.000803  [84000/167876]\n",
            "loss: 0.001130  [86800/167876]\n",
            "loss: 0.000706  [89600/167876]\n",
            "loss: 0.001130  [92400/167876]\n",
            "loss: 0.000731  [95200/167876]\n",
            "loss: 0.000868  [98000/167876]\n",
            "loss: 0.000752  [100800/167876]\n",
            "loss: 0.000875  [103600/167876]\n",
            "loss: 0.001100  [106400/167876]\n",
            "loss: 0.000822  [109200/167876]\n",
            "loss: 0.000689  [112000/167876]\n",
            "loss: 0.000741  [114800/167876]\n",
            "loss: 0.000849  [117600/167876]\n",
            "loss: 0.001277  [120400/167876]\n",
            "loss: 0.000847  [123200/167876]\n",
            "loss: 0.001060  [126000/167876]\n",
            "loss: 0.001755  [128800/167876]\n",
            "loss: 0.000710  [131600/167876]\n",
            "loss: 0.000884  [134400/167876]\n",
            "loss: 0.001002  [137200/167876]\n",
            "loss: 0.001451  [140000/167876]\n",
            "loss: 0.000719  [142800/167876]\n",
            "loss: 0.001003  [145600/167876]\n",
            "loss: 0.000787  [148400/167876]\n",
            "loss: 0.000765  [151200/167876]\n",
            "loss: 0.000988  [154000/167876]\n",
            "loss: 0.000515  [156800/167876]\n",
            "loss: 0.000655  [159600/167876]\n",
            "loss: 0.001128  [162400/167876]\n",
            "loss: 0.000952  [165200/167876]\n",
            "distortion gain: avg MSE: 0.000763, avg abs error: 0.0213\n",
            "learning rate: 0.000730 -> 0.000640\n",
            "---------------------------\n",
            "\n",
            "Epoch 5\n",
            "loss: 0.000636  [  0/167876]\n",
            "loss: 0.001021  [2800/167876]\n",
            "loss: 0.000795  [5600/167876]\n",
            "loss: 0.001048  [8400/167876]\n",
            "loss: 0.000577  [11200/167876]\n",
            "loss: 0.000736  [14000/167876]\n",
            "loss: 0.000918  [16800/167876]\n",
            "loss: 0.000659  [19600/167876]\n",
            "loss: 0.000686  [22400/167876]\n",
            "loss: 0.000660  [25200/167876]\n",
            "loss: 0.000761  [28000/167876]\n",
            "loss: 0.000635  [30800/167876]\n",
            "loss: 0.000821  [33600/167876]\n",
            "loss: 0.000783  [36400/167876]\n",
            "loss: 0.000716  [39200/167876]\n",
            "loss: 0.000632  [42000/167876]\n",
            "loss: 0.001162  [44800/167876]\n",
            "loss: 0.000723  [47600/167876]\n",
            "loss: 0.000742  [50400/167876]\n",
            "loss: 0.000921  [53200/167876]\n",
            "loss: 0.001008  [56000/167876]\n",
            "loss: 0.000564  [58800/167876]\n",
            "loss: 0.000878  [61600/167876]\n",
            "loss: 0.000725  [64400/167876]\n",
            "loss: 0.000829  [67200/167876]\n",
            "loss: 0.001054  [70000/167876]\n",
            "loss: 0.000592  [72800/167876]\n",
            "loss: 0.000659  [75600/167876]\n",
            "loss: 0.000670  [78400/167876]\n",
            "loss: 0.000493  [81200/167876]\n",
            "loss: 0.001042  [84000/167876]\n",
            "loss: 0.000661  [86800/167876]\n",
            "loss: 0.000726  [89600/167876]\n",
            "loss: 0.000831  [92400/167876]\n",
            "loss: 0.000810  [95200/167876]\n",
            "loss: 0.001337  [98000/167876]\n",
            "loss: 0.000625  [100800/167876]\n",
            "loss: 0.001332  [103600/167876]\n",
            "loss: 0.001376  [106400/167876]\n",
            "loss: 0.000654  [109200/167876]\n",
            "loss: 0.000727  [112000/167876]\n",
            "loss: 0.001230  [114800/167876]\n",
            "loss: 0.000563  [117600/167876]\n",
            "loss: 0.000668  [120400/167876]\n",
            "loss: 0.000628  [123200/167876]\n",
            "loss: 0.000818  [126000/167876]\n",
            "loss: 0.000758  [128800/167876]\n",
            "loss: 0.000757  [131600/167876]\n",
            "loss: 0.000964  [134400/167876]\n",
            "loss: 0.000478  [137200/167876]\n",
            "loss: 0.000502  [140000/167876]\n",
            "loss: 0.001112  [142800/167876]\n",
            "loss: 0.000754  [145600/167876]\n",
            "loss: 0.000587  [148400/167876]\n",
            "loss: 0.000750  [151200/167876]\n",
            "loss: 0.000669  [154000/167876]\n",
            "loss: 0.000565  [156800/167876]\n",
            "loss: 0.000615  [159600/167876]\n",
            "loss: 0.000640  [162400/167876]\n",
            "loss: 0.000656  [165200/167876]\n",
            "distortion gain: avg MSE: 0.000820, avg abs error: 0.0223\n",
            "learning rate: 0.000640 -> 0.000550\n",
            "---------------------------\n",
            "\n",
            "Epoch 6\n",
            "loss: 0.000928  [  0/167876]\n",
            "loss: 0.000666  [2800/167876]\n",
            "loss: 0.000670  [5600/167876]\n",
            "loss: 0.000640  [8400/167876]\n",
            "loss: 0.000626  [11200/167876]\n",
            "loss: 0.000528  [14000/167876]\n",
            "loss: 0.000860  [16800/167876]\n",
            "loss: 0.000546  [19600/167876]\n",
            "loss: 0.000825  [22400/167876]\n",
            "loss: 0.000845  [25200/167876]\n",
            "loss: 0.000775  [28000/167876]\n",
            "loss: 0.000747  [30800/167876]\n",
            "loss: 0.000544  [33600/167876]\n",
            "loss: 0.000697  [36400/167876]\n",
            "loss: 0.001204  [39200/167876]\n",
            "loss: 0.000545  [42000/167876]\n",
            "loss: 0.000717  [44800/167876]\n",
            "loss: 0.000556  [47600/167876]\n",
            "loss: 0.000630  [50400/167876]\n",
            "loss: 0.000459  [53200/167876]\n",
            "loss: 0.000938  [56000/167876]\n",
            "loss: 0.000526  [58800/167876]\n",
            "loss: 0.000494  [61600/167876]\n",
            "loss: 0.000704  [64400/167876]\n",
            "loss: 0.000506  [67200/167876]\n",
            "loss: 0.000691  [70000/167876]\n",
            "loss: 0.001629  [72800/167876]\n",
            "loss: 0.000537  [75600/167876]\n",
            "loss: 0.000896  [78400/167876]\n",
            "loss: 0.000468  [81200/167876]\n",
            "loss: 0.000583  [84000/167876]\n",
            "loss: 0.000555  [86800/167876]\n",
            "loss: 0.000765  [89600/167876]\n",
            "loss: 0.000706  [92400/167876]\n",
            "loss: 0.000616  [95200/167876]\n",
            "loss: 0.000563  [98000/167876]\n",
            "loss: 0.000975  [100800/167876]\n",
            "loss: 0.000797  [103600/167876]\n",
            "loss: 0.000541  [106400/167876]\n",
            "loss: 0.000582  [109200/167876]\n",
            "loss: 0.000729  [112000/167876]\n",
            "loss: 0.000831  [114800/167876]\n",
            "loss: 0.000555  [117600/167876]\n",
            "loss: 0.000630  [120400/167876]\n",
            "loss: 0.000929  [123200/167876]\n",
            "loss: 0.000666  [126000/167876]\n",
            "loss: 0.001710  [128800/167876]\n",
            "loss: 0.000870  [131600/167876]\n",
            "loss: 0.001283  [134400/167876]\n",
            "loss: 0.001036  [137200/167876]\n",
            "loss: 0.000588  [140000/167876]\n",
            "loss: 0.000727  [142800/167876]\n",
            "loss: 0.000724  [145600/167876]\n",
            "loss: 0.000607  [148400/167876]\n",
            "loss: 0.000682  [151200/167876]\n",
            "loss: 0.000575  [154000/167876]\n",
            "loss: 0.000557  [156800/167876]\n",
            "loss: 0.001043  [159600/167876]\n",
            "loss: 0.000460  [162400/167876]\n",
            "loss: 0.000454  [165200/167876]\n",
            "distortion gain: avg MSE: 0.000627, avg abs error: 0.0195\n",
            "learning rate: 0.000550 -> 0.000460\n",
            "---------------------------\n",
            "\n",
            "Epoch 7\n",
            "loss: 0.000795  [  0/167876]\n",
            "loss: 0.000933  [2800/167876]\n",
            "loss: 0.000713  [5600/167876]\n",
            "loss: 0.000573  [8400/167876]\n",
            "loss: 0.000784  [11200/167876]\n",
            "loss: 0.000554  [14000/167876]\n",
            "loss: 0.000674  [16800/167876]\n",
            "loss: 0.000639  [19600/167876]\n",
            "loss: 0.000603  [22400/167876]\n",
            "loss: 0.000565  [25200/167876]\n",
            "loss: 0.000607  [28000/167876]\n",
            "loss: 0.000471  [30800/167876]\n",
            "loss: 0.000706  [33600/167876]\n",
            "loss: 0.000585  [36400/167876]\n",
            "loss: 0.000593  [39200/167876]\n",
            "loss: 0.000480  [42000/167876]\n",
            "loss: 0.000675  [44800/167876]\n",
            "loss: 0.000604  [47600/167876]\n",
            "loss: 0.000510  [50400/167876]\n",
            "loss: 0.000562  [53200/167876]\n",
            "loss: 0.000671  [56000/167876]\n",
            "loss: 0.000639  [58800/167876]\n",
            "loss: 0.000431  [61600/167876]\n",
            "loss: 0.000503  [64400/167876]\n",
            "loss: 0.000629  [67200/167876]\n",
            "loss: 0.000689  [70000/167876]\n",
            "loss: 0.000489  [72800/167876]\n",
            "loss: 0.000561  [75600/167876]\n",
            "loss: 0.000458  [78400/167876]\n",
            "loss: 0.000519  [81200/167876]\n",
            "loss: 0.000726  [84000/167876]\n",
            "loss: 0.000572  [86800/167876]\n",
            "loss: 0.000601  [89600/167876]\n",
            "loss: 0.000647  [92400/167876]\n",
            "loss: 0.000576  [95200/167876]\n",
            "loss: 0.000508  [98000/167876]\n",
            "loss: 0.000653  [100800/167876]\n",
            "loss: 0.000455  [103600/167876]\n",
            "loss: 0.000535  [106400/167876]\n",
            "loss: 0.000594  [109200/167876]\n",
            "loss: 0.000545  [112000/167876]\n",
            "loss: 0.000570  [114800/167876]\n",
            "loss: 0.000527  [117600/167876]\n",
            "loss: 0.000707  [120400/167876]\n",
            "loss: 0.000827  [123200/167876]\n",
            "loss: 0.000359  [126000/167876]\n",
            "loss: 0.000663  [128800/167876]\n",
            "loss: 0.000690  [131600/167876]\n",
            "loss: 0.000457  [134400/167876]\n",
            "loss: 0.000722  [137200/167876]\n",
            "loss: 0.000579  [140000/167876]\n",
            "loss: 0.000420  [142800/167876]\n",
            "loss: 0.000526  [145600/167876]\n",
            "loss: 0.000699  [148400/167876]\n",
            "loss: 0.000804  [151200/167876]\n",
            "loss: 0.000897  [154000/167876]\n",
            "loss: 0.000473  [156800/167876]\n",
            "loss: 0.000533  [159600/167876]\n",
            "loss: 0.000780  [162400/167876]\n",
            "loss: 0.000682  [165200/167876]\n",
            "distortion gain: avg MSE: 0.000794, avg abs error: 0.0221\n",
            "learning rate: 0.000460 -> 0.000370\n",
            "---------------------------\n",
            "\n",
            "Epoch 8\n",
            "loss: 0.001042  [  0/167876]\n",
            "loss: 0.000640  [2800/167876]\n",
            "loss: 0.000428  [5600/167876]\n",
            "loss: 0.000513  [8400/167876]\n",
            "loss: 0.000497  [11200/167876]\n",
            "loss: 0.000512  [14000/167876]\n",
            "loss: 0.000537  [16800/167876]\n",
            "loss: 0.000617  [19600/167876]\n",
            "loss: 0.000581  [22400/167876]\n",
            "loss: 0.000442  [25200/167876]\n",
            "loss: 0.001052  [28000/167876]\n",
            "loss: 0.000795  [30800/167876]\n",
            "loss: 0.000470  [33600/167876]\n",
            "loss: 0.000457  [36400/167876]\n",
            "loss: 0.000543  [39200/167876]\n",
            "loss: 0.000556  [42000/167876]\n",
            "loss: 0.000553  [44800/167876]\n",
            "loss: 0.000565  [47600/167876]\n",
            "loss: 0.000537  [50400/167876]\n",
            "loss: 0.000485  [53200/167876]\n",
            "loss: 0.000415  [56000/167876]\n",
            "loss: 0.000518  [58800/167876]\n",
            "loss: 0.000495  [61600/167876]\n",
            "loss: 0.000681  [64400/167876]\n",
            "loss: 0.000483  [67200/167876]\n",
            "loss: 0.000537  [70000/167876]\n",
            "loss: 0.000371  [72800/167876]\n",
            "loss: 0.000602  [75600/167876]\n",
            "loss: 0.000730  [78400/167876]\n",
            "loss: 0.000531  [81200/167876]\n",
            "loss: 0.000525  [84000/167876]\n",
            "loss: 0.000565  [86800/167876]\n",
            "loss: 0.000563  [89600/167876]\n",
            "loss: 0.000428  [92400/167876]\n",
            "loss: 0.000508  [95200/167876]\n",
            "loss: 0.000507  [98000/167876]\n",
            "loss: 0.000409  [100800/167876]\n",
            "loss: 0.000463  [103600/167876]\n",
            "loss: 0.000452  [106400/167876]\n",
            "loss: 0.000605  [109200/167876]\n",
            "loss: 0.000403  [112000/167876]\n",
            "loss: 0.000496  [114800/167876]\n",
            "loss: 0.000556  [117600/167876]\n",
            "loss: 0.000592  [120400/167876]\n",
            "loss: 0.000479  [123200/167876]\n",
            "loss: 0.000654  [126000/167876]\n",
            "loss: 0.000538  [128800/167876]\n",
            "loss: 0.000806  [131600/167876]\n",
            "loss: 0.000543  [134400/167876]\n",
            "loss: 0.000406  [137200/167876]\n",
            "loss: 0.000789  [140000/167876]\n",
            "loss: 0.000412  [142800/167876]\n",
            "loss: 0.000610  [145600/167876]\n",
            "loss: 0.000762  [148400/167876]\n",
            "loss: 0.000649  [151200/167876]\n",
            "loss: 0.000524  [154000/167876]\n",
            "loss: 0.000462  [156800/167876]\n",
            "loss: 0.000497  [159600/167876]\n",
            "loss: 0.000564  [162400/167876]\n",
            "loss: 0.000563  [165200/167876]\n",
            "distortion gain: avg MSE: 0.000618, avg abs error: 0.0192\n",
            "learning rate: 0.000370 -> 0.000280\n",
            "---------------------------\n",
            "\n",
            "Epoch 9\n",
            "loss: 0.001029  [  0/167876]\n",
            "loss: 0.000578  [2800/167876]\n",
            "loss: 0.000439  [5600/167876]\n",
            "loss: 0.000351  [8400/167876]\n",
            "loss: 0.000347  [11200/167876]\n",
            "loss: 0.000492  [14000/167876]\n",
            "loss: 0.000446  [16800/167876]\n",
            "loss: 0.000720  [19600/167876]\n",
            "loss: 0.000457  [22400/167876]\n",
            "loss: 0.000429  [25200/167876]\n",
            "loss: 0.000498  [28000/167876]\n",
            "loss: 0.000409  [30800/167876]\n",
            "loss: 0.000338  [33600/167876]\n",
            "loss: 0.000507  [36400/167876]\n",
            "loss: 0.000387  [39200/167876]\n",
            "loss: 0.000374  [42000/167876]\n",
            "loss: 0.000662  [44800/167876]\n",
            "loss: 0.000462  [47600/167876]\n",
            "loss: 0.000462  [50400/167876]\n",
            "loss: 0.000368  [53200/167876]\n",
            "loss: 0.000409  [56000/167876]\n",
            "loss: 0.000410  [58800/167876]\n",
            "loss: 0.000430  [61600/167876]\n",
            "loss: 0.000673  [64400/167876]\n",
            "loss: 0.000553  [67200/167876]\n",
            "loss: 0.000438  [70000/167876]\n",
            "loss: 0.000388  [72800/167876]\n",
            "loss: 0.000511  [75600/167876]\n",
            "loss: 0.000363  [78400/167876]\n",
            "loss: 0.000545  [81200/167876]\n",
            "loss: 0.000412  [84000/167876]\n",
            "loss: 0.000628  [86800/167876]\n",
            "loss: 0.000437  [89600/167876]\n",
            "loss: 0.000370  [92400/167876]\n",
            "loss: 0.000439  [95200/167876]\n",
            "loss: 0.000526  [98000/167876]\n",
            "loss: 0.000376  [100800/167876]\n",
            "loss: 0.000319  [103600/167876]\n",
            "loss: 0.000550  [106400/167876]\n",
            "loss: 0.000755  [109200/167876]\n",
            "loss: 0.000403  [112000/167876]\n",
            "loss: 0.000361  [114800/167876]\n",
            "loss: 0.000384  [117600/167876]\n",
            "loss: 0.000473  [120400/167876]\n",
            "loss: 0.000471  [123200/167876]\n",
            "loss: 0.000490  [126000/167876]\n",
            "loss: 0.000440  [128800/167876]\n",
            "loss: 0.000628  [131600/167876]\n",
            "loss: 0.000721  [134400/167876]\n",
            "loss: 0.000322  [137200/167876]\n",
            "loss: 0.000403  [140000/167876]\n",
            "loss: 0.000580  [142800/167876]\n",
            "loss: 0.000433  [145600/167876]\n",
            "loss: 0.000395  [148400/167876]\n",
            "loss: 0.000384  [151200/167876]\n",
            "loss: 0.000548  [154000/167876]\n",
            "loss: 0.000358  [156800/167876]\n",
            "loss: 0.000457  [159600/167876]\n",
            "loss: 0.000358  [162400/167876]\n",
            "loss: 0.000522  [165200/167876]\n",
            "distortion gain: avg MSE: 0.000462, avg abs error: 0.0165\n",
            "learning rate: 0.000280 -> 0.000190\n",
            "---------------------------\n",
            "\n",
            "Epoch 10\n",
            "loss: 0.000326  [  0/167876]\n",
            "loss: 0.000380  [2800/167876]\n",
            "loss: 0.000514  [5600/167876]\n",
            "loss: 0.000423  [8400/167876]\n",
            "loss: 0.000402  [11200/167876]\n",
            "loss: 0.000347  [14000/167876]\n",
            "loss: 0.000522  [16800/167876]\n",
            "loss: 0.000507  [19600/167876]\n",
            "loss: 0.000358  [22400/167876]\n",
            "loss: 0.000419  [25200/167876]\n",
            "loss: 0.000527  [28000/167876]\n",
            "loss: 0.000462  [30800/167876]\n",
            "loss: 0.000368  [33600/167876]\n",
            "loss: 0.000385  [36400/167876]\n",
            "loss: 0.000271  [39200/167876]\n",
            "loss: 0.000541  [42000/167876]\n",
            "loss: 0.000375  [44800/167876]\n",
            "loss: 0.000366  [47600/167876]\n",
            "loss: 0.000375  [50400/167876]\n",
            "loss: 0.000378  [53200/167876]\n",
            "loss: 0.000406  [56000/167876]\n",
            "loss: 0.000307  [58800/167876]\n",
            "loss: 0.000435  [61600/167876]\n",
            "loss: 0.000363  [64400/167876]\n",
            "loss: 0.000402  [67200/167876]\n",
            "loss: 0.000327  [70000/167876]\n",
            "loss: 0.000489  [72800/167876]\n",
            "loss: 0.000415  [75600/167876]\n",
            "loss: 0.000333  [78400/167876]\n",
            "loss: 0.000308  [81200/167876]\n",
            "loss: 0.000350  [84000/167876]\n",
            "loss: 0.000264  [86800/167876]\n",
            "loss: 0.000349  [89600/167876]\n",
            "loss: 0.000486  [92400/167876]\n",
            "loss: 0.000409  [95200/167876]\n",
            "loss: 0.000399  [98000/167876]\n",
            "loss: 0.000336  [100800/167876]\n",
            "loss: 0.000416  [103600/167876]\n",
            "loss: 0.000469  [106400/167876]\n",
            "loss: 0.000426  [109200/167876]\n",
            "loss: 0.000482  [112000/167876]\n",
            "loss: 0.000439  [114800/167876]\n",
            "loss: 0.000438  [117600/167876]\n",
            "loss: 0.000449  [120400/167876]\n",
            "loss: 0.000398  [123200/167876]\n",
            "loss: 0.000433  [126000/167876]\n",
            "loss: 0.000503  [128800/167876]\n",
            "loss: 0.000312  [131600/167876]\n",
            "loss: 0.000421  [134400/167876]\n",
            "loss: 0.000591  [137200/167876]\n",
            "loss: 0.000464  [140000/167876]\n",
            "loss: 0.000344  [142800/167876]\n",
            "loss: 0.000378  [145600/167876]\n",
            "loss: 0.000472  [148400/167876]\n",
            "loss: 0.000376  [151200/167876]\n",
            "loss: 0.000283  [154000/167876]\n",
            "loss: 0.000426  [156800/167876]\n",
            "loss: 0.000404  [159600/167876]\n",
            "loss: 0.000446  [162400/167876]\n",
            "loss: 0.000446  [165200/167876]\n",
            "distortion gain: avg MSE: 0.000454, avg abs error: 0.0163\n",
            "learning rate: 0.000190 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 11\n",
            "loss: 0.000346  [  0/167876]\n",
            "loss: 0.000323  [2800/167876]\n",
            "loss: 0.000318  [5600/167876]\n",
            "loss: 0.000262  [8400/167876]\n",
            "loss: 0.000379  [11200/167876]\n",
            "loss: 0.000329  [14000/167876]\n",
            "loss: 0.000349  [16800/167876]\n",
            "loss: 0.000341  [19600/167876]\n",
            "loss: 0.000323  [22400/167876]\n",
            "loss: 0.000351  [25200/167876]\n",
            "loss: 0.000369  [28000/167876]\n",
            "loss: 0.000360  [30800/167876]\n",
            "loss: 0.000339  [33600/167876]\n",
            "loss: 0.000310  [36400/167876]\n",
            "loss: 0.000409  [39200/167876]\n",
            "loss: 0.000474  [42000/167876]\n",
            "loss: 0.000341  [44800/167876]\n",
            "loss: 0.000370  [47600/167876]\n",
            "loss: 0.000300  [50400/167876]\n",
            "loss: 0.000322  [53200/167876]\n",
            "loss: 0.000315  [56000/167876]\n",
            "loss: 0.000245  [58800/167876]\n",
            "loss: 0.000455  [61600/167876]\n",
            "loss: 0.000403  [64400/167876]\n",
            "loss: 0.000333  [67200/167876]\n",
            "loss: 0.000348  [70000/167876]\n",
            "loss: 0.000424  [72800/167876]\n",
            "loss: 0.000393  [75600/167876]\n",
            "loss: 0.000302  [78400/167876]\n",
            "loss: 0.000441  [81200/167876]\n",
            "loss: 0.000475  [84000/167876]\n",
            "loss: 0.000268  [86800/167876]\n",
            "loss: 0.000334  [89600/167876]\n",
            "loss: 0.000325  [92400/167876]\n",
            "loss: 0.000279  [95200/167876]\n",
            "loss: 0.000270  [98000/167876]\n",
            "loss: 0.000292  [100800/167876]\n",
            "loss: 0.000346  [103600/167876]\n",
            "loss: 0.000400  [106400/167876]\n",
            "loss: 0.000423  [109200/167876]\n",
            "loss: 0.000364  [112000/167876]\n",
            "loss: 0.000335  [114800/167876]\n",
            "loss: 0.000454  [117600/167876]\n",
            "loss: 0.000320  [120400/167876]\n",
            "loss: 0.000397  [123200/167876]\n",
            "loss: 0.000335  [126000/167876]\n",
            "loss: 0.000320  [128800/167876]\n",
            "loss: 0.000369  [131600/167876]\n",
            "loss: 0.000340  [134400/167876]\n",
            "loss: 0.000278  [137200/167876]\n",
            "loss: 0.000370  [140000/167876]\n",
            "loss: 0.000330  [142800/167876]\n",
            "loss: 0.000290  [145600/167876]\n",
            "loss: 0.000301  [148400/167876]\n",
            "loss: 0.000300  [151200/167876]\n",
            "loss: 0.000399  [154000/167876]\n",
            "loss: 0.000336  [156800/167876]\n",
            "loss: 0.000337  [159600/167876]\n",
            "loss: 0.000320  [162400/167876]\n",
            "loss: 0.000445  [165200/167876]\n",
            "distortion gain: avg MSE: 0.000409, avg abs error: 0.0155\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 12\n",
            "loss: 0.000384  [  0/167876]\n",
            "loss: 0.000327  [2800/167876]\n",
            "loss: 0.000303  [5600/167876]\n",
            "loss: 0.000250  [8400/167876]\n",
            "loss: 0.000279  [11200/167876]\n",
            "loss: 0.000457  [14000/167876]\n",
            "loss: 0.000403  [16800/167876]\n",
            "loss: 0.000386  [19600/167876]\n",
            "loss: 0.000377  [22400/167876]\n",
            "loss: 0.000344  [25200/167876]\n",
            "loss: 0.000330  [28000/167876]\n",
            "loss: 0.000359  [30800/167876]\n",
            "loss: 0.000371  [33600/167876]\n",
            "loss: 0.000399  [36400/167876]\n",
            "loss: 0.000314  [39200/167876]\n",
            "loss: 0.000357  [42000/167876]\n",
            "loss: 0.000363  [44800/167876]\n",
            "loss: 0.000349  [47600/167876]\n",
            "loss: 0.000388  [50400/167876]\n",
            "loss: 0.000426  [53200/167876]\n",
            "loss: 0.000431  [56000/167876]\n",
            "loss: 0.000371  [58800/167876]\n",
            "loss: 0.000335  [61600/167876]\n",
            "loss: 0.000344  [64400/167876]\n",
            "loss: 0.000460  [67200/167876]\n",
            "loss: 0.000307  [70000/167876]\n",
            "loss: 0.000300  [72800/167876]\n",
            "loss: 0.000335  [75600/167876]\n",
            "loss: 0.000390  [78400/167876]\n",
            "loss: 0.000326  [81200/167876]\n",
            "loss: 0.000322  [84000/167876]\n",
            "loss: 0.000299  [86800/167876]\n",
            "loss: 0.000380  [89600/167876]\n",
            "loss: 0.000379  [92400/167876]\n",
            "loss: 0.000452  [95200/167876]\n",
            "loss: 0.000410  [98000/167876]\n",
            "loss: 0.000340  [100800/167876]\n",
            "loss: 0.000402  [103600/167876]\n",
            "loss: 0.000321  [106400/167876]\n",
            "loss: 0.000379  [109200/167876]\n",
            "loss: 0.000381  [112000/167876]\n",
            "loss: 0.000335  [114800/167876]\n",
            "loss: 0.000356  [117600/167876]\n",
            "loss: 0.000446  [120400/167876]\n",
            "loss: 0.000328  [123200/167876]\n",
            "loss: 0.000298  [126000/167876]\n",
            "loss: 0.000334  [128800/167876]\n",
            "loss: 0.000277  [131600/167876]\n",
            "loss: 0.000356  [134400/167876]\n",
            "loss: 0.000317  [137200/167876]\n",
            "loss: 0.000335  [140000/167876]\n",
            "loss: 0.000404  [142800/167876]\n",
            "loss: 0.000278  [145600/167876]\n",
            "loss: 0.000375  [148400/167876]\n",
            "loss: 0.000284  [151200/167876]\n",
            "loss: 0.000278  [154000/167876]\n",
            "loss: 0.000331  [156800/167876]\n",
            "loss: 0.000340  [159600/167876]\n",
            "loss: 0.000280  [162400/167876]\n",
            "loss: 0.000312  [165200/167876]\n",
            "distortion gain: avg MSE: 0.000395, avg abs error: 0.0151\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 13\n",
            "loss: 0.000309  [  0/167876]\n",
            "loss: 0.000266  [2800/167876]\n",
            "loss: 0.000347  [5600/167876]\n",
            "loss: 0.000288  [8400/167876]\n",
            "loss: 0.000364  [11200/167876]\n",
            "loss: 0.000366  [14000/167876]\n",
            "loss: 0.000273  [16800/167876]\n",
            "loss: 0.000262  [19600/167876]\n",
            "loss: 0.000411  [22400/167876]\n",
            "loss: 0.000331  [25200/167876]\n",
            "loss: 0.000312  [28000/167876]\n",
            "loss: 0.000317  [30800/167876]\n",
            "loss: 0.000252  [33600/167876]\n",
            "loss: 0.000415  [36400/167876]\n",
            "loss: 0.000347  [39200/167876]\n",
            "loss: 0.000292  [42000/167876]\n",
            "loss: 0.000296  [44800/167876]\n",
            "loss: 0.000362  [47600/167876]\n",
            "loss: 0.000263  [50400/167876]\n",
            "loss: 0.000401  [53200/167876]\n",
            "loss: 0.000361  [56000/167876]\n",
            "loss: 0.000357  [58800/167876]\n",
            "loss: 0.000357  [61600/167876]\n",
            "loss: 0.000312  [64400/167876]\n",
            "loss: 0.000289  [67200/167876]\n",
            "loss: 0.000277  [70000/167876]\n",
            "loss: 0.000354  [72800/167876]\n",
            "loss: 0.000374  [75600/167876]\n",
            "loss: 0.000335  [78400/167876]\n",
            "loss: 0.000343  [81200/167876]\n",
            "loss: 0.000309  [84000/167876]\n",
            "loss: 0.000247  [86800/167876]\n",
            "loss: 0.000428  [89600/167876]\n",
            "loss: 0.000334  [92400/167876]\n",
            "loss: 0.000371  [95200/167876]\n",
            "loss: 0.000387  [98000/167876]\n",
            "loss: 0.000451  [100800/167876]\n",
            "loss: 0.000361  [103600/167876]\n",
            "loss: 0.000380  [106400/167876]\n",
            "loss: 0.000319  [109200/167876]\n",
            "loss: 0.000332  [112000/167876]\n",
            "loss: 0.000308  [114800/167876]\n",
            "loss: 0.000335  [117600/167876]\n",
            "loss: 0.000241  [120400/167876]\n",
            "loss: 0.000329  [123200/167876]\n",
            "loss: 0.000303  [126000/167876]\n",
            "loss: 0.000215  [128800/167876]\n",
            "loss: 0.000344  [131600/167876]\n",
            "loss: 0.000325  [134400/167876]\n",
            "loss: 0.000414  [137200/167876]\n",
            "loss: 0.000323  [140000/167876]\n",
            "loss: 0.000479  [142800/167876]\n",
            "loss: 0.000343  [145600/167876]\n",
            "loss: 0.000302  [148400/167876]\n",
            "loss: 0.000340  [151200/167876]\n",
            "loss: 0.000303  [154000/167876]\n",
            "loss: 0.000269  [156800/167876]\n",
            "loss: 0.000350  [159600/167876]\n",
            "loss: 0.000354  [162400/167876]\n",
            "loss: 0.000318  [165200/167876]\n",
            "distortion gain: avg MSE: 0.000469, avg abs error: 0.0169\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 14\n",
            "loss: 0.000347  [  0/167876]\n",
            "loss: 0.000453  [2800/167876]\n",
            "loss: 0.000345  [5600/167876]\n",
            "loss: 0.000330  [8400/167876]\n",
            "loss: 0.000273  [11200/167876]\n",
            "loss: 0.000393  [14000/167876]\n",
            "loss: 0.000303  [16800/167876]\n",
            "loss: 0.000303  [19600/167876]\n",
            "loss: 0.000325  [22400/167876]\n",
            "loss: 0.000323  [25200/167876]\n",
            "loss: 0.000283  [28000/167876]\n",
            "loss: 0.000343  [30800/167876]\n",
            "loss: 0.000249  [33600/167876]\n",
            "loss: 0.000306  [36400/167876]\n",
            "loss: 0.000315  [39200/167876]\n",
            "loss: 0.000374  [42000/167876]\n",
            "loss: 0.000292  [44800/167876]\n",
            "loss: 0.000335  [47600/167876]\n",
            "loss: 0.000459  [50400/167876]\n",
            "loss: 0.000336  [53200/167876]\n",
            "loss: 0.000358  [56000/167876]\n",
            "loss: 0.000310  [58800/167876]\n",
            "loss: 0.000289  [61600/167876]\n",
            "loss: 0.000287  [64400/167876]\n",
            "loss: 0.000216  [67200/167876]\n",
            "loss: 0.000317  [70000/167876]\n",
            "loss: 0.000325  [72800/167876]\n",
            "loss: 0.000350  [75600/167876]\n",
            "loss: 0.000372  [78400/167876]\n",
            "loss: 0.000376  [81200/167876]\n",
            "loss: 0.000355  [84000/167876]\n",
            "loss: 0.000354  [86800/167876]\n",
            "loss: 0.000266  [89600/167876]\n",
            "loss: 0.000267  [92400/167876]\n",
            "loss: 0.000381  [95200/167876]\n",
            "loss: 0.000245  [98000/167876]\n",
            "loss: 0.000318  [100800/167876]\n",
            "loss: 0.000299  [103600/167876]\n",
            "loss: 0.000381  [106400/167876]\n",
            "loss: 0.000283  [109200/167876]\n",
            "loss: 0.000344  [112000/167876]\n",
            "loss: 0.000363  [114800/167876]\n",
            "loss: 0.000340  [117600/167876]\n",
            "loss: 0.000402  [120400/167876]\n",
            "loss: 0.000396  [123200/167876]\n",
            "loss: 0.000285  [126000/167876]\n",
            "loss: 0.000309  [128800/167876]\n",
            "loss: 0.000306  [131600/167876]\n",
            "loss: 0.000307  [134400/167876]\n",
            "loss: 0.000276  [137200/167876]\n",
            "loss: 0.000270  [140000/167876]\n",
            "loss: 0.000323  [142800/167876]\n",
            "loss: 0.000305  [145600/167876]\n",
            "loss: 0.000366  [148400/167876]\n",
            "loss: 0.000238  [151200/167876]\n",
            "loss: 0.000253  [154000/167876]\n",
            "loss: 0.000343  [156800/167876]\n",
            "loss: 0.000372  [159600/167876]\n",
            "loss: 0.000322  [162400/167876]\n",
            "loss: 0.000246  [165200/167876]\n",
            "distortion gain: avg MSE: 0.000415, avg abs error: 0.0156\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 15\n",
            "loss: 0.000303  [  0/167876]\n",
            "loss: 0.000306  [2800/167876]\n",
            "loss: 0.000274  [5600/167876]\n",
            "loss: 0.000304  [8400/167876]\n",
            "loss: 0.000377  [11200/167876]\n",
            "loss: 0.000329  [14000/167876]\n",
            "loss: 0.000307  [16800/167876]\n",
            "loss: 0.000384  [19600/167876]\n",
            "loss: 0.000285  [22400/167876]\n",
            "loss: 0.000385  [25200/167876]\n",
            "loss: 0.000205  [28000/167876]\n",
            "loss: 0.000326  [30800/167876]\n",
            "loss: 0.000242  [33600/167876]\n",
            "loss: 0.000296  [36400/167876]\n",
            "loss: 0.000332  [39200/167876]\n",
            "loss: 0.000323  [42000/167876]\n",
            "loss: 0.000283  [44800/167876]\n",
            "loss: 0.000310  [47600/167876]\n",
            "loss: 0.000310  [50400/167876]\n",
            "loss: 0.000297  [53200/167876]\n",
            "loss: 0.000355  [56000/167876]\n",
            "loss: 0.000299  [58800/167876]\n",
            "loss: 0.000277  [61600/167876]\n",
            "loss: 0.000290  [64400/167876]\n",
            "loss: 0.000451  [67200/167876]\n",
            "loss: 0.000254  [70000/167876]\n",
            "loss: 0.000276  [72800/167876]\n",
            "loss: 0.000265  [75600/167876]\n",
            "loss: 0.000263  [78400/167876]\n",
            "loss: 0.000303  [81200/167876]\n",
            "loss: 0.000335  [84000/167876]\n",
            "loss: 0.000272  [86800/167876]\n",
            "loss: 0.000318  [89600/167876]\n",
            "loss: 0.000358  [92400/167876]\n",
            "loss: 0.000298  [95200/167876]\n",
            "loss: 0.000387  [98000/167876]\n",
            "loss: 0.000351  [100800/167876]\n",
            "loss: 0.000293  [103600/167876]\n",
            "loss: 0.000194  [106400/167876]\n",
            "loss: 0.000343  [109200/167876]\n",
            "loss: 0.000271  [112000/167876]\n",
            "loss: 0.000359  [114800/167876]\n",
            "loss: 0.000303  [117600/167876]\n",
            "loss: 0.000309  [120400/167876]\n",
            "loss: 0.000257  [123200/167876]\n",
            "loss: 0.000347  [126000/167876]\n",
            "loss: 0.000390  [128800/167876]\n",
            "loss: 0.000317  [131600/167876]\n",
            "loss: 0.000417  [134400/167876]\n",
            "loss: 0.000299  [137200/167876]\n",
            "loss: 0.000320  [140000/167876]\n",
            "loss: 0.000247  [142800/167876]\n",
            "loss: 0.000326  [145600/167876]\n",
            "loss: 0.000282  [148400/167876]\n",
            "loss: 0.000338  [151200/167876]\n",
            "loss: 0.000291  [154000/167876]\n",
            "loss: 0.000365  [156800/167876]\n",
            "loss: 0.000377  [159600/167876]\n",
            "loss: 0.000446  [162400/167876]\n",
            "loss: 0.000272  [165200/167876]\n",
            "distortion gain: avg MSE: 0.000409, avg abs error: 0.0154\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 16\n",
            "loss: 0.000313  [  0/167876]\n",
            "loss: 0.000366  [2800/167876]\n",
            "loss: 0.000315  [5600/167876]\n",
            "loss: 0.000480  [8400/167876]\n",
            "loss: 0.000266  [11200/167876]\n",
            "loss: 0.000255  [14000/167876]\n",
            "loss: 0.000314  [16800/167876]\n",
            "loss: 0.000256  [19600/167876]\n",
            "loss: 0.000247  [22400/167876]\n",
            "loss: 0.000291  [25200/167876]\n",
            "loss: 0.000316  [28000/167876]\n",
            "loss: 0.000277  [30800/167876]\n",
            "loss: 0.000286  [33600/167876]\n",
            "loss: 0.000325  [36400/167876]\n",
            "loss: 0.000238  [39200/167876]\n",
            "loss: 0.000270  [42000/167876]\n",
            "loss: 0.000250  [44800/167876]\n",
            "loss: 0.000336  [47600/167876]\n",
            "loss: 0.000329  [50400/167876]\n",
            "loss: 0.000304  [53200/167876]\n",
            "loss: 0.000289  [56000/167876]\n",
            "loss: 0.000337  [58800/167876]\n",
            "loss: 0.000407  [61600/167876]\n",
            "loss: 0.000322  [64400/167876]\n",
            "loss: 0.000264  [67200/167876]\n",
            "loss: 0.000354  [70000/167876]\n",
            "loss: 0.000248  [72800/167876]\n",
            "loss: 0.000287  [75600/167876]\n",
            "loss: 0.000442  [78400/167876]\n",
            "loss: 0.000287  [81200/167876]\n",
            "loss: 0.000256  [84000/167876]\n",
            "loss: 0.000258  [86800/167876]\n",
            "loss: 0.000328  [89600/167876]\n",
            "loss: 0.000363  [92400/167876]\n",
            "loss: 0.000477  [95200/167876]\n",
            "loss: 0.000359  [98000/167876]\n",
            "loss: 0.000243  [100800/167876]\n",
            "loss: 0.000319  [103600/167876]\n",
            "loss: 0.000330  [106400/167876]\n",
            "loss: 0.000259  [109200/167876]\n",
            "loss: 0.000250  [112000/167876]\n",
            "loss: 0.000367  [114800/167876]\n",
            "loss: 0.000283  [117600/167876]\n",
            "loss: 0.000288  [120400/167876]\n",
            "loss: 0.000235  [123200/167876]\n",
            "loss: 0.000313  [126000/167876]\n",
            "loss: 0.000318  [128800/167876]\n",
            "loss: 0.000303  [131600/167876]\n",
            "loss: 0.000264  [134400/167876]\n",
            "loss: 0.000333  [137200/167876]\n",
            "loss: 0.000326  [140000/167876]\n",
            "loss: 0.000294  [142800/167876]\n",
            "loss: 0.000360  [145600/167876]\n",
            "loss: 0.000248  [148400/167876]\n",
            "loss: 0.000273  [151200/167876]\n",
            "loss: 0.000291  [154000/167876]\n",
            "loss: 0.000298  [156800/167876]\n",
            "loss: 0.000284  [159600/167876]\n",
            "loss: 0.000247  [162400/167876]\n",
            "loss: 0.000329  [165200/167876]\n",
            "distortion gain: avg MSE: 0.000495, avg abs error: 0.0173\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 17\n",
            "loss: 0.000405  [  0/167876]\n",
            "loss: 0.000267  [2800/167876]\n",
            "loss: 0.000225  [5600/167876]\n",
            "loss: 0.000299  [8400/167876]\n",
            "loss: 0.000251  [11200/167876]\n",
            "loss: 0.000280  [14000/167876]\n",
            "loss: 0.000245  [16800/167876]\n",
            "loss: 0.000306  [19600/167876]\n",
            "loss: 0.000226  [22400/167876]\n",
            "loss: 0.000287  [25200/167876]\n",
            "loss: 0.000361  [28000/167876]\n",
            "loss: 0.000343  [30800/167876]\n",
            "loss: 0.000207  [33600/167876]\n",
            "loss: 0.000293  [36400/167876]\n",
            "loss: 0.000292  [39200/167876]\n",
            "loss: 0.000374  [42000/167876]\n",
            "loss: 0.000310  [44800/167876]\n",
            "loss: 0.000282  [47600/167876]\n",
            "loss: 0.000286  [50400/167876]\n",
            "loss: 0.000286  [53200/167876]\n",
            "loss: 0.000388  [56000/167876]\n",
            "loss: 0.000321  [58800/167876]\n",
            "loss: 0.000372  [61600/167876]\n",
            "loss: 0.000287  [64400/167876]\n",
            "loss: 0.000243  [67200/167876]\n",
            "loss: 0.000328  [70000/167876]\n",
            "loss: 0.000349  [72800/167876]\n",
            "loss: 0.000298  [75600/167876]\n",
            "loss: 0.000344  [78400/167876]\n",
            "loss: 0.000267  [81200/167876]\n",
            "loss: 0.000299  [84000/167876]\n",
            "loss: 0.000268  [86800/167876]\n",
            "loss: 0.000350  [89600/167876]\n",
            "loss: 0.000320  [92400/167876]\n",
            "loss: 0.000256  [95200/167876]\n",
            "loss: 0.000332  [98000/167876]\n",
            "loss: 0.000260  [100800/167876]\n",
            "loss: 0.000256  [103600/167876]\n",
            "loss: 0.000229  [106400/167876]\n",
            "loss: 0.000354  [109200/167876]\n",
            "loss: 0.000300  [112000/167876]\n",
            "loss: 0.000240  [114800/167876]\n",
            "loss: 0.000297  [117600/167876]\n",
            "loss: 0.000339  [120400/167876]\n",
            "loss: 0.000384  [123200/167876]\n",
            "loss: 0.000314  [126000/167876]\n",
            "loss: 0.000223  [128800/167876]\n",
            "loss: 0.000306  [131600/167876]\n",
            "loss: 0.000285  [134400/167876]\n",
            "loss: 0.000282  [137200/167876]\n",
            "loss: 0.000331  [140000/167876]\n",
            "loss: 0.000265  [142800/167876]\n",
            "loss: 0.000385  [145600/167876]\n",
            "loss: 0.000263  [148400/167876]\n",
            "loss: 0.000285  [151200/167876]\n",
            "loss: 0.000275  [154000/167876]\n",
            "loss: 0.000309  [156800/167876]\n",
            "loss: 0.000286  [159600/167876]\n",
            "loss: 0.000290  [162400/167876]\n",
            "loss: 0.000269  [165200/167876]\n",
            "distortion gain: avg MSE: 0.000398, avg abs error: 0.0151\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 18\n",
            "loss: 0.000282  [  0/167876]\n",
            "loss: 0.000246  [2800/167876]\n",
            "loss: 0.000246  [5600/167876]\n",
            "loss: 0.000307  [8400/167876]\n",
            "loss: 0.000265  [11200/167876]\n",
            "loss: 0.000356  [14000/167876]\n",
            "loss: 0.000279  [16800/167876]\n",
            "loss: 0.000353  [19600/167876]\n",
            "loss: 0.000251  [22400/167876]\n",
            "loss: 0.000383  [25200/167876]\n",
            "loss: 0.000201  [28000/167876]\n",
            "loss: 0.000248  [30800/167876]\n",
            "loss: 0.000357  [33600/167876]\n",
            "loss: 0.000298  [36400/167876]\n",
            "loss: 0.000288  [39200/167876]\n",
            "loss: 0.000235  [42000/167876]\n",
            "loss: 0.000348  [44800/167876]\n",
            "loss: 0.000335  [47600/167876]\n",
            "loss: 0.000248  [50400/167876]\n",
            "loss: 0.000268  [53200/167876]\n",
            "loss: 0.000293  [56000/167876]\n",
            "loss: 0.000312  [58800/167876]\n",
            "loss: 0.000257  [61600/167876]\n",
            "loss: 0.000249  [64400/167876]\n",
            "loss: 0.000310  [67200/167876]\n",
            "loss: 0.000265  [70000/167876]\n",
            "loss: 0.000276  [72800/167876]\n",
            "loss: 0.000320  [75600/167876]\n",
            "loss: 0.000278  [78400/167876]\n",
            "loss: 0.000372  [81200/167876]\n",
            "loss: 0.000279  [84000/167876]\n",
            "loss: 0.000193  [86800/167876]\n",
            "loss: 0.000288  [89600/167876]\n",
            "loss: 0.000290  [92400/167876]\n",
            "loss: 0.000223  [95200/167876]\n",
            "loss: 0.000301  [98000/167876]\n",
            "loss: 0.000317  [100800/167876]\n",
            "loss: 0.000301  [103600/167876]\n",
            "loss: 0.000283  [106400/167876]\n",
            "loss: 0.000242  [109200/167876]\n",
            "loss: 0.000304  [112000/167876]\n",
            "loss: 0.000315  [114800/167876]\n",
            "loss: 0.000266  [117600/167876]\n",
            "loss: 0.000387  [120400/167876]\n",
            "loss: 0.000309  [123200/167876]\n",
            "loss: 0.000326  [126000/167876]\n",
            "loss: 0.000316  [128800/167876]\n",
            "loss: 0.000274  [131600/167876]\n",
            "loss: 0.000300  [134400/167876]\n",
            "loss: 0.000346  [137200/167876]\n",
            "loss: 0.000367  [140000/167876]\n",
            "loss: 0.000300  [142800/167876]\n",
            "loss: 0.000234  [145600/167876]\n",
            "loss: 0.000282  [148400/167876]\n",
            "loss: 0.000219  [151200/167876]\n",
            "loss: 0.000340  [154000/167876]\n",
            "loss: 0.000237  [156800/167876]\n",
            "loss: 0.000258  [159600/167876]\n",
            "loss: 0.000314  [162400/167876]\n",
            "loss: 0.000369  [165200/167876]\n",
            "distortion gain: avg MSE: 0.000427, avg abs error: 0.0161\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 19\n",
            "loss: 0.000272  [  0/167876]\n",
            "loss: 0.000251  [2800/167876]\n",
            "loss: 0.000334  [5600/167876]\n",
            "loss: 0.000306  [8400/167876]\n",
            "loss: 0.000249  [11200/167876]\n",
            "loss: 0.000443  [14000/167876]\n",
            "loss: 0.000361  [16800/167876]\n",
            "loss: 0.000232  [19600/167876]\n",
            "loss: 0.000264  [22400/167876]\n",
            "loss: 0.000275  [25200/167876]\n",
            "loss: 0.000259  [28000/167876]\n",
            "loss: 0.000221  [30800/167876]\n",
            "loss: 0.000250  [33600/167876]\n",
            "loss: 0.000437  [36400/167876]\n",
            "loss: 0.000267  [39200/167876]\n",
            "loss: 0.000297  [42000/167876]\n",
            "loss: 0.000289  [44800/167876]\n",
            "loss: 0.000332  [47600/167876]\n",
            "loss: 0.000278  [50400/167876]\n",
            "loss: 0.000244  [53200/167876]\n",
            "loss: 0.000410  [56000/167876]\n",
            "loss: 0.000254  [58800/167876]\n",
            "loss: 0.000258  [61600/167876]\n",
            "loss: 0.000228  [64400/167876]\n",
            "loss: 0.000286  [67200/167876]\n",
            "loss: 0.000264  [70000/167876]\n",
            "loss: 0.000309  [72800/167876]\n",
            "loss: 0.000318  [75600/167876]\n",
            "loss: 0.000362  [78400/167876]\n",
            "loss: 0.000222  [81200/167876]\n",
            "loss: 0.000202  [84000/167876]\n",
            "loss: 0.000337  [86800/167876]\n",
            "loss: 0.000354  [89600/167876]\n",
            "loss: 0.000236  [92400/167876]\n",
            "loss: 0.000323  [95200/167876]\n",
            "loss: 0.000241  [98000/167876]\n",
            "loss: 0.000267  [100800/167876]\n",
            "loss: 0.000334  [103600/167876]\n",
            "loss: 0.000313  [106400/167876]\n",
            "loss: 0.000328  [109200/167876]\n",
            "loss: 0.000299  [112000/167876]\n",
            "loss: 0.000315  [114800/167876]\n",
            "loss: 0.000201  [117600/167876]\n",
            "loss: 0.000250  [120400/167876]\n",
            "loss: 0.000319  [123200/167876]\n",
            "loss: 0.000222  [126000/167876]\n",
            "loss: 0.000235  [128800/167876]\n",
            "loss: 0.000350  [131600/167876]\n",
            "loss: 0.000333  [134400/167876]\n",
            "loss: 0.000303  [137200/167876]\n",
            "loss: 0.000326  [140000/167876]\n",
            "loss: 0.000241  [142800/167876]\n",
            "loss: 0.000262  [145600/167876]\n",
            "loss: 0.000261  [148400/167876]\n",
            "loss: 0.000263  [151200/167876]\n",
            "loss: 0.000266  [154000/167876]\n",
            "loss: 0.000281  [156800/167876]\n",
            "loss: 0.000273  [159600/167876]\n",
            "loss: 0.000287  [162400/167876]\n",
            "loss: 0.000365  [165200/167876]\n",
            "distortion gain: avg MSE: 0.000365, avg abs error: 0.0144\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 20\n",
            "loss: 0.000284  [  0/167876]\n",
            "loss: 0.000320  [2800/167876]\n",
            "loss: 0.000352  [5600/167876]\n",
            "loss: 0.000307  [8400/167876]\n",
            "loss: 0.000308  [11200/167876]\n",
            "loss: 0.000253  [14000/167876]\n",
            "loss: 0.000327  [16800/167876]\n",
            "loss: 0.000253  [19600/167876]\n",
            "loss: 0.000298  [22400/167876]\n",
            "loss: 0.000288  [25200/167876]\n",
            "loss: 0.000198  [28000/167876]\n",
            "loss: 0.000249  [30800/167876]\n",
            "loss: 0.000283  [33600/167876]\n",
            "loss: 0.000263  [36400/167876]\n",
            "loss: 0.000230  [39200/167876]\n",
            "loss: 0.000286  [42000/167876]\n",
            "loss: 0.000238  [44800/167876]\n",
            "loss: 0.000213  [47600/167876]\n",
            "loss: 0.000213  [50400/167876]\n",
            "loss: 0.000239  [53200/167876]\n",
            "loss: 0.000304  [56000/167876]\n",
            "loss: 0.000321  [58800/167876]\n",
            "loss: 0.000331  [61600/167876]\n",
            "loss: 0.000287  [64400/167876]\n",
            "loss: 0.000268  [67200/167876]\n",
            "loss: 0.000241  [70000/167876]\n",
            "loss: 0.000244  [72800/167876]\n",
            "loss: 0.000241  [75600/167876]\n",
            "loss: 0.000250  [78400/167876]\n",
            "loss: 0.000303  [81200/167876]\n",
            "loss: 0.000338  [84000/167876]\n",
            "loss: 0.000368  [86800/167876]\n",
            "loss: 0.000247  [89600/167876]\n",
            "loss: 0.000301  [92400/167876]\n",
            "loss: 0.000326  [95200/167876]\n",
            "loss: 0.000295  [98000/167876]\n",
            "loss: 0.000415  [100800/167876]\n",
            "loss: 0.000210  [103600/167876]\n",
            "loss: 0.000302  [106400/167876]\n",
            "loss: 0.000288  [109200/167876]\n",
            "loss: 0.000315  [112000/167876]\n",
            "loss: 0.000263  [114800/167876]\n",
            "loss: 0.000307  [117600/167876]\n",
            "loss: 0.000243  [120400/167876]\n",
            "loss: 0.000267  [123200/167876]\n",
            "loss: 0.000267  [126000/167876]\n",
            "loss: 0.000299  [128800/167876]\n",
            "loss: 0.000341  [131600/167876]\n",
            "loss: 0.000292  [134400/167876]\n",
            "loss: 0.000277  [137200/167876]\n",
            "loss: 0.000272  [140000/167876]\n",
            "loss: 0.000302  [142800/167876]\n",
            "loss: 0.000263  [145600/167876]\n",
            "loss: 0.000324  [148400/167876]\n",
            "loss: 0.000268  [151200/167876]\n",
            "loss: 0.000223  [154000/167876]\n",
            "loss: 0.000276  [156800/167876]\n",
            "loss: 0.000289  [159600/167876]\n",
            "loss: 0.000265  [162400/167876]\n",
            "loss: 0.000278  [165200/167876]\n",
            "distortion gain: avg MSE: 0.000454, avg abs error: 0.0167\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Finished training\n",
            "distortion gain: avg MSE: 0.000451, avg abs error: 0.0167\n",
            "Epoch 1\n",
            "loss: 0.148900  [  0/167876]\n",
            "loss: 0.308508  [2800/167876]\n",
            "loss: 0.064663  [5600/167876]\n",
            "loss: 0.059052  [8400/167876]\n",
            "loss: 0.065148  [11200/167876]\n",
            "loss: 0.051654  [14000/167876]\n",
            "loss: 0.029336  [16800/167876]\n",
            "loss: 0.029037  [19600/167876]\n",
            "loss: 0.031170  [22400/167876]\n",
            "loss: 0.020669  [25200/167876]\n",
            "loss: 0.025870  [28000/167876]\n",
            "loss: 0.029133  [30800/167876]\n",
            "loss: 0.024578  [33600/167876]\n",
            "loss: 0.014577  [36400/167876]\n",
            "loss: 0.014152  [39200/167876]\n",
            "loss: 0.019848  [42000/167876]\n",
            "loss: 0.016714  [44800/167876]\n",
            "loss: 0.018535  [47600/167876]\n",
            "loss: 0.013712  [50400/167876]\n",
            "loss: 0.018042  [53200/167876]\n",
            "loss: 0.015679  [56000/167876]\n",
            "loss: 0.014717  [58800/167876]\n",
            "loss: 0.011034  [61600/167876]\n",
            "loss: 0.022647  [64400/167876]\n",
            "loss: 0.012614  [67200/167876]\n",
            "loss: 0.015094  [70000/167876]\n",
            "loss: 0.013230  [72800/167876]\n",
            "loss: 0.017950  [75600/167876]\n",
            "loss: 0.023362  [78400/167876]\n",
            "loss: 0.013943  [81200/167876]\n",
            "loss: 0.014149  [84000/167876]\n",
            "loss: 0.017180  [86800/167876]\n",
            "loss: 0.014106  [89600/167876]\n",
            "loss: 0.012738  [92400/167876]\n",
            "loss: 0.014376  [95200/167876]\n",
            "loss: 0.012073  [98000/167876]\n",
            "loss: 0.011259  [100800/167876]\n",
            "loss: 0.010349  [103600/167876]\n",
            "loss: 0.019557  [106400/167876]\n",
            "loss: 0.015424  [109200/167876]\n",
            "loss: 0.014466  [112000/167876]\n",
            "loss: 0.012210  [114800/167876]\n",
            "loss: 0.015767  [117600/167876]\n",
            "loss: 0.016430  [120400/167876]\n",
            "loss: 0.010222  [123200/167876]\n",
            "loss: 0.013703  [126000/167876]\n",
            "loss: 0.013610  [128800/167876]\n",
            "loss: 0.009939  [131600/167876]\n",
            "loss: 0.012119  [134400/167876]\n",
            "loss: 0.010442  [137200/167876]\n",
            "loss: 0.012196  [140000/167876]\n",
            "loss: 0.009618  [142800/167876]\n",
            "loss: 0.009855  [145600/167876]\n",
            "loss: 0.008373  [148400/167876]\n",
            "loss: 0.016705  [151200/167876]\n",
            "loss: 0.009230  [154000/167876]\n",
            "loss: 0.012713  [156800/167876]\n",
            "loss: 0.017927  [159600/167876]\n",
            "loss: 0.013104  [162400/167876]\n",
            "loss: 0.009261  [165200/167876]\n",
            "chorus depth: avg MSE: 0.011374, avg abs error: 0.0781\n",
            "learning rate: 0.001000 -> 0.000910\n",
            "---------------------------\n",
            "\n",
            "Epoch 2\n",
            "loss: 0.013264  [  0/167876]\n",
            "loss: 0.009501  [2800/167876]\n",
            "loss: 0.012219  [5600/167876]\n",
            "loss: 0.011402  [8400/167876]\n",
            "loss: 0.010846  [11200/167876]\n",
            "loss: 0.007973  [14000/167876]\n",
            "loss: 0.008552  [16800/167876]\n",
            "loss: 0.010358  [19600/167876]\n",
            "loss: 0.012395  [22400/167876]\n",
            "loss: 0.010757  [25200/167876]\n",
            "loss: 0.014780  [28000/167876]\n",
            "loss: 0.012502  [30800/167876]\n",
            "loss: 0.009062  [33600/167876]\n",
            "loss: 0.011298  [36400/167876]\n",
            "loss: 0.014361  [39200/167876]\n",
            "loss: 0.007695  [42000/167876]\n",
            "loss: 0.013054  [44800/167876]\n",
            "loss: 0.011333  [47600/167876]\n",
            "loss: 0.007079  [50400/167876]\n",
            "loss: 0.012908  [53200/167876]\n",
            "loss: 0.010710  [56000/167876]\n",
            "loss: 0.010859  [58800/167876]\n",
            "loss: 0.014928  [61600/167876]\n",
            "loss: 0.009524  [64400/167876]\n",
            "loss: 0.009232  [67200/167876]\n",
            "loss: 0.013058  [70000/167876]\n",
            "loss: 0.012632  [72800/167876]\n",
            "loss: 0.012638  [75600/167876]\n",
            "loss: 0.008409  [78400/167876]\n",
            "loss: 0.010650  [81200/167876]\n",
            "loss: 0.012334  [84000/167876]\n",
            "loss: 0.012658  [86800/167876]\n",
            "loss: 0.011047  [89600/167876]\n",
            "loss: 0.009085  [92400/167876]\n",
            "loss: 0.006700  [95200/167876]\n",
            "loss: 0.012197  [98000/167876]\n",
            "loss: 0.012186  [100800/167876]\n",
            "loss: 0.015359  [103600/167876]\n",
            "loss: 0.013412  [106400/167876]\n",
            "loss: 0.008640  [109200/167876]\n",
            "loss: 0.011663  [112000/167876]\n",
            "loss: 0.009350  [114800/167876]\n",
            "loss: 0.008952  [117600/167876]\n",
            "loss: 0.008483  [120400/167876]\n",
            "loss: 0.010740  [123200/167876]\n",
            "loss: 0.009610  [126000/167876]\n",
            "loss: 0.006602  [128800/167876]\n",
            "loss: 0.010455  [131600/167876]\n",
            "loss: 0.007952  [134400/167876]\n",
            "loss: 0.007860  [137200/167876]\n",
            "loss: 0.008330  [140000/167876]\n",
            "loss: 0.006906  [142800/167876]\n",
            "loss: 0.009254  [145600/167876]\n",
            "loss: 0.012430  [148400/167876]\n",
            "loss: 0.008304  [151200/167876]\n",
            "loss: 0.007624  [154000/167876]\n",
            "loss: 0.009289  [156800/167876]\n",
            "loss: 0.012447  [159600/167876]\n",
            "loss: 0.007831  [162400/167876]\n",
            "loss: 0.012772  [165200/167876]\n",
            "chorus depth: avg MSE: 0.009268, avg abs error: 0.0707\n",
            "learning rate: 0.000910 -> 0.000820\n",
            "---------------------------\n",
            "\n",
            "Epoch 3\n",
            "loss: 0.011964  [  0/167876]\n",
            "loss: 0.006996  [2800/167876]\n",
            "loss: 0.008495  [5600/167876]\n",
            "loss: 0.011181  [8400/167876]\n",
            "loss: 0.006931  [11200/167876]\n",
            "loss: 0.006193  [14000/167876]\n",
            "loss: 0.009716  [16800/167876]\n",
            "loss: 0.008207  [19600/167876]\n",
            "loss: 0.007435  [22400/167876]\n",
            "loss: 0.011750  [25200/167876]\n",
            "loss: 0.007626  [28000/167876]\n",
            "loss: 0.008337  [30800/167876]\n",
            "loss: 0.005656  [33600/167876]\n",
            "loss: 0.010333  [36400/167876]\n",
            "loss: 0.009636  [39200/167876]\n",
            "loss: 0.009387  [42000/167876]\n",
            "loss: 0.010717  [44800/167876]\n",
            "loss: 0.007664  [47600/167876]\n",
            "loss: 0.007986  [50400/167876]\n",
            "loss: 0.011132  [53200/167876]\n",
            "loss: 0.008714  [56000/167876]\n",
            "loss: 0.007980  [58800/167876]\n",
            "loss: 0.006465  [61600/167876]\n",
            "loss: 0.007865  [64400/167876]\n",
            "loss: 0.015916  [67200/167876]\n",
            "loss: 0.012054  [70000/167876]\n",
            "loss: 0.006962  [72800/167876]\n",
            "loss: 0.009704  [75600/167876]\n",
            "loss: 0.012913  [78400/167876]\n",
            "loss: 0.006897  [81200/167876]\n",
            "loss: 0.008724  [84000/167876]\n",
            "loss: 0.010974  [86800/167876]\n",
            "loss: 0.009532  [89600/167876]\n",
            "loss: 0.009374  [92400/167876]\n",
            "loss: 0.009805  [95200/167876]\n",
            "loss: 0.008062  [98000/167876]\n",
            "loss: 0.006927  [100800/167876]\n",
            "loss: 0.005424  [103600/167876]\n",
            "loss: 0.006770  [106400/167876]\n",
            "loss: 0.010688  [109200/167876]\n",
            "loss: 0.008596  [112000/167876]\n",
            "loss: 0.009249  [114800/167876]\n",
            "loss: 0.007975  [117600/167876]\n",
            "loss: 0.007589  [120400/167876]\n",
            "loss: 0.009838  [123200/167876]\n",
            "loss: 0.008030  [126000/167876]\n",
            "loss: 0.007437  [128800/167876]\n",
            "loss: 0.008942  [131600/167876]\n",
            "loss: 0.008827  [134400/167876]\n",
            "loss: 0.005608  [137200/167876]\n",
            "loss: 0.009190  [140000/167876]\n",
            "loss: 0.007090  [142800/167876]\n",
            "loss: 0.005324  [145600/167876]\n",
            "loss: 0.007552  [148400/167876]\n",
            "loss: 0.007843  [151200/167876]\n",
            "loss: 0.008907  [154000/167876]\n",
            "loss: 0.006188  [156800/167876]\n",
            "loss: 0.006373  [159600/167876]\n",
            "loss: 0.005829  [162400/167876]\n",
            "loss: 0.007683  [165200/167876]\n",
            "chorus depth: avg MSE: 0.009941, avg abs error: 0.0777\n",
            "learning rate: 0.000820 -> 0.000730\n",
            "---------------------------\n",
            "\n",
            "Epoch 4\n",
            "loss: 0.009442  [  0/167876]\n",
            "loss: 0.008383  [2800/167876]\n",
            "loss: 0.006387  [5600/167876]\n",
            "loss: 0.005779  [8400/167876]\n",
            "loss: 0.005537  [11200/167876]\n",
            "loss: 0.005011  [14000/167876]\n",
            "loss: 0.006182  [16800/167876]\n",
            "loss: 0.006824  [19600/167876]\n",
            "loss: 0.005258  [22400/167876]\n",
            "loss: 0.005931  [25200/167876]\n",
            "loss: 0.007959  [28000/167876]\n",
            "loss: 0.005950  [30800/167876]\n",
            "loss: 0.006833  [33600/167876]\n",
            "loss: 0.007068  [36400/167876]\n",
            "loss: 0.007434  [39200/167876]\n",
            "loss: 0.009739  [42000/167876]\n",
            "loss: 0.014717  [44800/167876]\n",
            "loss: 0.006060  [47600/167876]\n",
            "loss: 0.009900  [50400/167876]\n",
            "loss: 0.006620  [53200/167876]\n",
            "loss: 0.004933  [56000/167876]\n",
            "loss: 0.009508  [58800/167876]\n",
            "loss: 0.008405  [61600/167876]\n",
            "loss: 0.007629  [64400/167876]\n",
            "loss: 0.010044  [67200/167876]\n",
            "loss: 0.008611  [70000/167876]\n",
            "loss: 0.006817  [72800/167876]\n",
            "loss: 0.008150  [75600/167876]\n",
            "loss: 0.009423  [78400/167876]\n",
            "loss: 0.008952  [81200/167876]\n",
            "loss: 0.005248  [84000/167876]\n",
            "loss: 0.006686  [86800/167876]\n",
            "loss: 0.005472  [89600/167876]\n",
            "loss: 0.008013  [92400/167876]\n",
            "loss: 0.006342  [95200/167876]\n",
            "loss: 0.007754  [98000/167876]\n",
            "loss: 0.008909  [100800/167876]\n",
            "loss: 0.010171  [103600/167876]\n",
            "loss: 0.008334  [106400/167876]\n",
            "loss: 0.006410  [109200/167876]\n",
            "loss: 0.006362  [112000/167876]\n",
            "loss: 0.006761  [114800/167876]\n",
            "loss: 0.005638  [117600/167876]\n",
            "loss: 0.006174  [120400/167876]\n",
            "loss: 0.006532  [123200/167876]\n",
            "loss: 0.004444  [126000/167876]\n",
            "loss: 0.007400  [128800/167876]\n",
            "loss: 0.006706  [131600/167876]\n",
            "loss: 0.010545  [134400/167876]\n",
            "loss: 0.007278  [137200/167876]\n",
            "loss: 0.006908  [140000/167876]\n",
            "loss: 0.006714  [142800/167876]\n",
            "loss: 0.006307  [145600/167876]\n",
            "loss: 0.007821  [148400/167876]\n",
            "loss: 0.006252  [151200/167876]\n",
            "loss: 0.009477  [154000/167876]\n",
            "loss: 0.004757  [156800/167876]\n",
            "loss: 0.005836  [159600/167876]\n",
            "loss: 0.006069  [162400/167876]\n",
            "loss: 0.006059  [165200/167876]\n",
            "chorus depth: avg MSE: 0.010228, avg abs error: 0.0776\n",
            "learning rate: 0.000730 -> 0.000640\n",
            "---------------------------\n",
            "\n",
            "Epoch 5\n",
            "loss: 0.009747  [  0/167876]\n",
            "loss: 0.005125  [2800/167876]\n",
            "loss: 0.010249  [5600/167876]\n",
            "loss: 0.006469  [8400/167876]\n",
            "loss: 0.005354  [11200/167876]\n",
            "loss: 0.009643  [14000/167876]\n",
            "loss: 0.008830  [16800/167876]\n",
            "loss: 0.006967  [19600/167876]\n",
            "loss: 0.005928  [22400/167876]\n",
            "loss: 0.004647  [25200/167876]\n",
            "loss: 0.005259  [28000/167876]\n",
            "loss: 0.009165  [30800/167876]\n",
            "loss: 0.008948  [33600/167876]\n",
            "loss: 0.006885  [36400/167876]\n",
            "loss: 0.005506  [39200/167876]\n",
            "loss: 0.005254  [42000/167876]\n",
            "loss: 0.005264  [44800/167876]\n",
            "loss: 0.007477  [47600/167876]\n",
            "loss: 0.005535  [50400/167876]\n",
            "loss: 0.005408  [53200/167876]\n",
            "loss: 0.005627  [56000/167876]\n",
            "loss: 0.005095  [58800/167876]\n",
            "loss: 0.003921  [61600/167876]\n",
            "loss: 0.007439  [64400/167876]\n",
            "loss: 0.007622  [67200/167876]\n",
            "loss: 0.004130  [70000/167876]\n",
            "loss: 0.004110  [72800/167876]\n",
            "loss: 0.006815  [75600/167876]\n",
            "loss: 0.007206  [78400/167876]\n",
            "loss: 0.005828  [81200/167876]\n",
            "loss: 0.006467  [84000/167876]\n",
            "loss: 0.007658  [86800/167876]\n",
            "loss: 0.006937  [89600/167876]\n",
            "loss: 0.004439  [92400/167876]\n",
            "loss: 0.004903  [95200/167876]\n",
            "loss: 0.006962  [98000/167876]\n",
            "loss: 0.007332  [100800/167876]\n",
            "loss: 0.009390  [103600/167876]\n",
            "loss: 0.007350  [106400/167876]\n",
            "loss: 0.006500  [109200/167876]\n",
            "loss: 0.007149  [112000/167876]\n",
            "loss: 0.006971  [114800/167876]\n",
            "loss: 0.008130  [117600/167876]\n",
            "loss: 0.005742  [120400/167876]\n",
            "loss: 0.007045  [123200/167876]\n",
            "loss: 0.004321  [126000/167876]\n",
            "loss: 0.006922  [128800/167876]\n",
            "loss: 0.004953  [131600/167876]\n",
            "loss: 0.005037  [134400/167876]\n",
            "loss: 0.004995  [137200/167876]\n",
            "loss: 0.006394  [140000/167876]\n",
            "loss: 0.005860  [142800/167876]\n",
            "loss: 0.005570  [145600/167876]\n",
            "loss: 0.007285  [148400/167876]\n",
            "loss: 0.005515  [151200/167876]\n",
            "loss: 0.006932  [154000/167876]\n",
            "loss: 0.006596  [156800/167876]\n",
            "loss: 0.005459  [159600/167876]\n",
            "loss: 0.005307  [162400/167876]\n",
            "loss: 0.005344  [165200/167876]\n",
            "chorus depth: avg MSE: 0.008972, avg abs error: 0.0744\n",
            "learning rate: 0.000640 -> 0.000550\n",
            "---------------------------\n",
            "\n",
            "Epoch 6\n",
            "loss: 0.010004  [  0/167876]\n",
            "loss: 0.004687  [2800/167876]\n",
            "loss: 0.005179  [5600/167876]\n",
            "loss: 0.003650  [8400/167876]\n",
            "loss: 0.003948  [11200/167876]\n",
            "loss: 0.005544  [14000/167876]\n",
            "loss: 0.006372  [16800/167876]\n",
            "loss: 0.007183  [19600/167876]\n",
            "loss: 0.005970  [22400/167876]\n",
            "loss: 0.006525  [25200/167876]\n",
            "loss: 0.007258  [28000/167876]\n",
            "loss: 0.006623  [30800/167876]\n",
            "loss: 0.006997  [33600/167876]\n",
            "loss: 0.004767  [36400/167876]\n",
            "loss: 0.005837  [39200/167876]\n",
            "loss: 0.005910  [42000/167876]\n",
            "loss: 0.004850  [44800/167876]\n",
            "loss: 0.006475  [47600/167876]\n",
            "loss: 0.003876  [50400/167876]\n",
            "loss: 0.005782  [53200/167876]\n",
            "loss: 0.006012  [56000/167876]\n",
            "loss: 0.005364  [58800/167876]\n",
            "loss: 0.005679  [61600/167876]\n",
            "loss: 0.006171  [64400/167876]\n",
            "loss: 0.005893  [67200/167876]\n",
            "loss: 0.006287  [70000/167876]\n",
            "loss: 0.004659  [72800/167876]\n",
            "loss: 0.006796  [75600/167876]\n",
            "loss: 0.005227  [78400/167876]\n",
            "loss: 0.004191  [81200/167876]\n",
            "loss: 0.008153  [84000/167876]\n",
            "loss: 0.005252  [86800/167876]\n",
            "loss: 0.005299  [89600/167876]\n",
            "loss: 0.004088  [92400/167876]\n",
            "loss: 0.004544  [95200/167876]\n",
            "loss: 0.005002  [98000/167876]\n",
            "loss: 0.007104  [100800/167876]\n",
            "loss: 0.006609  [103600/167876]\n",
            "loss: 0.006409  [106400/167876]\n",
            "loss: 0.006117  [109200/167876]\n",
            "loss: 0.004628  [112000/167876]\n",
            "loss: 0.006894  [114800/167876]\n",
            "loss: 0.005080  [117600/167876]\n",
            "loss: 0.005073  [120400/167876]\n",
            "loss: 0.007526  [123200/167876]\n",
            "loss: 0.005723  [126000/167876]\n",
            "loss: 0.009158  [128800/167876]\n",
            "loss: 0.006376  [131600/167876]\n",
            "loss: 0.005681  [134400/167876]\n",
            "loss: 0.004022  [137200/167876]\n",
            "loss: 0.004185  [140000/167876]\n",
            "loss: 0.004525  [142800/167876]\n",
            "loss: 0.005626  [145600/167876]\n",
            "loss: 0.006541  [148400/167876]\n",
            "loss: 0.006704  [151200/167876]\n",
            "loss: 0.005885  [154000/167876]\n",
            "loss: 0.005335  [156800/167876]\n",
            "loss: 0.004889  [159600/167876]\n",
            "loss: 0.005934  [162400/167876]\n",
            "loss: 0.007596  [165200/167876]\n",
            "chorus depth: avg MSE: 0.005587, avg abs error: 0.0543\n",
            "learning rate: 0.000550 -> 0.000460\n",
            "---------------------------\n",
            "\n",
            "Epoch 7\n",
            "loss: 0.005354  [  0/167876]\n",
            "loss: 0.003083  [2800/167876]\n",
            "loss: 0.004574  [5600/167876]\n",
            "loss: 0.005436  [8400/167876]\n",
            "loss: 0.005186  [11200/167876]\n",
            "loss: 0.003821  [14000/167876]\n",
            "loss: 0.008190  [16800/167876]\n",
            "loss: 0.003596  [19600/167876]\n",
            "loss: 0.004494  [22400/167876]\n",
            "loss: 0.004513  [25200/167876]\n",
            "loss: 0.004572  [28000/167876]\n",
            "loss: 0.004483  [30800/167876]\n",
            "loss: 0.003666  [33600/167876]\n",
            "loss: 0.005084  [36400/167876]\n",
            "loss: 0.004277  [39200/167876]\n",
            "loss: 0.004471  [42000/167876]\n",
            "loss: 0.004857  [44800/167876]\n",
            "loss: 0.004994  [47600/167876]\n",
            "loss: 0.005566  [50400/167876]\n",
            "loss: 0.004500  [53200/167876]\n",
            "loss: 0.003993  [56000/167876]\n",
            "loss: 0.005465  [58800/167876]\n",
            "loss: 0.004875  [61600/167876]\n",
            "loss: 0.008246  [64400/167876]\n",
            "loss: 0.003535  [67200/167876]\n",
            "loss: 0.004924  [70000/167876]\n",
            "loss: 0.004550  [72800/167876]\n",
            "loss: 0.003642  [75600/167876]\n",
            "loss: 0.008306  [78400/167876]\n",
            "loss: 0.007696  [81200/167876]\n",
            "loss: 0.006106  [84000/167876]\n",
            "loss: 0.004442  [86800/167876]\n",
            "loss: 0.004133  [89600/167876]\n",
            "loss: 0.004810  [92400/167876]\n",
            "loss: 0.006335  [95200/167876]\n",
            "loss: 0.004857  [98000/167876]\n",
            "loss: 0.003264  [100800/167876]\n",
            "loss: 0.006498  [103600/167876]\n",
            "loss: 0.004856  [106400/167876]\n",
            "loss: 0.005921  [109200/167876]\n",
            "loss: 0.004706  [112000/167876]\n",
            "loss: 0.005011  [114800/167876]\n",
            "loss: 0.005292  [117600/167876]\n",
            "loss: 0.006146  [120400/167876]\n",
            "loss: 0.006196  [123200/167876]\n",
            "loss: 0.005669  [126000/167876]\n",
            "loss: 0.004733  [128800/167876]\n",
            "loss: 0.004759  [131600/167876]\n",
            "loss: 0.007906  [134400/167876]\n",
            "loss: 0.004893  [137200/167876]\n",
            "loss: 0.004285  [140000/167876]\n",
            "loss: 0.004618  [142800/167876]\n",
            "loss: 0.004326  [145600/167876]\n",
            "loss: 0.004224  [148400/167876]\n",
            "loss: 0.003614  [151200/167876]\n",
            "loss: 0.008379  [154000/167876]\n",
            "loss: 0.004981  [156800/167876]\n",
            "loss: 0.004157  [159600/167876]\n",
            "loss: 0.004675  [162400/167876]\n",
            "loss: 0.004343  [165200/167876]\n",
            "chorus depth: avg MSE: 0.005693, avg abs error: 0.0551\n",
            "learning rate: 0.000460 -> 0.000370\n",
            "---------------------------\n",
            "\n",
            "Epoch 8\n",
            "loss: 0.004457  [  0/167876]\n",
            "loss: 0.004921  [2800/167876]\n",
            "loss: 0.006001  [5600/167876]\n",
            "loss: 0.003014  [8400/167876]\n",
            "loss: 0.004593  [11200/167876]\n",
            "loss: 0.006286  [14000/167876]\n",
            "loss: 0.005182  [16800/167876]\n",
            "loss: 0.003993  [19600/167876]\n",
            "loss: 0.004033  [22400/167876]\n",
            "loss: 0.003616  [25200/167876]\n",
            "loss: 0.005553  [28000/167876]\n",
            "loss: 0.006842  [30800/167876]\n",
            "loss: 0.005406  [33600/167876]\n",
            "loss: 0.007173  [36400/167876]\n",
            "loss: 0.004328  [39200/167876]\n",
            "loss: 0.007192  [42000/167876]\n",
            "loss: 0.004375  [44800/167876]\n",
            "loss: 0.005126  [47600/167876]\n",
            "loss: 0.004935  [50400/167876]\n",
            "loss: 0.004535  [53200/167876]\n",
            "loss: 0.003810  [56000/167876]\n",
            "loss: 0.003636  [58800/167876]\n",
            "loss: 0.005140  [61600/167876]\n",
            "loss: 0.004667  [64400/167876]\n",
            "loss: 0.005443  [67200/167876]\n",
            "loss: 0.004348  [70000/167876]\n",
            "loss: 0.004542  [72800/167876]\n",
            "loss: 0.004459  [75600/167876]\n",
            "loss: 0.005055  [78400/167876]\n",
            "loss: 0.005032  [81200/167876]\n",
            "loss: 0.004408  [84000/167876]\n",
            "loss: 0.004013  [86800/167876]\n",
            "loss: 0.004540  [89600/167876]\n",
            "loss: 0.004398  [92400/167876]\n",
            "loss: 0.002921  [95200/167876]\n",
            "loss: 0.003897  [98000/167876]\n",
            "loss: 0.003052  [100800/167876]\n",
            "loss: 0.003915  [103600/167876]\n",
            "loss: 0.003171  [106400/167876]\n",
            "loss: 0.003395  [109200/167876]\n",
            "loss: 0.004221  [112000/167876]\n",
            "loss: 0.003835  [114800/167876]\n",
            "loss: 0.004537  [117600/167876]\n",
            "loss: 0.004626  [120400/167876]\n",
            "loss: 0.004168  [123200/167876]\n",
            "loss: 0.003724  [126000/167876]\n",
            "loss: 0.004013  [128800/167876]\n",
            "loss: 0.003561  [131600/167876]\n",
            "loss: 0.004353  [134400/167876]\n",
            "loss: 0.003695  [137200/167876]\n",
            "loss: 0.003934  [140000/167876]\n",
            "loss: 0.005185  [142800/167876]\n",
            "loss: 0.004309  [145600/167876]\n",
            "loss: 0.003563  [148400/167876]\n",
            "loss: 0.003347  [151200/167876]\n",
            "loss: 0.004692  [154000/167876]\n",
            "loss: 0.005329  [156800/167876]\n",
            "loss: 0.005279  [159600/167876]\n",
            "loss: 0.006371  [162400/167876]\n",
            "loss: 0.003350  [165200/167876]\n",
            "chorus depth: avg MSE: 0.005294, avg abs error: 0.0526\n",
            "learning rate: 0.000370 -> 0.000280\n",
            "---------------------------\n",
            "\n",
            "Epoch 9\n",
            "loss: 0.004474  [  0/167876]\n",
            "loss: 0.003842  [2800/167876]\n",
            "loss: 0.003544  [5600/167876]\n",
            "loss: 0.004047  [8400/167876]\n",
            "loss: 0.008692  [11200/167876]\n",
            "loss: 0.003044  [14000/167876]\n",
            "loss: 0.003682  [16800/167876]\n",
            "loss: 0.004720  [19600/167876]\n",
            "loss: 0.002882  [22400/167876]\n",
            "loss: 0.004711  [25200/167876]\n",
            "loss: 0.003371  [28000/167876]\n",
            "loss: 0.003174  [30800/167876]\n",
            "loss: 0.003883  [33600/167876]\n",
            "loss: 0.004666  [36400/167876]\n",
            "loss: 0.003684  [39200/167876]\n",
            "loss: 0.003314  [42000/167876]\n",
            "loss: 0.003023  [44800/167876]\n",
            "loss: 0.003815  [47600/167876]\n",
            "loss: 0.003663  [50400/167876]\n",
            "loss: 0.004532  [53200/167876]\n",
            "loss: 0.004055  [56000/167876]\n",
            "loss: 0.003538  [58800/167876]\n",
            "loss: 0.003758  [61600/167876]\n",
            "loss: 0.003523  [64400/167876]\n",
            "loss: 0.004142  [67200/167876]\n",
            "loss: 0.003812  [70000/167876]\n",
            "loss: 0.003747  [72800/167876]\n",
            "loss: 0.003802  [75600/167876]\n",
            "loss: 0.003917  [78400/167876]\n",
            "loss: 0.003446  [81200/167876]\n",
            "loss: 0.003447  [84000/167876]\n",
            "loss: 0.004056  [86800/167876]\n",
            "loss: 0.004605  [89600/167876]\n",
            "loss: 0.004201  [92400/167876]\n",
            "loss: 0.003755  [95200/167876]\n",
            "loss: 0.004240  [98000/167876]\n",
            "loss: 0.003650  [100800/167876]\n",
            "loss: 0.004171  [103600/167876]\n",
            "loss: 0.003370  [106400/167876]\n",
            "loss: 0.004753  [109200/167876]\n",
            "loss: 0.003074  [112000/167876]\n",
            "loss: 0.004055  [114800/167876]\n",
            "loss: 0.004377  [117600/167876]\n",
            "loss: 0.003547  [120400/167876]\n",
            "loss: 0.004136  [123200/167876]\n",
            "loss: 0.003817  [126000/167876]\n",
            "loss: 0.004025  [128800/167876]\n",
            "loss: 0.003249  [131600/167876]\n",
            "loss: 0.003537  [134400/167876]\n",
            "loss: 0.004851  [137200/167876]\n",
            "loss: 0.005401  [140000/167876]\n",
            "loss: 0.004261  [142800/167876]\n",
            "loss: 0.002977  [145600/167876]\n",
            "loss: 0.004791  [148400/167876]\n",
            "loss: 0.004370  [151200/167876]\n",
            "loss: 0.002881  [154000/167876]\n",
            "loss: 0.003245  [156800/167876]\n",
            "loss: 0.004378  [159600/167876]\n",
            "loss: 0.004671  [162400/167876]\n",
            "loss: 0.003852  [165200/167876]\n",
            "chorus depth: avg MSE: 0.004963, avg abs error: 0.0512\n",
            "learning rate: 0.000280 -> 0.000190\n",
            "---------------------------\n",
            "\n",
            "Epoch 10\n",
            "loss: 0.003441  [  0/167876]\n",
            "loss: 0.004565  [2800/167876]\n",
            "loss: 0.004140  [5600/167876]\n",
            "loss: 0.002378  [8400/167876]\n",
            "loss: 0.002436  [11200/167876]\n",
            "loss: 0.003007  [14000/167876]\n",
            "loss: 0.003767  [16800/167876]\n",
            "loss: 0.003340  [19600/167876]\n",
            "loss: 0.003731  [22400/167876]\n",
            "loss: 0.004292  [25200/167876]\n",
            "loss: 0.003007  [28000/167876]\n",
            "loss: 0.002692  [30800/167876]\n",
            "loss: 0.002484  [33600/167876]\n",
            "loss: 0.003483  [36400/167876]\n",
            "loss: 0.004415  [39200/167876]\n",
            "loss: 0.003016  [42000/167876]\n",
            "loss: 0.003915  [44800/167876]\n",
            "loss: 0.004231  [47600/167876]\n",
            "loss: 0.003721  [50400/167876]\n",
            "loss: 0.003250  [53200/167876]\n",
            "loss: 0.003720  [56000/167876]\n",
            "loss: 0.003792  [58800/167876]\n",
            "loss: 0.003419  [61600/167876]\n",
            "loss: 0.002994  [64400/167876]\n",
            "loss: 0.003789  [67200/167876]\n",
            "loss: 0.003155  [70000/167876]\n",
            "loss: 0.003256  [72800/167876]\n",
            "loss: 0.003343  [75600/167876]\n",
            "loss: 0.004639  [78400/167876]\n",
            "loss: 0.002688  [81200/167876]\n",
            "loss: 0.003060  [84000/167876]\n",
            "loss: 0.002899  [86800/167876]\n",
            "loss: 0.003164  [89600/167876]\n",
            "loss: 0.002871  [92400/167876]\n",
            "loss: 0.004213  [95200/167876]\n",
            "loss: 0.003637  [98000/167876]\n",
            "loss: 0.003594  [100800/167876]\n",
            "loss: 0.003273  [103600/167876]\n",
            "loss: 0.003837  [106400/167876]\n",
            "loss: 0.003415  [109200/167876]\n",
            "loss: 0.003141  [112000/167876]\n",
            "loss: 0.004259  [114800/167876]\n",
            "loss: 0.003910  [117600/167876]\n",
            "loss: 0.003544  [120400/167876]\n",
            "loss: 0.003808  [123200/167876]\n",
            "loss: 0.004504  [126000/167876]\n",
            "loss: 0.004455  [128800/167876]\n",
            "loss: 0.002784  [131600/167876]\n",
            "loss: 0.003883  [134400/167876]\n",
            "loss: 0.003232  [137200/167876]\n",
            "loss: 0.003239  [140000/167876]\n",
            "loss: 0.004192  [142800/167876]\n",
            "loss: 0.002990  [145600/167876]\n",
            "loss: 0.003401  [148400/167876]\n",
            "loss: 0.002894  [151200/167876]\n",
            "loss: 0.003443  [154000/167876]\n",
            "loss: 0.003707  [156800/167876]\n",
            "loss: 0.003614  [159600/167876]\n",
            "loss: 0.003529  [162400/167876]\n",
            "loss: 0.003404  [165200/167876]\n",
            "chorus depth: avg MSE: 0.005095, avg abs error: 0.0519\n",
            "learning rate: 0.000190 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 11\n",
            "loss: 0.005439  [  0/167876]\n",
            "loss: 0.003913  [2800/167876]\n",
            "loss: 0.003737  [5600/167876]\n",
            "loss: 0.002856  [8400/167876]\n",
            "loss: 0.003170  [11200/167876]\n",
            "loss: 0.003319  [14000/167876]\n",
            "loss: 0.002978  [16800/167876]\n",
            "loss: 0.003247  [19600/167876]\n",
            "loss: 0.003601  [22400/167876]\n",
            "loss: 0.002127  [25200/167876]\n",
            "loss: 0.003269  [28000/167876]\n",
            "loss: 0.002653  [30800/167876]\n",
            "loss: 0.003623  [33600/167876]\n",
            "loss: 0.003112  [36400/167876]\n",
            "loss: 0.003132  [39200/167876]\n",
            "loss: 0.002840  [42000/167876]\n",
            "loss: 0.003306  [44800/167876]\n",
            "loss: 0.002967  [47600/167876]\n",
            "loss: 0.003130  [50400/167876]\n",
            "loss: 0.004618  [53200/167876]\n",
            "loss: 0.002717  [56000/167876]\n",
            "loss: 0.002332  [58800/167876]\n",
            "loss: 0.002112  [61600/167876]\n",
            "loss: 0.003805  [64400/167876]\n",
            "loss: 0.003027  [67200/167876]\n",
            "loss: 0.002959  [70000/167876]\n",
            "loss: 0.004350  [72800/167876]\n",
            "loss: 0.003624  [75600/167876]\n",
            "loss: 0.003527  [78400/167876]\n",
            "loss: 0.002393  [81200/167876]\n",
            "loss: 0.002744  [84000/167876]\n",
            "loss: 0.002932  [86800/167876]\n",
            "loss: 0.002368  [89600/167876]\n",
            "loss: 0.002626  [92400/167876]\n",
            "loss: 0.002828  [95200/167876]\n",
            "loss: 0.002737  [98000/167876]\n",
            "loss: 0.003197  [100800/167876]\n",
            "loss: 0.002832  [103600/167876]\n",
            "loss: 0.003425  [106400/167876]\n",
            "loss: 0.002883  [109200/167876]\n",
            "loss: 0.002810  [112000/167876]\n",
            "loss: 0.004117  [114800/167876]\n",
            "loss: 0.003071  [117600/167876]\n",
            "loss: 0.002863  [120400/167876]\n",
            "loss: 0.003386  [123200/167876]\n",
            "loss: 0.003639  [126000/167876]\n",
            "loss: 0.003582  [128800/167876]\n",
            "loss: 0.003006  [131600/167876]\n",
            "loss: 0.003313  [134400/167876]\n",
            "loss: 0.002744  [137200/167876]\n",
            "loss: 0.002561  [140000/167876]\n",
            "loss: 0.002815  [142800/167876]\n",
            "loss: 0.003468  [145600/167876]\n",
            "loss: 0.003640  [148400/167876]\n",
            "loss: 0.003923  [151200/167876]\n",
            "loss: 0.003051  [154000/167876]\n",
            "loss: 0.003851  [156800/167876]\n",
            "loss: 0.003015  [159600/167876]\n",
            "loss: 0.002434  [162400/167876]\n",
            "loss: 0.004283  [165200/167876]\n",
            "chorus depth: avg MSE: 0.004757, avg abs error: 0.0495\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 12\n",
            "loss: 0.003328  [  0/167876]\n",
            "loss: 0.003995  [2800/167876]\n",
            "loss: 0.003165  [5600/167876]\n",
            "loss: 0.002604  [8400/167876]\n",
            "loss: 0.002075  [11200/167876]\n",
            "loss: 0.003324  [14000/167876]\n",
            "loss: 0.002624  [16800/167876]\n",
            "loss: 0.003681  [19600/167876]\n",
            "loss: 0.003202  [22400/167876]\n",
            "loss: 0.002390  [25200/167876]\n",
            "loss: 0.003056  [28000/167876]\n",
            "loss: 0.003182  [30800/167876]\n",
            "loss: 0.002369  [33600/167876]\n",
            "loss: 0.002352  [36400/167876]\n",
            "loss: 0.002456  [39200/167876]\n",
            "loss: 0.003139  [42000/167876]\n",
            "loss: 0.003201  [44800/167876]\n",
            "loss: 0.002590  [47600/167876]\n",
            "loss: 0.003098  [50400/167876]\n",
            "loss: 0.002467  [53200/167876]\n",
            "loss: 0.002767  [56000/167876]\n",
            "loss: 0.003817  [58800/167876]\n",
            "loss: 0.002893  [61600/167876]\n",
            "loss: 0.002822  [64400/167876]\n",
            "loss: 0.002736  [67200/167876]\n",
            "loss: 0.003801  [70000/167876]\n",
            "loss: 0.002624  [72800/167876]\n",
            "loss: 0.002236  [75600/167876]\n",
            "loss: 0.003189  [78400/167876]\n",
            "loss: 0.002756  [81200/167876]\n",
            "loss: 0.002703  [84000/167876]\n",
            "loss: 0.003078  [86800/167876]\n",
            "loss: 0.002722  [89600/167876]\n",
            "loss: 0.003812  [92400/167876]\n",
            "loss: 0.003424  [95200/167876]\n",
            "loss: 0.003376  [98000/167876]\n",
            "loss: 0.002905  [100800/167876]\n",
            "loss: 0.002858  [103600/167876]\n",
            "loss: 0.003696  [106400/167876]\n",
            "loss: 0.002602  [109200/167876]\n",
            "loss: 0.002112  [112000/167876]\n",
            "loss: 0.002509  [114800/167876]\n",
            "loss: 0.002312  [117600/167876]\n",
            "loss: 0.002681  [120400/167876]\n",
            "loss: 0.002747  [123200/167876]\n",
            "loss: 0.002383  [126000/167876]\n",
            "loss: 0.003303  [128800/167876]\n",
            "loss: 0.003352  [131600/167876]\n",
            "loss: 0.003191  [134400/167876]\n",
            "loss: 0.003693  [137200/167876]\n",
            "loss: 0.003129  [140000/167876]\n",
            "loss: 0.002680  [142800/167876]\n",
            "loss: 0.002813  [145600/167876]\n",
            "loss: 0.003449  [148400/167876]\n",
            "loss: 0.003482  [151200/167876]\n",
            "loss: 0.002485  [154000/167876]\n",
            "loss: 0.003022  [156800/167876]\n",
            "loss: 0.003324  [159600/167876]\n",
            "loss: 0.003200  [162400/167876]\n",
            "loss: 0.002870  [165200/167876]\n",
            "chorus depth: avg MSE: 0.004710, avg abs error: 0.0497\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 13\n",
            "loss: 0.004079  [  0/167876]\n",
            "loss: 0.003043  [2800/167876]\n",
            "loss: 0.003512  [5600/167876]\n",
            "loss: 0.002576  [8400/167876]\n",
            "loss: 0.002688  [11200/167876]\n",
            "loss: 0.003116  [14000/167876]\n",
            "loss: 0.002481  [16800/167876]\n",
            "loss: 0.002650  [19600/167876]\n",
            "loss: 0.003018  [22400/167876]\n",
            "loss: 0.003164  [25200/167876]\n",
            "loss: 0.002976  [28000/167876]\n",
            "loss: 0.002842  [30800/167876]\n",
            "loss: 0.002576  [33600/167876]\n",
            "loss: 0.002806  [36400/167876]\n",
            "loss: 0.002819  [39200/167876]\n",
            "loss: 0.003400  [42000/167876]\n",
            "loss: 0.002814  [44800/167876]\n",
            "loss: 0.003836  [47600/167876]\n",
            "loss: 0.003622  [50400/167876]\n",
            "loss: 0.002994  [53200/167876]\n",
            "loss: 0.003441  [56000/167876]\n",
            "loss: 0.003929  [58800/167876]\n",
            "loss: 0.002792  [61600/167876]\n",
            "loss: 0.002883  [64400/167876]\n",
            "loss: 0.002356  [67200/167876]\n",
            "loss: 0.002622  [70000/167876]\n",
            "loss: 0.002838  [72800/167876]\n",
            "loss: 0.002680  [75600/167876]\n",
            "loss: 0.003045  [78400/167876]\n",
            "loss: 0.002288  [81200/167876]\n",
            "loss: 0.003153  [84000/167876]\n",
            "loss: 0.002591  [86800/167876]\n",
            "loss: 0.004230  [89600/167876]\n",
            "loss: 0.002452  [92400/167876]\n",
            "loss: 0.003025  [95200/167876]\n",
            "loss: 0.002272  [98000/167876]\n",
            "loss: 0.003153  [100800/167876]\n",
            "loss: 0.002472  [103600/167876]\n",
            "loss: 0.003671  [106400/167876]\n",
            "loss: 0.004059  [109200/167876]\n",
            "loss: 0.002665  [112000/167876]\n",
            "loss: 0.004525  [114800/167876]\n",
            "loss: 0.003184  [117600/167876]\n",
            "loss: 0.002947  [120400/167876]\n",
            "loss: 0.003178  [123200/167876]\n",
            "loss: 0.002657  [126000/167876]\n",
            "loss: 0.002761  [128800/167876]\n",
            "loss: 0.003152  [131600/167876]\n",
            "loss: 0.003150  [134400/167876]\n",
            "loss: 0.003431  [137200/167876]\n",
            "loss: 0.003010  [140000/167876]\n",
            "loss: 0.003371  [142800/167876]\n",
            "loss: 0.002870  [145600/167876]\n",
            "loss: 0.003017  [148400/167876]\n",
            "loss: 0.002835  [151200/167876]\n",
            "loss: 0.003064  [154000/167876]\n",
            "loss: 0.002793  [156800/167876]\n",
            "loss: 0.003285  [159600/167876]\n",
            "loss: 0.002976  [162400/167876]\n",
            "loss: 0.003457  [165200/167876]\n",
            "chorus depth: avg MSE: 0.004594, avg abs error: 0.0488\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 14\n",
            "loss: 0.002854  [  0/167876]\n",
            "loss: 0.003228  [2800/167876]\n",
            "loss: 0.002728  [5600/167876]\n",
            "loss: 0.002769  [8400/167876]\n",
            "loss: 0.003430  [11200/167876]\n",
            "loss: 0.002623  [14000/167876]\n",
            "loss: 0.002724  [16800/167876]\n",
            "loss: 0.002183  [19600/167876]\n",
            "loss: 0.003281  [22400/167876]\n",
            "loss: 0.002369  [25200/167876]\n",
            "loss: 0.003387  [28000/167876]\n",
            "loss: 0.002196  [30800/167876]\n",
            "loss: 0.002486  [33600/167876]\n",
            "loss: 0.002911  [36400/167876]\n",
            "loss: 0.002970  [39200/167876]\n",
            "loss: 0.002002  [42000/167876]\n",
            "loss: 0.002613  [44800/167876]\n",
            "loss: 0.003447  [47600/167876]\n",
            "loss: 0.003144  [50400/167876]\n",
            "loss: 0.002609  [53200/167876]\n",
            "loss: 0.003683  [56000/167876]\n",
            "loss: 0.003595  [58800/167876]\n",
            "loss: 0.003313  [61600/167876]\n",
            "loss: 0.003141  [64400/167876]\n",
            "loss: 0.002721  [67200/167876]\n",
            "loss: 0.003941  [70000/167876]\n",
            "loss: 0.003159  [72800/167876]\n",
            "loss: 0.002740  [75600/167876]\n",
            "loss: 0.003817  [78400/167876]\n",
            "loss: 0.003624  [81200/167876]\n",
            "loss: 0.002776  [84000/167876]\n",
            "loss: 0.002584  [86800/167876]\n",
            "loss: 0.003425  [89600/167876]\n",
            "loss: 0.002755  [92400/167876]\n",
            "loss: 0.003203  [95200/167876]\n",
            "loss: 0.002524  [98000/167876]\n",
            "loss: 0.003683  [100800/167876]\n",
            "loss: 0.002939  [103600/167876]\n",
            "loss: 0.002646  [106400/167876]\n",
            "loss: 0.002900  [109200/167876]\n",
            "loss: 0.003707  [112000/167876]\n",
            "loss: 0.002526  [114800/167876]\n",
            "loss: 0.003715  [117600/167876]\n",
            "loss: 0.003193  [120400/167876]\n",
            "loss: 0.003053  [123200/167876]\n",
            "loss: 0.003866  [126000/167876]\n",
            "loss: 0.002822  [128800/167876]\n",
            "loss: 0.003620  [131600/167876]\n",
            "loss: 0.002326  [134400/167876]\n",
            "loss: 0.002439  [137200/167876]\n",
            "loss: 0.002640  [140000/167876]\n",
            "loss: 0.002478  [142800/167876]\n",
            "loss: 0.002459  [145600/167876]\n",
            "loss: 0.002882  [148400/167876]\n",
            "loss: 0.002943  [151200/167876]\n",
            "loss: 0.002744  [154000/167876]\n",
            "loss: 0.002655  [156800/167876]\n",
            "loss: 0.002393  [159600/167876]\n",
            "loss: 0.003224  [162400/167876]\n",
            "loss: 0.002643  [165200/167876]\n",
            "chorus depth: avg MSE: 0.004624, avg abs error: 0.0489\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 15\n",
            "loss: 0.002050  [  0/167876]\n",
            "loss: 0.002659  [2800/167876]\n",
            "loss: 0.002494  [5600/167876]\n",
            "loss: 0.003079  [8400/167876]\n",
            "loss: 0.002537  [11200/167876]\n",
            "loss: 0.002697  [14000/167876]\n",
            "loss: 0.002410  [16800/167876]\n",
            "loss: 0.002786  [19600/167876]\n",
            "loss: 0.002166  [22400/167876]\n",
            "loss: 0.003076  [25200/167876]\n",
            "loss: 0.002116  [28000/167876]\n",
            "loss: 0.002684  [30800/167876]\n",
            "loss: 0.002525  [33600/167876]\n",
            "loss: 0.002863  [36400/167876]\n",
            "loss: 0.002088  [39200/167876]\n",
            "loss: 0.002482  [42000/167876]\n",
            "loss: 0.002896  [44800/167876]\n",
            "loss: 0.002409  [47600/167876]\n",
            "loss: 0.002715  [50400/167876]\n",
            "loss: 0.003300  [53200/167876]\n",
            "loss: 0.002287  [56000/167876]\n",
            "loss: 0.002552  [58800/167876]\n",
            "loss: 0.002804  [61600/167876]\n",
            "loss: 0.003056  [64400/167876]\n",
            "loss: 0.003289  [67200/167876]\n",
            "loss: 0.002577  [70000/167876]\n",
            "loss: 0.002938  [72800/167876]\n",
            "loss: 0.002484  [75600/167876]\n",
            "loss: 0.002618  [78400/167876]\n",
            "loss: 0.003335  [81200/167876]\n",
            "loss: 0.002051  [84000/167876]\n",
            "loss: 0.002171  [86800/167876]\n",
            "loss: 0.001973  [89600/167876]\n",
            "loss: 0.003044  [92400/167876]\n",
            "loss: 0.002179  [95200/167876]\n",
            "loss: 0.002789  [98000/167876]\n",
            "loss: 0.002376  [100800/167876]\n",
            "loss: 0.002529  [103600/167876]\n",
            "loss: 0.003294  [106400/167876]\n",
            "loss: 0.003476  [109200/167876]\n",
            "loss: 0.002874  [112000/167876]\n",
            "loss: 0.002318  [114800/167876]\n",
            "loss: 0.002949  [117600/167876]\n",
            "loss: 0.002162  [120400/167876]\n",
            "loss: 0.003473  [123200/167876]\n",
            "loss: 0.002389  [126000/167876]\n",
            "loss: 0.002800  [128800/167876]\n",
            "loss: 0.003229  [131600/167876]\n",
            "loss: 0.002293  [134400/167876]\n",
            "loss: 0.003376  [137200/167876]\n",
            "loss: 0.003684  [140000/167876]\n",
            "loss: 0.001732  [142800/167876]\n",
            "loss: 0.002112  [145600/167876]\n",
            "loss: 0.002674  [148400/167876]\n",
            "loss: 0.003346  [151200/167876]\n",
            "loss: 0.002775  [154000/167876]\n",
            "loss: 0.002383  [156800/167876]\n",
            "loss: 0.003099  [159600/167876]\n",
            "loss: 0.002796  [162400/167876]\n",
            "loss: 0.002982  [165200/167876]\n",
            "chorus depth: avg MSE: 0.004989, avg abs error: 0.0516\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 16\n",
            "loss: 0.002910  [  0/167876]\n",
            "loss: 0.002161  [2800/167876]\n",
            "loss: 0.003130  [5600/167876]\n",
            "loss: 0.002533  [8400/167876]\n",
            "loss: 0.002512  [11200/167876]\n",
            "loss: 0.003074  [14000/167876]\n",
            "loss: 0.002457  [16800/167876]\n",
            "loss: 0.003002  [19600/167876]\n",
            "loss: 0.002895  [22400/167876]\n",
            "loss: 0.002931  [25200/167876]\n",
            "loss: 0.002410  [28000/167876]\n",
            "loss: 0.002616  [30800/167876]\n",
            "loss: 0.002691  [33600/167876]\n",
            "loss: 0.002037  [36400/167876]\n",
            "loss: 0.003373  [39200/167876]\n",
            "loss: 0.002388  [42000/167876]\n",
            "loss: 0.002147  [44800/167876]\n",
            "loss: 0.003510  [47600/167876]\n",
            "loss: 0.002660  [50400/167876]\n",
            "loss: 0.002703  [53200/167876]\n",
            "loss: 0.002824  [56000/167876]\n",
            "loss: 0.002184  [58800/167876]\n",
            "loss: 0.001841  [61600/167876]\n",
            "loss: 0.003467  [64400/167876]\n",
            "loss: 0.003452  [67200/167876]\n",
            "loss: 0.002636  [70000/167876]\n",
            "loss: 0.002343  [72800/167876]\n",
            "loss: 0.002284  [75600/167876]\n",
            "loss: 0.002003  [78400/167876]\n",
            "loss: 0.002531  [81200/167876]\n",
            "loss: 0.002311  [84000/167876]\n",
            "loss: 0.002534  [86800/167876]\n",
            "loss: 0.002481  [89600/167876]\n",
            "loss: 0.003806  [92400/167876]\n",
            "loss: 0.002608  [95200/167876]\n",
            "loss: 0.002901  [98000/167876]\n",
            "loss: 0.002586  [100800/167876]\n",
            "loss: 0.003337  [103600/167876]\n",
            "loss: 0.002789  [106400/167876]\n",
            "loss: 0.002267  [109200/167876]\n",
            "loss: 0.002190  [112000/167876]\n",
            "loss: 0.003708  [114800/167876]\n",
            "loss: 0.002319  [117600/167876]\n",
            "loss: 0.003229  [120400/167876]\n",
            "loss: 0.002495  [123200/167876]\n",
            "loss: 0.003070  [126000/167876]\n",
            "loss: 0.002992  [128800/167876]\n",
            "loss: 0.002824  [131600/167876]\n",
            "loss: 0.002992  [134400/167876]\n",
            "loss: 0.002725  [137200/167876]\n",
            "loss: 0.001804  [140000/167876]\n",
            "loss: 0.003524  [142800/167876]\n",
            "loss: 0.003047  [145600/167876]\n",
            "loss: 0.002536  [148400/167876]\n",
            "loss: 0.002473  [151200/167876]\n",
            "loss: 0.002521  [154000/167876]\n",
            "loss: 0.003115  [156800/167876]\n",
            "loss: 0.002268  [159600/167876]\n",
            "loss: 0.003584  [162400/167876]\n",
            "loss: 0.002041  [165200/167876]\n",
            "chorus depth: avg MSE: 0.004765, avg abs error: 0.0499\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 17\n",
            "loss: 0.001971  [  0/167876]\n",
            "loss: 0.003247  [2800/167876]\n",
            "loss: 0.002793  [5600/167876]\n",
            "loss: 0.002404  [8400/167876]\n",
            "loss: 0.003211  [11200/167876]\n",
            "loss: 0.002729  [14000/167876]\n",
            "loss: 0.003104  [16800/167876]\n",
            "loss: 0.002484  [19600/167876]\n",
            "loss: 0.003406  [22400/167876]\n",
            "loss: 0.002524  [25200/167876]\n",
            "loss: 0.003293  [28000/167876]\n",
            "loss: 0.002207  [30800/167876]\n",
            "loss: 0.001875  [33600/167876]\n",
            "loss: 0.002564  [36400/167876]\n",
            "loss: 0.002940  [39200/167876]\n",
            "loss: 0.002376  [42000/167876]\n",
            "loss: 0.004214  [44800/167876]\n",
            "loss: 0.002227  [47600/167876]\n",
            "loss: 0.002068  [50400/167876]\n",
            "loss: 0.003435  [53200/167876]\n",
            "loss: 0.003764  [56000/167876]\n",
            "loss: 0.002788  [58800/167876]\n",
            "loss: 0.003671  [61600/167876]\n",
            "loss: 0.002619  [64400/167876]\n",
            "loss: 0.003064  [67200/167876]\n",
            "loss: 0.002598  [70000/167876]\n",
            "loss: 0.003374  [72800/167876]\n",
            "loss: 0.002379  [75600/167876]\n",
            "loss: 0.002679  [78400/167876]\n",
            "loss: 0.002549  [81200/167876]\n",
            "loss: 0.002829  [84000/167876]\n",
            "loss: 0.002750  [86800/167876]\n",
            "loss: 0.003038  [89600/167876]\n",
            "loss: 0.003649  [92400/167876]\n",
            "loss: 0.002824  [95200/167876]\n",
            "loss: 0.002650  [98000/167876]\n",
            "loss: 0.002411  [100800/167876]\n",
            "loss: 0.001772  [103600/167876]\n",
            "loss: 0.002399  [106400/167876]\n",
            "loss: 0.002056  [109200/167876]\n",
            "loss: 0.002306  [112000/167876]\n",
            "loss: 0.003271  [114800/167876]\n",
            "loss: 0.002045  [117600/167876]\n",
            "loss: 0.002798  [120400/167876]\n",
            "loss: 0.003008  [123200/167876]\n",
            "loss: 0.002274  [126000/167876]\n",
            "loss: 0.002586  [128800/167876]\n",
            "loss: 0.003513  [131600/167876]\n",
            "loss: 0.003347  [134400/167876]\n",
            "loss: 0.002320  [137200/167876]\n",
            "loss: 0.002010  [140000/167876]\n",
            "loss: 0.002460  [142800/167876]\n",
            "loss: 0.002752  [145600/167876]\n",
            "loss: 0.002506  [148400/167876]\n",
            "loss: 0.002600  [151200/167876]\n",
            "loss: 0.002352  [154000/167876]\n",
            "loss: 0.002625  [156800/167876]\n",
            "loss: 0.002512  [159600/167876]\n",
            "loss: 0.002609  [162400/167876]\n",
            "loss: 0.002656  [165200/167876]\n",
            "chorus depth: avg MSE: 0.004563, avg abs error: 0.0488\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 18\n",
            "loss: 0.003731  [  0/167876]\n",
            "loss: 0.002374  [2800/167876]\n",
            "loss: 0.002258  [5600/167876]\n",
            "loss: 0.002573  [8400/167876]\n",
            "loss: 0.001999  [11200/167876]\n",
            "loss: 0.002655  [14000/167876]\n",
            "loss: 0.002458  [16800/167876]\n",
            "loss: 0.002260  [19600/167876]\n",
            "loss: 0.002252  [22400/167876]\n",
            "loss: 0.002514  [25200/167876]\n",
            "loss: 0.003727  [28000/167876]\n",
            "loss: 0.002673  [30800/167876]\n",
            "loss: 0.002121  [33600/167876]\n",
            "loss: 0.002222  [36400/167876]\n",
            "loss: 0.003188  [39200/167876]\n",
            "loss: 0.002063  [42000/167876]\n",
            "loss: 0.002608  [44800/167876]\n",
            "loss: 0.002520  [47600/167876]\n",
            "loss: 0.002466  [50400/167876]\n",
            "loss: 0.003423  [53200/167876]\n",
            "loss: 0.002487  [56000/167876]\n",
            "loss: 0.002872  [58800/167876]\n",
            "loss: 0.002488  [61600/167876]\n",
            "loss: 0.001890  [64400/167876]\n",
            "loss: 0.003112  [67200/167876]\n",
            "loss: 0.002708  [70000/167876]\n",
            "loss: 0.003489  [72800/167876]\n",
            "loss: 0.002456  [75600/167876]\n",
            "loss: 0.002320  [78400/167876]\n",
            "loss: 0.002726  [81200/167876]\n",
            "loss: 0.002516  [84000/167876]\n",
            "loss: 0.002620  [86800/167876]\n",
            "loss: 0.003488  [89600/167876]\n",
            "loss: 0.002518  [92400/167876]\n",
            "loss: 0.002636  [95200/167876]\n",
            "loss: 0.002958  [98000/167876]\n",
            "loss: 0.002763  [100800/167876]\n",
            "loss: 0.003052  [103600/167876]\n",
            "loss: 0.002545  [106400/167876]\n",
            "loss: 0.002312  [109200/167876]\n",
            "loss: 0.002456  [112000/167876]\n",
            "loss: 0.002617  [114800/167876]\n",
            "loss: 0.002270  [117600/167876]\n",
            "loss: 0.002274  [120400/167876]\n",
            "loss: 0.002394  [123200/167876]\n",
            "loss: 0.003554  [126000/167876]\n",
            "loss: 0.002469  [128800/167876]\n",
            "loss: 0.002162  [131600/167876]\n",
            "loss: 0.002618  [134400/167876]\n",
            "loss: 0.002420  [137200/167876]\n",
            "loss: 0.003092  [140000/167876]\n",
            "loss: 0.002436  [142800/167876]\n",
            "loss: 0.002589  [145600/167876]\n",
            "loss: 0.002915  [148400/167876]\n",
            "loss: 0.003567  [151200/167876]\n",
            "loss: 0.003061  [154000/167876]\n",
            "loss: 0.003080  [156800/167876]\n",
            "loss: 0.002809  [159600/167876]\n",
            "loss: 0.002594  [162400/167876]\n",
            "loss: 0.003013  [165200/167876]\n",
            "chorus depth: avg MSE: 0.004719, avg abs error: 0.0493\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 19\n",
            "loss: 0.002421  [  0/167876]\n",
            "loss: 0.003198  [2800/167876]\n",
            "loss: 0.002289  [5600/167876]\n",
            "loss: 0.003124  [8400/167876]\n",
            "loss: 0.002540  [11200/167876]\n",
            "loss: 0.003289  [14000/167876]\n",
            "loss: 0.002541  [16800/167876]\n",
            "loss: 0.002972  [19600/167876]\n",
            "loss: 0.003089  [22400/167876]\n",
            "loss: 0.002075  [25200/167876]\n",
            "loss: 0.003106  [28000/167876]\n",
            "loss: 0.002002  [30800/167876]\n",
            "loss: 0.002204  [33600/167876]\n",
            "loss: 0.002834  [36400/167876]\n",
            "loss: 0.002668  [39200/167876]\n",
            "loss: 0.003185  [42000/167876]\n",
            "loss: 0.002979  [44800/167876]\n",
            "loss: 0.002296  [47600/167876]\n",
            "loss: 0.002329  [50400/167876]\n",
            "loss: 0.002630  [53200/167876]\n",
            "loss: 0.002464  [56000/167876]\n",
            "loss: 0.002187  [58800/167876]\n",
            "loss: 0.002356  [61600/167876]\n",
            "loss: 0.002652  [64400/167876]\n",
            "loss: 0.002387  [67200/167876]\n",
            "loss: 0.003033  [70000/167876]\n",
            "loss: 0.002758  [72800/167876]\n",
            "loss: 0.001816  [75600/167876]\n",
            "loss: 0.002776  [78400/167876]\n",
            "loss: 0.002284  [81200/167876]\n",
            "loss: 0.003145  [84000/167876]\n",
            "loss: 0.002968  [86800/167876]\n",
            "loss: 0.002822  [89600/167876]\n",
            "loss: 0.003303  [92400/167876]\n",
            "loss: 0.002701  [95200/167876]\n",
            "loss: 0.001927  [98000/167876]\n",
            "loss: 0.002139  [100800/167876]\n",
            "loss: 0.002487  [103600/167876]\n",
            "loss: 0.002381  [106400/167876]\n",
            "loss: 0.003001  [109200/167876]\n",
            "loss: 0.002644  [112000/167876]\n",
            "loss: 0.002228  [114800/167876]\n",
            "loss: 0.001973  [117600/167876]\n",
            "loss: 0.003209  [120400/167876]\n",
            "loss: 0.003152  [123200/167876]\n",
            "loss: 0.002715  [126000/167876]\n",
            "loss: 0.002060  [128800/167876]\n",
            "loss: 0.002207  [131600/167876]\n",
            "loss: 0.002471  [134400/167876]\n",
            "loss: 0.002483  [137200/167876]\n",
            "loss: 0.002175  [140000/167876]\n",
            "loss: 0.002853  [142800/167876]\n",
            "loss: 0.002054  [145600/167876]\n",
            "loss: 0.003118  [148400/167876]\n",
            "loss: 0.003274  [151200/167876]\n",
            "loss: 0.003017  [154000/167876]\n",
            "loss: 0.002390  [156800/167876]\n",
            "loss: 0.002522  [159600/167876]\n",
            "loss: 0.002617  [162400/167876]\n",
            "loss: 0.002174  [165200/167876]\n",
            "chorus depth: avg MSE: 0.004565, avg abs error: 0.0487\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 20\n",
            "loss: 0.001825  [  0/167876]\n",
            "loss: 0.001916  [2800/167876]\n",
            "loss: 0.002569  [5600/167876]\n",
            "loss: 0.002311  [8400/167876]\n",
            "loss: 0.002239  [11200/167876]\n",
            "loss: 0.003109  [14000/167876]\n",
            "loss: 0.003053  [16800/167876]\n",
            "loss: 0.002745  [19600/167876]\n",
            "loss: 0.003016  [22400/167876]\n",
            "loss: 0.002724  [25200/167876]\n",
            "loss: 0.001961  [28000/167876]\n",
            "loss: 0.002357  [30800/167876]\n",
            "loss: 0.002927  [33600/167876]\n",
            "loss: 0.002284  [36400/167876]\n",
            "loss: 0.002034  [39200/167876]\n",
            "loss: 0.002520  [42000/167876]\n",
            "loss: 0.003261  [44800/167876]\n",
            "loss: 0.002907  [47600/167876]\n",
            "loss: 0.002831  [50400/167876]\n",
            "loss: 0.002815  [53200/167876]\n",
            "loss: 0.002426  [56000/167876]\n",
            "loss: 0.002803  [58800/167876]\n",
            "loss: 0.003407  [61600/167876]\n",
            "loss: 0.002074  [64400/167876]\n",
            "loss: 0.002682  [67200/167876]\n",
            "loss: 0.002668  [70000/167876]\n",
            "loss: 0.002301  [72800/167876]\n",
            "loss: 0.002260  [75600/167876]\n",
            "loss: 0.002314  [78400/167876]\n",
            "loss: 0.001727  [81200/167876]\n",
            "loss: 0.002223  [84000/167876]\n",
            "loss: 0.002710  [86800/167876]\n",
            "loss: 0.002464  [89600/167876]\n",
            "loss: 0.002492  [92400/167876]\n",
            "loss: 0.001934  [95200/167876]\n",
            "loss: 0.002172  [98000/167876]\n",
            "loss: 0.002046  [100800/167876]\n",
            "loss: 0.002755  [103600/167876]\n",
            "loss: 0.002274  [106400/167876]\n",
            "loss: 0.003139  [109200/167876]\n",
            "loss: 0.002726  [112000/167876]\n",
            "loss: 0.002084  [114800/167876]\n",
            "loss: 0.002540  [117600/167876]\n",
            "loss: 0.002233  [120400/167876]\n",
            "loss: 0.002520  [123200/167876]\n",
            "loss: 0.002457  [126000/167876]\n",
            "loss: 0.001969  [128800/167876]\n",
            "loss: 0.003120  [131600/167876]\n",
            "loss: 0.002392  [134400/167876]\n",
            "loss: 0.002432  [137200/167876]\n",
            "loss: 0.001788  [140000/167876]\n",
            "loss: 0.002870  [142800/167876]\n",
            "loss: 0.002303  [145600/167876]\n",
            "loss: 0.002973  [148400/167876]\n",
            "loss: 0.002000  [151200/167876]\n",
            "loss: 0.002517  [154000/167876]\n",
            "loss: 0.002014  [156800/167876]\n",
            "loss: 0.002683  [159600/167876]\n",
            "loss: 0.002324  [162400/167876]\n",
            "loss: 0.002512  [165200/167876]\n",
            "chorus depth: avg MSE: 0.004527, avg abs error: 0.0486\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Finished training\n",
            "chorus depth: avg MSE: 0.004569, avg abs error: 0.049\n",
            "Epoch 1\n",
            "loss: 0.117615  [  0/167876]\n",
            "loss: 0.285272  [2800/167876]\n",
            "loss: 0.302222  [5600/167876]\n",
            "loss: 0.071552  [8400/167876]\n",
            "loss: 0.064868  [11200/167876]\n",
            "loss: 0.069713  [14000/167876]\n",
            "loss: 0.055437  [16800/167876]\n",
            "loss: 0.035411  [19600/167876]\n",
            "loss: 0.030247  [22400/167876]\n",
            "loss: 0.014541  [25200/167876]\n",
            "loss: 0.016406  [28000/167876]\n",
            "loss: 0.020314  [30800/167876]\n",
            "loss: 0.018667  [33600/167876]\n",
            "loss: 0.017138  [36400/167876]\n",
            "loss: 0.017913  [39200/167876]\n",
            "loss: 0.010919  [42000/167876]\n",
            "loss: 0.014130  [44800/167876]\n",
            "loss: 0.022815  [47600/167876]\n",
            "loss: 0.019066  [50400/167876]\n",
            "loss: 0.010589  [53200/167876]\n",
            "loss: 0.011588  [56000/167876]\n",
            "loss: 0.008270  [58800/167876]\n",
            "loss: 0.019205  [61600/167876]\n",
            "loss: 0.009016  [64400/167876]\n",
            "loss: 0.007819  [67200/167876]\n",
            "loss: 0.010442  [70000/167876]\n",
            "loss: 0.011285  [72800/167876]\n",
            "loss: 0.008340  [75600/167876]\n",
            "loss: 0.011162  [78400/167876]\n",
            "loss: 0.015255  [81200/167876]\n",
            "loss: 0.006363  [84000/167876]\n",
            "loss: 0.009308  [86800/167876]\n",
            "loss: 0.008967  [89600/167876]\n",
            "loss: 0.008001  [92400/167876]\n",
            "loss: 0.005595  [95200/167876]\n",
            "loss: 0.006547  [98000/167876]\n",
            "loss: 0.010593  [100800/167876]\n",
            "loss: 0.006255  [103600/167876]\n",
            "loss: 0.005992  [106400/167876]\n",
            "loss: 0.010272  [109200/167876]\n",
            "loss: 0.008824  [112000/167876]\n",
            "loss: 0.009057  [114800/167876]\n",
            "loss: 0.008088  [117600/167876]\n",
            "loss: 0.006251  [120400/167876]\n",
            "loss: 0.004290  [123200/167876]\n",
            "loss: 0.006347  [126000/167876]\n",
            "loss: 0.006471  [128800/167876]\n",
            "loss: 0.011127  [131600/167876]\n",
            "loss: 0.007619  [134400/167876]\n",
            "loss: 0.011554  [137200/167876]\n",
            "loss: 0.007074  [140000/167876]\n",
            "loss: 0.007552  [142800/167876]\n",
            "loss: 0.011768  [145600/167876]\n",
            "loss: 0.005900  [148400/167876]\n",
            "loss: 0.009175  [151200/167876]\n",
            "loss: 0.007320  [154000/167876]\n",
            "loss: 0.007333  [156800/167876]\n",
            "loss: 0.005550  [159600/167876]\n",
            "loss: 0.006352  [162400/167876]\n",
            "loss: 0.008650  [165200/167876]\n",
            "tremolo rate: avg MSE: 0.015173, avg abs error: 0.0944\n",
            "learning rate: 0.001000 -> 0.000910\n",
            "---------------------------\n",
            "\n",
            "Epoch 2\n",
            "loss: 0.007232  [  0/167876]\n",
            "loss: 0.006900  [2800/167876]\n",
            "loss: 0.006336  [5600/167876]\n",
            "loss: 0.003921  [8400/167876]\n",
            "loss: 0.005362  [11200/167876]\n",
            "loss: 0.007189  [14000/167876]\n",
            "loss: 0.005726  [16800/167876]\n",
            "loss: 0.005895  [19600/167876]\n",
            "loss: 0.007110  [22400/167876]\n",
            "loss: 0.008945  [25200/167876]\n",
            "loss: 0.005109  [28000/167876]\n",
            "loss: 0.007295  [30800/167876]\n",
            "loss: 0.005171  [33600/167876]\n",
            "loss: 0.008423  [36400/167876]\n",
            "loss: 0.003265  [39200/167876]\n",
            "loss: 0.004544  [42000/167876]\n",
            "loss: 0.006635  [44800/167876]\n",
            "loss: 0.006931  [47600/167876]\n",
            "loss: 0.013648  [50400/167876]\n",
            "loss: 0.005420  [53200/167876]\n",
            "loss: 0.007058  [56000/167876]\n",
            "loss: 0.004031  [58800/167876]\n",
            "loss: 0.004534  [61600/167876]\n",
            "loss: 0.004142  [64400/167876]\n",
            "loss: 0.007999  [67200/167876]\n",
            "loss: 0.008826  [70000/167876]\n",
            "loss: 0.009221  [72800/167876]\n",
            "loss: 0.007220  [75600/167876]\n",
            "loss: 0.004418  [78400/167876]\n",
            "loss: 0.005893  [81200/167876]\n",
            "loss: 0.005345  [84000/167876]\n",
            "loss: 0.005200  [86800/167876]\n",
            "loss: 0.008450  [89600/167876]\n",
            "loss: 0.008118  [92400/167876]\n",
            "loss: 0.006873  [95200/167876]\n",
            "loss: 0.007408  [98000/167876]\n",
            "loss: 0.005051  [100800/167876]\n",
            "loss: 0.006311  [103600/167876]\n",
            "loss: 0.002727  [106400/167876]\n",
            "loss: 0.003985  [109200/167876]\n",
            "loss: 0.007077  [112000/167876]\n",
            "loss: 0.006516  [114800/167876]\n",
            "loss: 0.004952  [117600/167876]\n",
            "loss: 0.004629  [120400/167876]\n",
            "loss: 0.003035  [123200/167876]\n",
            "loss: 0.004418  [126000/167876]\n",
            "loss: 0.006853  [128800/167876]\n",
            "loss: 0.004703  [131600/167876]\n",
            "loss: 0.005152  [134400/167876]\n",
            "loss: 0.004952  [137200/167876]\n",
            "loss: 0.004283  [140000/167876]\n",
            "loss: 0.007086  [142800/167876]\n",
            "loss: 0.009949  [145600/167876]\n",
            "loss: 0.003124  [148400/167876]\n",
            "loss: 0.006031  [151200/167876]\n",
            "loss: 0.007801  [154000/167876]\n",
            "loss: 0.009321  [156800/167876]\n",
            "loss: 0.004769  [159600/167876]\n",
            "loss: 0.004656  [162400/167876]\n",
            "loss: 0.005627  [165200/167876]\n",
            "tremolo rate: avg MSE: 0.007849, avg abs error: 0.0536\n",
            "learning rate: 0.000910 -> 0.000820\n",
            "---------------------------\n",
            "\n",
            "Epoch 3\n",
            "loss: 0.005572  [  0/167876]\n",
            "loss: 0.004925  [2800/167876]\n",
            "loss: 0.005252  [5600/167876]\n",
            "loss: 0.003796  [8400/167876]\n",
            "loss: 0.005655  [11200/167876]\n",
            "loss: 0.009383  [14000/167876]\n",
            "loss: 0.005168  [16800/167876]\n",
            "loss: 0.003895  [19600/167876]\n",
            "loss: 0.003739  [22400/167876]\n",
            "loss: 0.006688  [25200/167876]\n",
            "loss: 0.003114  [28000/167876]\n",
            "loss: 0.005038  [30800/167876]\n",
            "loss: 0.004316  [33600/167876]\n",
            "loss: 0.005441  [36400/167876]\n",
            "loss: 0.010604  [39200/167876]\n",
            "loss: 0.003529  [42000/167876]\n",
            "loss: 0.005526  [44800/167876]\n",
            "loss: 0.003785  [47600/167876]\n",
            "loss: 0.009617  [50400/167876]\n",
            "loss: 0.002777  [53200/167876]\n",
            "loss: 0.003090  [56000/167876]\n",
            "loss: 0.002652  [58800/167876]\n",
            "loss: 0.005350  [61600/167876]\n",
            "loss: 0.007061  [64400/167876]\n",
            "loss: 0.004875  [67200/167876]\n",
            "loss: 0.004591  [70000/167876]\n",
            "loss: 0.004319  [72800/167876]\n",
            "loss: 0.003544  [75600/167876]\n",
            "loss: 0.005078  [78400/167876]\n",
            "loss: 0.004055  [81200/167876]\n",
            "loss: 0.002933  [84000/167876]\n",
            "loss: 0.005720  [86800/167876]\n",
            "loss: 0.005518  [89600/167876]\n",
            "loss: 0.004070  [92400/167876]\n",
            "loss: 0.004256  [95200/167876]\n",
            "loss: 0.005749  [98000/167876]\n",
            "loss: 0.002803  [100800/167876]\n",
            "loss: 0.005743  [103600/167876]\n",
            "loss: 0.005813  [106400/167876]\n",
            "loss: 0.007312  [109200/167876]\n",
            "loss: 0.005898  [112000/167876]\n",
            "loss: 0.003039  [114800/167876]\n",
            "loss: 0.003277  [117600/167876]\n",
            "loss: 0.004078  [120400/167876]\n",
            "loss: 0.002942  [123200/167876]\n",
            "loss: 0.005590  [126000/167876]\n",
            "loss: 0.003327  [128800/167876]\n",
            "loss: 0.004142  [131600/167876]\n",
            "loss: 0.003299  [134400/167876]\n",
            "loss: 0.005641  [137200/167876]\n",
            "loss: 0.002067  [140000/167876]\n",
            "loss: 0.004543  [142800/167876]\n",
            "loss: 0.003707  [145600/167876]\n",
            "loss: 0.002954  [148400/167876]\n",
            "loss: 0.003308  [151200/167876]\n",
            "loss: 0.007621  [154000/167876]\n",
            "loss: 0.004218  [156800/167876]\n",
            "loss: 0.003676  [159600/167876]\n",
            "loss: 0.002592  [162400/167876]\n",
            "loss: 0.006611  [165200/167876]\n",
            "tremolo rate: avg MSE: 0.006327, avg abs error: 0.0588\n",
            "learning rate: 0.000820 -> 0.000730\n",
            "---------------------------\n",
            "\n",
            "Epoch 4\n",
            "loss: 0.005254  [  0/167876]\n",
            "loss: 0.006211  [2800/167876]\n",
            "loss: 0.003515  [5600/167876]\n",
            "loss: 0.004595  [8400/167876]\n",
            "loss: 0.004084  [11200/167876]\n",
            "loss: 0.001868  [14000/167876]\n",
            "loss: 0.005002  [16800/167876]\n",
            "loss: 0.007940  [19600/167876]\n",
            "loss: 0.002835  [22400/167876]\n",
            "loss: 0.002980  [25200/167876]\n",
            "loss: 0.003361  [28000/167876]\n",
            "loss: 0.002502  [30800/167876]\n",
            "loss: 0.002633  [33600/167876]\n",
            "loss: 0.005020  [36400/167876]\n",
            "loss: 0.003840  [39200/167876]\n",
            "loss: 0.004001  [42000/167876]\n",
            "loss: 0.002935  [44800/167876]\n",
            "loss: 0.002797  [47600/167876]\n",
            "loss: 0.002419  [50400/167876]\n",
            "loss: 0.006146  [53200/167876]\n",
            "loss: 0.005693  [56000/167876]\n",
            "loss: 0.003982  [58800/167876]\n",
            "loss: 0.003852  [61600/167876]\n",
            "loss: 0.002512  [64400/167876]\n",
            "loss: 0.005768  [67200/167876]\n",
            "loss: 0.003087  [70000/167876]\n",
            "loss: 0.005203  [72800/167876]\n",
            "loss: 0.004129  [75600/167876]\n",
            "loss: 0.004245  [78400/167876]\n",
            "loss: 0.005370  [81200/167876]\n",
            "loss: 0.004692  [84000/167876]\n",
            "loss: 0.003801  [86800/167876]\n",
            "loss: 0.003049  [89600/167876]\n",
            "loss: 0.004123  [92400/167876]\n",
            "loss: 0.003878  [95200/167876]\n",
            "loss: 0.005139  [98000/167876]\n",
            "loss: 0.003378  [100800/167876]\n",
            "loss: 0.004291  [103600/167876]\n",
            "loss: 0.004083  [106400/167876]\n",
            "loss: 0.002767  [109200/167876]\n",
            "loss: 0.005477  [112000/167876]\n",
            "loss: 0.003540  [114800/167876]\n",
            "loss: 0.004460  [117600/167876]\n",
            "loss: 0.003094  [120400/167876]\n",
            "loss: 0.002181  [123200/167876]\n",
            "loss: 0.005991  [126000/167876]\n",
            "loss: 0.004473  [128800/167876]\n",
            "loss: 0.003417  [131600/167876]\n",
            "loss: 0.005425  [134400/167876]\n",
            "loss: 0.002510  [137200/167876]\n",
            "loss: 0.003529  [140000/167876]\n",
            "loss: 0.003657  [142800/167876]\n",
            "loss: 0.002895  [145600/167876]\n",
            "loss: 0.004563  [148400/167876]\n",
            "loss: 0.004828  [151200/167876]\n",
            "loss: 0.003875  [154000/167876]\n",
            "loss: 0.004551  [156800/167876]\n",
            "loss: 0.004141  [159600/167876]\n",
            "loss: 0.004011  [162400/167876]\n",
            "loss: 0.003923  [165200/167876]\n",
            "tremolo rate: avg MSE: 0.006115, avg abs error: 0.0505\n",
            "learning rate: 0.000730 -> 0.000640\n",
            "---------------------------\n",
            "\n",
            "Epoch 5\n",
            "loss: 0.003280  [  0/167876]\n",
            "loss: 0.003902  [2800/167876]\n",
            "loss: 0.002794  [5600/167876]\n",
            "loss: 0.002465  [8400/167876]\n",
            "loss: 0.002991  [11200/167876]\n",
            "loss: 0.002714  [14000/167876]\n",
            "loss: 0.003243  [16800/167876]\n",
            "loss: 0.004391  [19600/167876]\n",
            "loss: 0.002653  [22400/167876]\n",
            "loss: 0.003016  [25200/167876]\n",
            "loss: 0.005750  [28000/167876]\n",
            "loss: 0.002618  [30800/167876]\n",
            "loss: 0.003736  [33600/167876]\n",
            "loss: 0.005383  [36400/167876]\n",
            "loss: 0.002997  [39200/167876]\n",
            "loss: 0.003044  [42000/167876]\n",
            "loss: 0.002454  [44800/167876]\n",
            "loss: 0.002835  [47600/167876]\n",
            "loss: 0.006618  [50400/167876]\n",
            "loss: 0.003956  [53200/167876]\n",
            "loss: 0.004371  [56000/167876]\n",
            "loss: 0.001626  [58800/167876]\n",
            "loss: 0.002658  [61600/167876]\n",
            "loss: 0.002803  [64400/167876]\n",
            "loss: 0.003599  [67200/167876]\n",
            "loss: 0.004485  [70000/167876]\n",
            "loss: 0.003843  [72800/167876]\n",
            "loss: 0.005438  [75600/167876]\n",
            "loss: 0.002669  [78400/167876]\n",
            "loss: 0.001827  [81200/167876]\n",
            "loss: 0.002625  [84000/167876]\n",
            "loss: 0.002016  [86800/167876]\n",
            "loss: 0.002155  [89600/167876]\n",
            "loss: 0.001914  [92400/167876]\n",
            "loss: 0.002566  [95200/167876]\n",
            "loss: 0.002267  [98000/167876]\n",
            "loss: 0.003388  [100800/167876]\n",
            "loss: 0.004439  [103600/167876]\n",
            "loss: 0.005214  [106400/167876]\n",
            "loss: 0.004074  [109200/167876]\n",
            "loss: 0.003605  [112000/167876]\n",
            "loss: 0.002922  [114800/167876]\n",
            "loss: 0.002032  [117600/167876]\n",
            "loss: 0.003576  [120400/167876]\n",
            "loss: 0.002006  [123200/167876]\n",
            "loss: 0.002592  [126000/167876]\n",
            "loss: 0.003110  [128800/167876]\n",
            "loss: 0.002857  [131600/167876]\n",
            "loss: 0.005007  [134400/167876]\n",
            "loss: 0.003465  [137200/167876]\n",
            "loss: 0.002396  [140000/167876]\n",
            "loss: 0.005297  [142800/167876]\n",
            "loss: 0.002970  [145600/167876]\n",
            "loss: 0.003091  [148400/167876]\n",
            "loss: 0.001734  [151200/167876]\n",
            "loss: 0.002043  [154000/167876]\n",
            "loss: 0.002438  [156800/167876]\n",
            "loss: 0.002850  [159600/167876]\n",
            "loss: 0.001903  [162400/167876]\n",
            "loss: 0.004390  [165200/167876]\n",
            "tremolo rate: avg MSE: 0.004121, avg abs error: 0.0421\n",
            "learning rate: 0.000640 -> 0.000550\n",
            "---------------------------\n",
            "\n",
            "Epoch 6\n",
            "loss: 0.001973  [  0/167876]\n",
            "loss: 0.002539  [2800/167876]\n",
            "loss: 0.002225  [5600/167876]\n",
            "loss: 0.002900  [8400/167876]\n",
            "loss: 0.001649  [11200/167876]\n",
            "loss: 0.001314  [14000/167876]\n",
            "loss: 0.001673  [16800/167876]\n",
            "loss: 0.002074  [19600/167876]\n",
            "loss: 0.004205  [22400/167876]\n",
            "loss: 0.002804  [25200/167876]\n",
            "loss: 0.003649  [28000/167876]\n",
            "loss: 0.004279  [30800/167876]\n",
            "loss: 0.002212  [33600/167876]\n",
            "loss: 0.002303  [36400/167876]\n",
            "loss: 0.002886  [39200/167876]\n",
            "loss: 0.002999  [42000/167876]\n",
            "loss: 0.002345  [44800/167876]\n",
            "loss: 0.002585  [47600/167876]\n",
            "loss: 0.002670  [50400/167876]\n",
            "loss: 0.001517  [53200/167876]\n",
            "loss: 0.003936  [56000/167876]\n",
            "loss: 0.001915  [58800/167876]\n",
            "loss: 0.002761  [61600/167876]\n",
            "loss: 0.002962  [64400/167876]\n",
            "loss: 0.001774  [67200/167876]\n",
            "loss: 0.004532  [70000/167876]\n",
            "loss: 0.001329  [72800/167876]\n",
            "loss: 0.004544  [75600/167876]\n",
            "loss: 0.004815  [78400/167876]\n",
            "loss: 0.002178  [81200/167876]\n",
            "loss: 0.005079  [84000/167876]\n",
            "loss: 0.002826  [86800/167876]\n",
            "loss: 0.003036  [89600/167876]\n",
            "loss: 0.003281  [92400/167876]\n",
            "loss: 0.002326  [95200/167876]\n",
            "loss: 0.001971  [98000/167876]\n",
            "loss: 0.002439  [100800/167876]\n",
            "loss: 0.002106  [103600/167876]\n",
            "loss: 0.001842  [106400/167876]\n",
            "loss: 0.002591  [109200/167876]\n",
            "loss: 0.002754  [112000/167876]\n",
            "loss: 0.001709  [114800/167876]\n",
            "loss: 0.002327  [117600/167876]\n",
            "loss: 0.004439  [120400/167876]\n",
            "loss: 0.002370  [123200/167876]\n",
            "loss: 0.002029  [126000/167876]\n",
            "loss: 0.001987  [128800/167876]\n",
            "loss: 0.004103  [131600/167876]\n",
            "loss: 0.002312  [134400/167876]\n",
            "loss: 0.002496  [137200/167876]\n",
            "loss: 0.002390  [140000/167876]\n",
            "loss: 0.002254  [142800/167876]\n",
            "loss: 0.002234  [145600/167876]\n",
            "loss: 0.003105  [148400/167876]\n",
            "loss: 0.003114  [151200/167876]\n",
            "loss: 0.001968  [154000/167876]\n",
            "loss: 0.004199  [156800/167876]\n",
            "loss: 0.002449  [159600/167876]\n",
            "loss: 0.003382  [162400/167876]\n",
            "loss: 0.002443  [165200/167876]\n",
            "tremolo rate: avg MSE: 0.002631, avg abs error: 0.0303\n",
            "learning rate: 0.000550 -> 0.000460\n",
            "---------------------------\n",
            "\n",
            "Epoch 7\n",
            "loss: 0.001695  [  0/167876]\n",
            "loss: 0.001511  [2800/167876]\n",
            "loss: 0.001945  [5600/167876]\n",
            "loss: 0.002005  [8400/167876]\n",
            "loss: 0.002473  [11200/167876]\n",
            "loss: 0.003145  [14000/167876]\n",
            "loss: 0.002966  [16800/167876]\n",
            "loss: 0.001716  [19600/167876]\n",
            "loss: 0.002856  [22400/167876]\n",
            "loss: 0.001312  [25200/167876]\n",
            "loss: 0.001997  [28000/167876]\n",
            "loss: 0.002367  [30800/167876]\n",
            "loss: 0.003243  [33600/167876]\n",
            "loss: 0.003515  [36400/167876]\n",
            "loss: 0.001205  [39200/167876]\n",
            "loss: 0.002190  [42000/167876]\n",
            "loss: 0.001673  [44800/167876]\n",
            "loss: 0.001065  [47600/167876]\n",
            "loss: 0.002417  [50400/167876]\n",
            "loss: 0.002315  [53200/167876]\n",
            "loss: 0.001825  [56000/167876]\n",
            "loss: 0.001873  [58800/167876]\n",
            "loss: 0.001715  [61600/167876]\n",
            "loss: 0.001728  [64400/167876]\n",
            "loss: 0.002195  [67200/167876]\n",
            "loss: 0.002498  [70000/167876]\n",
            "loss: 0.001677  [72800/167876]\n",
            "loss: 0.002189  [75600/167876]\n",
            "loss: 0.004013  [78400/167876]\n",
            "loss: 0.002117  [81200/167876]\n",
            "loss: 0.002152  [84000/167876]\n",
            "loss: 0.001436  [86800/167876]\n",
            "loss: 0.002235  [89600/167876]\n",
            "loss: 0.001876  [92400/167876]\n",
            "loss: 0.001819  [95200/167876]\n",
            "loss: 0.002755  [98000/167876]\n",
            "loss: 0.001435  [100800/167876]\n",
            "loss: 0.001949  [103600/167876]\n",
            "loss: 0.003166  [106400/167876]\n",
            "loss: 0.001216  [109200/167876]\n",
            "loss: 0.001579  [112000/167876]\n",
            "loss: 0.002457  [114800/167876]\n",
            "loss: 0.001563  [117600/167876]\n",
            "loss: 0.002437  [120400/167876]\n",
            "loss: 0.001023  [123200/167876]\n",
            "loss: 0.001750  [126000/167876]\n",
            "loss: 0.001517  [128800/167876]\n",
            "loss: 0.001869  [131600/167876]\n",
            "loss: 0.002070  [134400/167876]\n",
            "loss: 0.002502  [137200/167876]\n",
            "loss: 0.001673  [140000/167876]\n",
            "loss: 0.003139  [142800/167876]\n",
            "loss: 0.002140  [145600/167876]\n",
            "loss: 0.002601  [148400/167876]\n",
            "loss: 0.003103  [151200/167876]\n",
            "loss: 0.001649  [154000/167876]\n",
            "loss: 0.001492  [156800/167876]\n",
            "loss: 0.002797  [159600/167876]\n",
            "loss: 0.002757  [162400/167876]\n",
            "loss: 0.002281  [165200/167876]\n",
            "tremolo rate: avg MSE: 0.002648, avg abs error: 0.0289\n",
            "learning rate: 0.000460 -> 0.000370\n",
            "---------------------------\n",
            "\n",
            "Epoch 8\n",
            "loss: 0.001079  [  0/167876]\n",
            "loss: 0.001123  [2800/167876]\n",
            "loss: 0.001456  [5600/167876]\n",
            "loss: 0.003043  [8400/167876]\n",
            "loss: 0.002635  [11200/167876]\n",
            "loss: 0.003574  [14000/167876]\n",
            "loss: 0.001271  [16800/167876]\n",
            "loss: 0.001607  [19600/167876]\n",
            "loss: 0.001173  [22400/167876]\n",
            "loss: 0.002264  [25200/167876]\n",
            "loss: 0.002137  [28000/167876]\n",
            "loss: 0.001581  [30800/167876]\n",
            "loss: 0.002601  [33600/167876]\n",
            "loss: 0.002674  [36400/167876]\n",
            "loss: 0.001373  [39200/167876]\n",
            "loss: 0.000891  [42000/167876]\n",
            "loss: 0.001863  [44800/167876]\n",
            "loss: 0.002718  [47600/167876]\n",
            "loss: 0.002175  [50400/167876]\n",
            "loss: 0.001371  [53200/167876]\n",
            "loss: 0.001863  [56000/167876]\n",
            "loss: 0.001388  [58800/167876]\n",
            "loss: 0.001490  [61600/167876]\n",
            "loss: 0.002916  [64400/167876]\n",
            "loss: 0.001423  [67200/167876]\n",
            "loss: 0.002360  [70000/167876]\n",
            "loss: 0.001840  [72800/167876]\n",
            "loss: 0.002082  [75600/167876]\n",
            "loss: 0.002206  [78400/167876]\n",
            "loss: 0.001868  [81200/167876]\n",
            "loss: 0.001868  [84000/167876]\n",
            "loss: 0.001146  [86800/167876]\n",
            "loss: 0.001978  [89600/167876]\n",
            "loss: 0.001790  [92400/167876]\n",
            "loss: 0.003352  [95200/167876]\n",
            "loss: 0.002703  [98000/167876]\n",
            "loss: 0.002373  [100800/167876]\n",
            "loss: 0.003348  [103600/167876]\n",
            "loss: 0.001499  [106400/167876]\n",
            "loss: 0.001643  [109200/167876]\n",
            "loss: 0.002028  [112000/167876]\n",
            "loss: 0.000687  [114800/167876]\n",
            "loss: 0.001903  [117600/167876]\n",
            "loss: 0.001879  [120400/167876]\n",
            "loss: 0.002577  [123200/167876]\n",
            "loss: 0.002009  [126000/167876]\n",
            "loss: 0.002125  [128800/167876]\n",
            "loss: 0.001540  [131600/167876]\n",
            "loss: 0.002531  [134400/167876]\n",
            "loss: 0.002899  [137200/167876]\n",
            "loss: 0.003448  [140000/167876]\n",
            "loss: 0.003074  [142800/167876]\n",
            "loss: 0.002321  [145600/167876]\n",
            "loss: 0.002023  [148400/167876]\n",
            "loss: 0.002240  [151200/167876]\n",
            "loss: 0.001000  [154000/167876]\n",
            "loss: 0.001408  [156800/167876]\n",
            "loss: 0.002894  [159600/167876]\n",
            "loss: 0.002459  [162400/167876]\n",
            "loss: 0.001276  [165200/167876]\n",
            "tremolo rate: avg MSE: 0.003704, avg abs error: 0.0402\n",
            "learning rate: 0.000370 -> 0.000280\n",
            "---------------------------\n",
            "\n",
            "Epoch 9\n",
            "loss: 0.001643  [  0/167876]\n",
            "loss: 0.002102  [2800/167876]\n",
            "loss: 0.001255  [5600/167876]\n",
            "loss: 0.002007  [8400/167876]\n",
            "loss: 0.001642  [11200/167876]\n",
            "loss: 0.001489  [14000/167876]\n",
            "loss: 0.001929  [16800/167876]\n",
            "loss: 0.001758  [19600/167876]\n",
            "loss: 0.003092  [22400/167876]\n",
            "loss: 0.002376  [25200/167876]\n",
            "loss: 0.001786  [28000/167876]\n",
            "loss: 0.001519  [30800/167876]\n",
            "loss: 0.001141  [33600/167876]\n",
            "loss: 0.002255  [36400/167876]\n",
            "loss: 0.002712  [39200/167876]\n",
            "loss: 0.001610  [42000/167876]\n",
            "loss: 0.002687  [44800/167876]\n",
            "loss: 0.001164  [47600/167876]\n",
            "loss: 0.001833  [50400/167876]\n",
            "loss: 0.001952  [53200/167876]\n",
            "loss: 0.000951  [56000/167876]\n",
            "loss: 0.001325  [58800/167876]\n",
            "loss: 0.001646  [61600/167876]\n",
            "loss: 0.000911  [64400/167876]\n",
            "loss: 0.001151  [67200/167876]\n",
            "loss: 0.001296  [70000/167876]\n",
            "loss: 0.002545  [72800/167876]\n",
            "loss: 0.001211  [75600/167876]\n",
            "loss: 0.003163  [78400/167876]\n",
            "loss: 0.002044  [81200/167876]\n",
            "loss: 0.001534  [84000/167876]\n",
            "loss: 0.001279  [86800/167876]\n",
            "loss: 0.001400  [89600/167876]\n",
            "loss: 0.001622  [92400/167876]\n",
            "loss: 0.001194  [95200/167876]\n",
            "loss: 0.001194  [98000/167876]\n",
            "loss: 0.001652  [100800/167876]\n",
            "loss: 0.001998  [103600/167876]\n",
            "loss: 0.000973  [106400/167876]\n",
            "loss: 0.001582  [109200/167876]\n",
            "loss: 0.002510  [112000/167876]\n",
            "loss: 0.002567  [114800/167876]\n",
            "loss: 0.001786  [117600/167876]\n",
            "loss: 0.001864  [120400/167876]\n",
            "loss: 0.001537  [123200/167876]\n",
            "loss: 0.001419  [126000/167876]\n",
            "loss: 0.001213  [128800/167876]\n",
            "loss: 0.001272  [131600/167876]\n",
            "loss: 0.001388  [134400/167876]\n",
            "loss: 0.002076  [137200/167876]\n",
            "loss: 0.001044  [140000/167876]\n",
            "loss: 0.001337  [142800/167876]\n",
            "loss: 0.002614  [145600/167876]\n",
            "loss: 0.001305  [148400/167876]\n",
            "loss: 0.002475  [151200/167876]\n",
            "loss: 0.001256  [154000/167876]\n",
            "loss: 0.001824  [156800/167876]\n",
            "loss: 0.002063  [159600/167876]\n",
            "loss: 0.002792  [162400/167876]\n",
            "loss: 0.001563  [165200/167876]\n",
            "tremolo rate: avg MSE: 0.002258, avg abs error: 0.0292\n",
            "learning rate: 0.000280 -> 0.000190\n",
            "---------------------------\n",
            "\n",
            "Epoch 10\n",
            "loss: 0.001853  [  0/167876]\n",
            "loss: 0.001646  [2800/167876]\n",
            "loss: 0.001328  [5600/167876]\n",
            "loss: 0.000948  [8400/167876]\n",
            "loss: 0.001146  [11200/167876]\n",
            "loss: 0.001410  [14000/167876]\n",
            "loss: 0.000943  [16800/167876]\n",
            "loss: 0.002382  [19600/167876]\n",
            "loss: 0.001122  [22400/167876]\n",
            "loss: 0.001267  [25200/167876]\n",
            "loss: 0.001529  [28000/167876]\n",
            "loss: 0.000586  [30800/167876]\n",
            "loss: 0.001163  [33600/167876]\n",
            "loss: 0.001119  [36400/167876]\n",
            "loss: 0.001534  [39200/167876]\n",
            "loss: 0.001355  [42000/167876]\n",
            "loss: 0.000767  [44800/167876]\n",
            "loss: 0.001658  [47600/167876]\n",
            "loss: 0.000938  [50400/167876]\n",
            "loss: 0.001858  [53200/167876]\n",
            "loss: 0.001614  [56000/167876]\n",
            "loss: 0.001299  [58800/167876]\n",
            "loss: 0.001189  [61600/167876]\n",
            "loss: 0.001473  [64400/167876]\n",
            "loss: 0.002086  [67200/167876]\n",
            "loss: 0.001350  [70000/167876]\n",
            "loss: 0.001838  [72800/167876]\n",
            "loss: 0.001942  [75600/167876]\n",
            "loss: 0.001611  [78400/167876]\n",
            "loss: 0.001256  [81200/167876]\n",
            "loss: 0.000583  [84000/167876]\n",
            "loss: 0.001649  [86800/167876]\n",
            "loss: 0.001336  [89600/167876]\n",
            "loss: 0.001019  [92400/167876]\n",
            "loss: 0.001392  [95200/167876]\n",
            "loss: 0.000776  [98000/167876]\n",
            "loss: 0.001116  [100800/167876]\n",
            "loss: 0.002889  [103600/167876]\n",
            "loss: 0.002437  [106400/167876]\n",
            "loss: 0.001512  [109200/167876]\n",
            "loss: 0.001292  [112000/167876]\n",
            "loss: 0.001522  [114800/167876]\n",
            "loss: 0.001480  [117600/167876]\n",
            "loss: 0.000888  [120400/167876]\n",
            "loss: 0.000870  [123200/167876]\n",
            "loss: 0.001797  [126000/167876]\n",
            "loss: 0.001864  [128800/167876]\n",
            "loss: 0.001250  [131600/167876]\n",
            "loss: 0.001287  [134400/167876]\n",
            "loss: 0.001281  [137200/167876]\n",
            "loss: 0.001446  [140000/167876]\n",
            "loss: 0.001952  [142800/167876]\n",
            "loss: 0.001662  [145600/167876]\n",
            "loss: 0.001414  [148400/167876]\n",
            "loss: 0.000999  [151200/167876]\n",
            "loss: 0.000928  [154000/167876]\n",
            "loss: 0.001508  [156800/167876]\n",
            "loss: 0.002223  [159600/167876]\n",
            "loss: 0.001650  [162400/167876]\n",
            "loss: 0.001580  [165200/167876]\n",
            "tremolo rate: avg MSE: 0.002118, avg abs error: 0.0274\n",
            "learning rate: 0.000190 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 11\n",
            "loss: 0.001120  [  0/167876]\n",
            "loss: 0.002447  [2800/167876]\n",
            "loss: 0.001585  [5600/167876]\n",
            "loss: 0.001965  [8400/167876]\n",
            "loss: 0.001346  [11200/167876]\n",
            "loss: 0.000921  [14000/167876]\n",
            "loss: 0.001492  [16800/167876]\n",
            "loss: 0.000977  [19600/167876]\n",
            "loss: 0.001090  [22400/167876]\n",
            "loss: 0.000921  [25200/167876]\n",
            "loss: 0.001203  [28000/167876]\n",
            "loss: 0.000637  [30800/167876]\n",
            "loss: 0.001589  [33600/167876]\n",
            "loss: 0.001514  [36400/167876]\n",
            "loss: 0.001118  [39200/167876]\n",
            "loss: 0.001124  [42000/167876]\n",
            "loss: 0.001054  [44800/167876]\n",
            "loss: 0.001094  [47600/167876]\n",
            "loss: 0.001022  [50400/167876]\n",
            "loss: 0.001633  [53200/167876]\n",
            "loss: 0.001366  [56000/167876]\n",
            "loss: 0.001056  [58800/167876]\n",
            "loss: 0.001640  [61600/167876]\n",
            "loss: 0.001037  [64400/167876]\n",
            "loss: 0.001006  [67200/167876]\n",
            "loss: 0.001019  [70000/167876]\n",
            "loss: 0.000816  [72800/167876]\n",
            "loss: 0.000598  [75600/167876]\n",
            "loss: 0.001676  [78400/167876]\n",
            "loss: 0.001324  [81200/167876]\n",
            "loss: 0.001012  [84000/167876]\n",
            "loss: 0.000993  [86800/167876]\n",
            "loss: 0.002039  [89600/167876]\n",
            "loss: 0.001241  [92400/167876]\n",
            "loss: 0.001350  [95200/167876]\n",
            "loss: 0.001104  [98000/167876]\n",
            "loss: 0.001004  [100800/167876]\n",
            "loss: 0.001504  [103600/167876]\n",
            "loss: 0.000919  [106400/167876]\n",
            "loss: 0.002406  [109200/167876]\n",
            "loss: 0.000899  [112000/167876]\n",
            "loss: 0.000945  [114800/167876]\n",
            "loss: 0.001297  [117600/167876]\n",
            "loss: 0.001109  [120400/167876]\n",
            "loss: 0.000888  [123200/167876]\n",
            "loss: 0.001466  [126000/167876]\n",
            "loss: 0.001188  [128800/167876]\n",
            "loss: 0.001523  [131600/167876]\n",
            "loss: 0.002356  [134400/167876]\n",
            "loss: 0.001148  [137200/167876]\n",
            "loss: 0.001213  [140000/167876]\n",
            "loss: 0.001110  [142800/167876]\n",
            "loss: 0.000961  [145600/167876]\n",
            "loss: 0.001459  [148400/167876]\n",
            "loss: 0.000859  [151200/167876]\n",
            "loss: 0.000852  [154000/167876]\n",
            "loss: 0.001538  [156800/167876]\n",
            "loss: 0.001457  [159600/167876]\n",
            "loss: 0.000847  [162400/167876]\n",
            "loss: 0.000671  [165200/167876]\n",
            "tremolo rate: avg MSE: 0.002040, avg abs error: 0.0257\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 12\n",
            "loss: 0.001291  [  0/167876]\n",
            "loss: 0.000780  [2800/167876]\n",
            "loss: 0.000686  [5600/167876]\n",
            "loss: 0.001216  [8400/167876]\n",
            "loss: 0.001235  [11200/167876]\n",
            "loss: 0.001839  [14000/167876]\n",
            "loss: 0.001261  [16800/167876]\n",
            "loss: 0.001770  [19600/167876]\n",
            "loss: 0.002671  [22400/167876]\n",
            "loss: 0.001367  [25200/167876]\n",
            "loss: 0.001478  [28000/167876]\n",
            "loss: 0.001141  [30800/167876]\n",
            "loss: 0.000985  [33600/167876]\n",
            "loss: 0.001447  [36400/167876]\n",
            "loss: 0.000716  [39200/167876]\n",
            "loss: 0.000805  [42000/167876]\n",
            "loss: 0.002162  [44800/167876]\n",
            "loss: 0.001435  [47600/167876]\n",
            "loss: 0.001906  [50400/167876]\n",
            "loss: 0.000609  [53200/167876]\n",
            "loss: 0.000928  [56000/167876]\n",
            "loss: 0.001547  [58800/167876]\n",
            "loss: 0.001425  [61600/167876]\n",
            "loss: 0.001601  [64400/167876]\n",
            "loss: 0.002155  [67200/167876]\n",
            "loss: 0.000644  [70000/167876]\n",
            "loss: 0.001030  [72800/167876]\n",
            "loss: 0.000972  [75600/167876]\n",
            "loss: 0.002130  [78400/167876]\n",
            "loss: 0.000859  [81200/167876]\n",
            "loss: 0.001541  [84000/167876]\n",
            "loss: 0.001173  [86800/167876]\n",
            "loss: 0.000881  [89600/167876]\n",
            "loss: 0.001329  [92400/167876]\n",
            "loss: 0.001881  [95200/167876]\n",
            "loss: 0.001386  [98000/167876]\n",
            "loss: 0.001623  [100800/167876]\n",
            "loss: 0.000969  [103600/167876]\n",
            "loss: 0.001048  [106400/167876]\n",
            "loss: 0.003154  [109200/167876]\n",
            "loss: 0.000608  [112000/167876]\n",
            "loss: 0.001338  [114800/167876]\n",
            "loss: 0.001104  [117600/167876]\n",
            "loss: 0.001032  [120400/167876]\n",
            "loss: 0.000991  [123200/167876]\n",
            "loss: 0.000840  [126000/167876]\n",
            "loss: 0.002298  [128800/167876]\n",
            "loss: 0.000889  [131600/167876]\n",
            "loss: 0.000900  [134400/167876]\n",
            "loss: 0.001155  [137200/167876]\n",
            "loss: 0.001156  [140000/167876]\n",
            "loss: 0.001726  [142800/167876]\n",
            "loss: 0.001094  [145600/167876]\n",
            "loss: 0.001067  [148400/167876]\n",
            "loss: 0.001524  [151200/167876]\n",
            "loss: 0.001791  [154000/167876]\n",
            "loss: 0.001276  [156800/167876]\n",
            "loss: 0.000858  [159600/167876]\n",
            "loss: 0.001543  [162400/167876]\n",
            "loss: 0.001383  [165200/167876]\n",
            "tremolo rate: avg MSE: 0.002003, avg abs error: 0.026\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 13\n",
            "loss: 0.000664  [  0/167876]\n",
            "loss: 0.002363  [2800/167876]\n",
            "loss: 0.001075  [5600/167876]\n",
            "loss: 0.001249  [8400/167876]\n",
            "loss: 0.001541  [11200/167876]\n",
            "loss: 0.001516  [14000/167876]\n",
            "loss: 0.001285  [16800/167876]\n",
            "loss: 0.001176  [19600/167876]\n",
            "loss: 0.001781  [22400/167876]\n",
            "loss: 0.000822  [25200/167876]\n",
            "loss: 0.001134  [28000/167876]\n",
            "loss: 0.000975  [30800/167876]\n",
            "loss: 0.000844  [33600/167876]\n",
            "loss: 0.000770  [36400/167876]\n",
            "loss: 0.001784  [39200/167876]\n",
            "loss: 0.000673  [42000/167876]\n",
            "loss: 0.001297  [44800/167876]\n",
            "loss: 0.001132  [47600/167876]\n",
            "loss: 0.000933  [50400/167876]\n",
            "loss: 0.001397  [53200/167876]\n",
            "loss: 0.001346  [56000/167876]\n",
            "loss: 0.000745  [58800/167876]\n",
            "loss: 0.000791  [61600/167876]\n",
            "loss: 0.001266  [64400/167876]\n",
            "loss: 0.000981  [67200/167876]\n",
            "loss: 0.000909  [70000/167876]\n",
            "loss: 0.001760  [72800/167876]\n",
            "loss: 0.001336  [75600/167876]\n",
            "loss: 0.001933  [78400/167876]\n",
            "loss: 0.001727  [81200/167876]\n",
            "loss: 0.001247  [84000/167876]\n",
            "loss: 0.001234  [86800/167876]\n",
            "loss: 0.001035  [89600/167876]\n",
            "loss: 0.000913  [92400/167876]\n",
            "loss: 0.000947  [95200/167876]\n",
            "loss: 0.000590  [98000/167876]\n",
            "loss: 0.001453  [100800/167876]\n",
            "loss: 0.001238  [103600/167876]\n",
            "loss: 0.001211  [106400/167876]\n",
            "loss: 0.000911  [109200/167876]\n",
            "loss: 0.001181  [112000/167876]\n",
            "loss: 0.000976  [114800/167876]\n",
            "loss: 0.001459  [117600/167876]\n",
            "loss: 0.001108  [120400/167876]\n",
            "loss: 0.000695  [123200/167876]\n",
            "loss: 0.000981  [126000/167876]\n",
            "loss: 0.001040  [128800/167876]\n",
            "loss: 0.001092  [131600/167876]\n",
            "loss: 0.001295  [134400/167876]\n",
            "loss: 0.000972  [137200/167876]\n",
            "loss: 0.001114  [140000/167876]\n",
            "loss: 0.001387  [142800/167876]\n",
            "loss: 0.000887  [145600/167876]\n",
            "loss: 0.001504  [148400/167876]\n",
            "loss: 0.001968  [151200/167876]\n",
            "loss: 0.001548  [154000/167876]\n",
            "loss: 0.001103  [156800/167876]\n",
            "loss: 0.000752  [159600/167876]\n",
            "loss: 0.001231  [162400/167876]\n",
            "loss: 0.001080  [165200/167876]\n",
            "tremolo rate: avg MSE: 0.002150, avg abs error: 0.0267\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 14\n",
            "loss: 0.000621  [  0/167876]\n",
            "loss: 0.001037  [2800/167876]\n",
            "loss: 0.001269  [5600/167876]\n",
            "loss: 0.001409  [8400/167876]\n",
            "loss: 0.001392  [11200/167876]\n",
            "loss: 0.001019  [14000/167876]\n",
            "loss: 0.001055  [16800/167876]\n",
            "loss: 0.000881  [19600/167876]\n",
            "loss: 0.001531  [22400/167876]\n",
            "loss: 0.001111  [25200/167876]\n",
            "loss: 0.000591  [28000/167876]\n",
            "loss: 0.001524  [30800/167876]\n",
            "loss: 0.000870  [33600/167876]\n",
            "loss: 0.001172  [36400/167876]\n",
            "loss: 0.000992  [39200/167876]\n",
            "loss: 0.000866  [42000/167876]\n",
            "loss: 0.001159  [44800/167876]\n",
            "loss: 0.000882  [47600/167876]\n",
            "loss: 0.000955  [50400/167876]\n",
            "loss: 0.001566  [53200/167876]\n",
            "loss: 0.001556  [56000/167876]\n",
            "loss: 0.001266  [58800/167876]\n",
            "loss: 0.000958  [61600/167876]\n",
            "loss: 0.000448  [64400/167876]\n",
            "loss: 0.000892  [67200/167876]\n",
            "loss: 0.000788  [70000/167876]\n",
            "loss: 0.000776  [72800/167876]\n",
            "loss: 0.001801  [75600/167876]\n",
            "loss: 0.001133  [78400/167876]\n",
            "loss: 0.000510  [81200/167876]\n",
            "loss: 0.001174  [84000/167876]\n",
            "loss: 0.000592  [86800/167876]\n",
            "loss: 0.001077  [89600/167876]\n",
            "loss: 0.000913  [92400/167876]\n",
            "loss: 0.001212  [95200/167876]\n",
            "loss: 0.001038  [98000/167876]\n",
            "loss: 0.000903  [100800/167876]\n",
            "loss: 0.000733  [103600/167876]\n",
            "loss: 0.000954  [106400/167876]\n",
            "loss: 0.000588  [109200/167876]\n",
            "loss: 0.000807  [112000/167876]\n",
            "loss: 0.000999  [114800/167876]\n",
            "loss: 0.001032  [117600/167876]\n",
            "loss: 0.000754  [120400/167876]\n",
            "loss: 0.001233  [123200/167876]\n",
            "loss: 0.001394  [126000/167876]\n",
            "loss: 0.000982  [128800/167876]\n",
            "loss: 0.000524  [131600/167876]\n",
            "loss: 0.001244  [134400/167876]\n",
            "loss: 0.001055  [137200/167876]\n",
            "loss: 0.001019  [140000/167876]\n",
            "loss: 0.001130  [142800/167876]\n",
            "loss: 0.000678  [145600/167876]\n",
            "loss: 0.000988  [148400/167876]\n",
            "loss: 0.001402  [151200/167876]\n",
            "loss: 0.000792  [154000/167876]\n",
            "loss: 0.000798  [156800/167876]\n",
            "loss: 0.001096  [159600/167876]\n",
            "loss: 0.001492  [162400/167876]\n",
            "loss: 0.000742  [165200/167876]\n",
            "tremolo rate: avg MSE: 0.002010, avg abs error: 0.0259\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 15\n",
            "loss: 0.001281  [  0/167876]\n",
            "loss: 0.001063  [2800/167876]\n",
            "loss: 0.000857  [5600/167876]\n",
            "loss: 0.001007  [8400/167876]\n",
            "loss: 0.001232  [11200/167876]\n",
            "loss: 0.001113  [14000/167876]\n",
            "loss: 0.000629  [16800/167876]\n",
            "loss: 0.000823  [19600/167876]\n",
            "loss: 0.000535  [22400/167876]\n",
            "loss: 0.000967  [25200/167876]\n",
            "loss: 0.001181  [28000/167876]\n",
            "loss: 0.001271  [30800/167876]\n",
            "loss: 0.000906  [33600/167876]\n",
            "loss: 0.000892  [36400/167876]\n",
            "loss: 0.001108  [39200/167876]\n",
            "loss: 0.000718  [42000/167876]\n",
            "loss: 0.001729  [44800/167876]\n",
            "loss: 0.001315  [47600/167876]\n",
            "loss: 0.000988  [50400/167876]\n",
            "loss: 0.001405  [53200/167876]\n",
            "loss: 0.001136  [56000/167876]\n",
            "loss: 0.000958  [58800/167876]\n",
            "loss: 0.000686  [61600/167876]\n",
            "loss: 0.000635  [64400/167876]\n",
            "loss: 0.001067  [67200/167876]\n",
            "loss: 0.000775  [70000/167876]\n",
            "loss: 0.000752  [72800/167876]\n",
            "loss: 0.001098  [75600/167876]\n",
            "loss: 0.000863  [78400/167876]\n",
            "loss: 0.001221  [81200/167876]\n",
            "loss: 0.001081  [84000/167876]\n",
            "loss: 0.001405  [86800/167876]\n",
            "loss: 0.001102  [89600/167876]\n",
            "loss: 0.001037  [92400/167876]\n",
            "loss: 0.000798  [95200/167876]\n",
            "loss: 0.001468  [98000/167876]\n",
            "loss: 0.001325  [100800/167876]\n",
            "loss: 0.000871  [103600/167876]\n",
            "loss: 0.000838  [106400/167876]\n",
            "loss: 0.000842  [109200/167876]\n",
            "loss: 0.000786  [112000/167876]\n",
            "loss: 0.001134  [114800/167876]\n",
            "loss: 0.001573  [117600/167876]\n",
            "loss: 0.000871  [120400/167876]\n",
            "loss: 0.000930  [123200/167876]\n",
            "loss: 0.001287  [126000/167876]\n",
            "loss: 0.000701  [128800/167876]\n",
            "loss: 0.000991  [131600/167876]\n",
            "loss: 0.000801  [134400/167876]\n",
            "loss: 0.000898  [137200/167876]\n",
            "loss: 0.000794  [140000/167876]\n",
            "loss: 0.000804  [142800/167876]\n",
            "loss: 0.001746  [145600/167876]\n",
            "loss: 0.000980  [148400/167876]\n",
            "loss: 0.001401  [151200/167876]\n",
            "loss: 0.001144  [154000/167876]\n",
            "loss: 0.000880  [156800/167876]\n",
            "loss: 0.000676  [159600/167876]\n",
            "loss: 0.000792  [162400/167876]\n",
            "loss: 0.001420  [165200/167876]\n",
            "tremolo rate: avg MSE: 0.002000, avg abs error: 0.0258\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 16\n",
            "loss: 0.001508  [  0/167876]\n",
            "loss: 0.000814  [2800/167876]\n",
            "loss: 0.000968  [5600/167876]\n",
            "loss: 0.000434  [8400/167876]\n",
            "loss: 0.001519  [11200/167876]\n",
            "loss: 0.001102  [14000/167876]\n",
            "loss: 0.000940  [16800/167876]\n",
            "loss: 0.000975  [19600/167876]\n",
            "loss: 0.001126  [22400/167876]\n",
            "loss: 0.001286  [25200/167876]\n",
            "loss: 0.000597  [28000/167876]\n",
            "loss: 0.000837  [30800/167876]\n",
            "loss: 0.000576  [33600/167876]\n",
            "loss: 0.001140  [36400/167876]\n",
            "loss: 0.000953  [39200/167876]\n",
            "loss: 0.000905  [42000/167876]\n",
            "loss: 0.000942  [44800/167876]\n",
            "loss: 0.000755  [47600/167876]\n",
            "loss: 0.001052  [50400/167876]\n",
            "loss: 0.001153  [53200/167876]\n",
            "loss: 0.001034  [56000/167876]\n",
            "loss: 0.001145  [58800/167876]\n",
            "loss: 0.001052  [61600/167876]\n",
            "loss: 0.000759  [64400/167876]\n",
            "loss: 0.000990  [67200/167876]\n",
            "loss: 0.001277  [70000/167876]\n",
            "loss: 0.001148  [72800/167876]\n",
            "loss: 0.000587  [75600/167876]\n",
            "loss: 0.001052  [78400/167876]\n",
            "loss: 0.000920  [81200/167876]\n",
            "loss: 0.001190  [84000/167876]\n",
            "loss: 0.000656  [86800/167876]\n",
            "loss: 0.001344  [89600/167876]\n",
            "loss: 0.001067  [92400/167876]\n",
            "loss: 0.001427  [95200/167876]\n",
            "loss: 0.000879  [98000/167876]\n",
            "loss: 0.001128  [100800/167876]\n",
            "loss: 0.000812  [103600/167876]\n",
            "loss: 0.000766  [106400/167876]\n",
            "loss: 0.001033  [109200/167876]\n",
            "loss: 0.001211  [112000/167876]\n",
            "loss: 0.001229  [114800/167876]\n",
            "loss: 0.000821  [117600/167876]\n",
            "loss: 0.000698  [120400/167876]\n",
            "loss: 0.000852  [123200/167876]\n",
            "loss: 0.000957  [126000/167876]\n",
            "loss: 0.001084  [128800/167876]\n",
            "loss: 0.001082  [131600/167876]\n",
            "loss: 0.000533  [134400/167876]\n",
            "loss: 0.000953  [137200/167876]\n",
            "loss: 0.001384  [140000/167876]\n",
            "loss: 0.001149  [142800/167876]\n",
            "loss: 0.001646  [145600/167876]\n",
            "loss: 0.000972  [148400/167876]\n",
            "loss: 0.001315  [151200/167876]\n",
            "loss: 0.000935  [154000/167876]\n",
            "loss: 0.001613  [156800/167876]\n",
            "loss: 0.000832  [159600/167876]\n",
            "loss: 0.000694  [162400/167876]\n",
            "loss: 0.001530  [165200/167876]\n",
            "tremolo rate: avg MSE: 0.002094, avg abs error: 0.0265\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 17\n",
            "loss: 0.001480  [  0/167876]\n",
            "loss: 0.000815  [2800/167876]\n",
            "loss: 0.001019  [5600/167876]\n",
            "loss: 0.001171  [8400/167876]\n",
            "loss: 0.001041  [11200/167876]\n",
            "loss: 0.000742  [14000/167876]\n",
            "loss: 0.000792  [16800/167876]\n",
            "loss: 0.000880  [19600/167876]\n",
            "loss: 0.000773  [22400/167876]\n",
            "loss: 0.000825  [25200/167876]\n",
            "loss: 0.000778  [28000/167876]\n",
            "loss: 0.000924  [30800/167876]\n",
            "loss: 0.000989  [33600/167876]\n",
            "loss: 0.001095  [36400/167876]\n",
            "loss: 0.001082  [39200/167876]\n",
            "loss: 0.000762  [42000/167876]\n",
            "loss: 0.000902  [44800/167876]\n",
            "loss: 0.000872  [47600/167876]\n",
            "loss: 0.000689  [50400/167876]\n",
            "loss: 0.001215  [53200/167876]\n",
            "loss: 0.001045  [56000/167876]\n",
            "loss: 0.000658  [58800/167876]\n",
            "loss: 0.000867  [61600/167876]\n",
            "loss: 0.001046  [64400/167876]\n",
            "loss: 0.000773  [67200/167876]\n",
            "loss: 0.000765  [70000/167876]\n",
            "loss: 0.000825  [72800/167876]\n",
            "loss: 0.000768  [75600/167876]\n",
            "loss: 0.000954  [78400/167876]\n",
            "loss: 0.000815  [81200/167876]\n",
            "loss: 0.000754  [84000/167876]\n",
            "loss: 0.000608  [86800/167876]\n",
            "loss: 0.000727  [89600/167876]\n",
            "loss: 0.000882  [92400/167876]\n",
            "loss: 0.000967  [95200/167876]\n",
            "loss: 0.001009  [98000/167876]\n",
            "loss: 0.000961  [100800/167876]\n",
            "loss: 0.000906  [103600/167876]\n",
            "loss: 0.001555  [106400/167876]\n",
            "loss: 0.000658  [109200/167876]\n",
            "loss: 0.001152  [112000/167876]\n",
            "loss: 0.000847  [114800/167876]\n",
            "loss: 0.000901  [117600/167876]\n",
            "loss: 0.001007  [120400/167876]\n",
            "loss: 0.001021  [123200/167876]\n",
            "loss: 0.001147  [126000/167876]\n",
            "loss: 0.000998  [128800/167876]\n",
            "loss: 0.001194  [131600/167876]\n",
            "loss: 0.001053  [134400/167876]\n",
            "loss: 0.000962  [137200/167876]\n",
            "loss: 0.001068  [140000/167876]\n",
            "loss: 0.001544  [142800/167876]\n",
            "loss: 0.000720  [145600/167876]\n",
            "loss: 0.000802  [148400/167876]\n",
            "loss: 0.001001  [151200/167876]\n",
            "loss: 0.001537  [154000/167876]\n",
            "loss: 0.001232  [156800/167876]\n",
            "loss: 0.000740  [159600/167876]\n",
            "loss: 0.000716  [162400/167876]\n",
            "loss: 0.001320  [165200/167876]\n",
            "tremolo rate: avg MSE: 0.001996, avg abs error: 0.0257\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 18\n",
            "loss: 0.001358  [  0/167876]\n",
            "loss: 0.001063  [2800/167876]\n",
            "loss: 0.000914  [5600/167876]\n",
            "loss: 0.000805  [8400/167876]\n",
            "loss: 0.001047  [11200/167876]\n",
            "loss: 0.000811  [14000/167876]\n",
            "loss: 0.001061  [16800/167876]\n",
            "loss: 0.000537  [19600/167876]\n",
            "loss: 0.000701  [22400/167876]\n",
            "loss: 0.000859  [25200/167876]\n",
            "loss: 0.001113  [28000/167876]\n",
            "loss: 0.000807  [30800/167876]\n",
            "loss: 0.000735  [33600/167876]\n",
            "loss: 0.001281  [36400/167876]\n",
            "loss: 0.001117  [39200/167876]\n",
            "loss: 0.000669  [42000/167876]\n",
            "loss: 0.000674  [44800/167876]\n",
            "loss: 0.000732  [47600/167876]\n",
            "loss: 0.000553  [50400/167876]\n",
            "loss: 0.000824  [53200/167876]\n",
            "loss: 0.000690  [56000/167876]\n",
            "loss: 0.000898  [58800/167876]\n",
            "loss: 0.000660  [61600/167876]\n",
            "loss: 0.000841  [64400/167876]\n",
            "loss: 0.001155  [67200/167876]\n",
            "loss: 0.001078  [70000/167876]\n",
            "loss: 0.000926  [72800/167876]\n",
            "loss: 0.001040  [75600/167876]\n",
            "loss: 0.000789  [78400/167876]\n",
            "loss: 0.000846  [81200/167876]\n",
            "loss: 0.000764  [84000/167876]\n",
            "loss: 0.000866  [86800/167876]\n",
            "loss: 0.000913  [89600/167876]\n",
            "loss: 0.001237  [92400/167876]\n",
            "loss: 0.000988  [95200/167876]\n",
            "loss: 0.000669  [98000/167876]\n",
            "loss: 0.000585  [100800/167876]\n",
            "loss: 0.000736  [103600/167876]\n",
            "loss: 0.001055  [106400/167876]\n",
            "loss: 0.000921  [109200/167876]\n",
            "loss: 0.000849  [112000/167876]\n",
            "loss: 0.001254  [114800/167876]\n",
            "loss: 0.000801  [117600/167876]\n",
            "loss: 0.001070  [120400/167876]\n",
            "loss: 0.002124  [123200/167876]\n",
            "loss: 0.000923  [126000/167876]\n",
            "loss: 0.000554  [128800/167876]\n",
            "loss: 0.001240  [131600/167876]\n",
            "loss: 0.000952  [134400/167876]\n",
            "loss: 0.000988  [137200/167876]\n",
            "loss: 0.000646  [140000/167876]\n",
            "loss: 0.001123  [142800/167876]\n",
            "loss: 0.000798  [145600/167876]\n",
            "loss: 0.001633  [148400/167876]\n",
            "loss: 0.000546  [151200/167876]\n",
            "loss: 0.000752  [154000/167876]\n",
            "loss: 0.001001  [156800/167876]\n",
            "loss: 0.000830  [159600/167876]\n",
            "loss: 0.001048  [162400/167876]\n",
            "loss: 0.000979  [165200/167876]\n",
            "tremolo rate: avg MSE: 0.002015, avg abs error: 0.0259\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 19\n",
            "loss: 0.000775  [  0/167876]\n",
            "loss: 0.001099  [2800/167876]\n",
            "loss: 0.001015  [5600/167876]\n",
            "loss: 0.000572  [8400/167876]\n",
            "loss: 0.001220  [11200/167876]\n",
            "loss: 0.000634  [14000/167876]\n",
            "loss: 0.000598  [16800/167876]\n",
            "loss: 0.000719  [19600/167876]\n",
            "loss: 0.000688  [22400/167876]\n",
            "loss: 0.000844  [25200/167876]\n",
            "loss: 0.000786  [28000/167876]\n",
            "loss: 0.000740  [30800/167876]\n",
            "loss: 0.000860  [33600/167876]\n",
            "loss: 0.001082  [36400/167876]\n",
            "loss: 0.000956  [39200/167876]\n",
            "loss: 0.000991  [42000/167876]\n",
            "loss: 0.001189  [44800/167876]\n",
            "loss: 0.000678  [47600/167876]\n",
            "loss: 0.000666  [50400/167876]\n",
            "loss: 0.001091  [53200/167876]\n",
            "loss: 0.000679  [56000/167876]\n",
            "loss: 0.000882  [58800/167876]\n",
            "loss: 0.000783  [61600/167876]\n",
            "loss: 0.001184  [64400/167876]\n",
            "loss: 0.000807  [67200/167876]\n",
            "loss: 0.000725  [70000/167876]\n",
            "loss: 0.000805  [72800/167876]\n",
            "loss: 0.001362  [75600/167876]\n",
            "loss: 0.000837  [78400/167876]\n",
            "loss: 0.000763  [81200/167876]\n",
            "loss: 0.001270  [84000/167876]\n",
            "loss: 0.000855  [86800/167876]\n",
            "loss: 0.000746  [89600/167876]\n",
            "loss: 0.001017  [92400/167876]\n",
            "loss: 0.001008  [95200/167876]\n",
            "loss: 0.000783  [98000/167876]\n",
            "loss: 0.001493  [100800/167876]\n",
            "loss: 0.000836  [103600/167876]\n",
            "loss: 0.001415  [106400/167876]\n",
            "loss: 0.000882  [109200/167876]\n",
            "loss: 0.000773  [112000/167876]\n",
            "loss: 0.000962  [114800/167876]\n",
            "loss: 0.000849  [117600/167876]\n",
            "loss: 0.000756  [120400/167876]\n",
            "loss: 0.000709  [123200/167876]\n",
            "loss: 0.000975  [126000/167876]\n",
            "loss: 0.001154  [128800/167876]\n",
            "loss: 0.000902  [131600/167876]\n",
            "loss: 0.000779  [134400/167876]\n",
            "loss: 0.000879  [137200/167876]\n",
            "loss: 0.001163  [140000/167876]\n",
            "loss: 0.000751  [142800/167876]\n",
            "loss: 0.000588  [145600/167876]\n",
            "loss: 0.000599  [148400/167876]\n",
            "loss: 0.001158  [151200/167876]\n",
            "loss: 0.000869  [154000/167876]\n",
            "loss: 0.000967  [156800/167876]\n",
            "loss: 0.000861  [159600/167876]\n",
            "loss: 0.000737  [162400/167876]\n",
            "loss: 0.000796  [165200/167876]\n",
            "tremolo rate: avg MSE: 0.002031, avg abs error: 0.026\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 20\n",
            "loss: 0.000767  [  0/167876]\n",
            "loss: 0.000763  [2800/167876]\n",
            "loss: 0.001215  [5600/167876]\n",
            "loss: 0.000898  [8400/167876]\n",
            "loss: 0.001025  [11200/167876]\n",
            "loss: 0.000651  [14000/167876]\n",
            "loss: 0.000753  [16800/167876]\n",
            "loss: 0.000994  [19600/167876]\n",
            "loss: 0.000730  [22400/167876]\n",
            "loss: 0.000561  [25200/167876]\n",
            "loss: 0.001177  [28000/167876]\n",
            "loss: 0.000767  [30800/167876]\n",
            "loss: 0.000625  [33600/167876]\n",
            "loss: 0.001103  [36400/167876]\n",
            "loss: 0.000837  [39200/167876]\n",
            "loss: 0.000939  [42000/167876]\n",
            "loss: 0.001320  [44800/167876]\n",
            "loss: 0.000740  [47600/167876]\n",
            "loss: 0.000506  [50400/167876]\n",
            "loss: 0.000605  [53200/167876]\n",
            "loss: 0.000480  [56000/167876]\n",
            "loss: 0.001047  [58800/167876]\n",
            "loss: 0.000882  [61600/167876]\n",
            "loss: 0.000580  [64400/167876]\n",
            "loss: 0.000746  [67200/167876]\n",
            "loss: 0.000984  [70000/167876]\n",
            "loss: 0.000844  [72800/167876]\n",
            "loss: 0.000717  [75600/167876]\n",
            "loss: 0.000671  [78400/167876]\n",
            "loss: 0.000869  [81200/167876]\n",
            "loss: 0.000788  [84000/167876]\n",
            "loss: 0.001327  [86800/167876]\n",
            "loss: 0.000842  [89600/167876]\n",
            "loss: 0.000903  [92400/167876]\n",
            "loss: 0.000831  [95200/167876]\n",
            "loss: 0.001223  [98000/167876]\n",
            "loss: 0.000988  [100800/167876]\n",
            "loss: 0.001225  [103600/167876]\n",
            "loss: 0.000803  [106400/167876]\n",
            "loss: 0.000953  [109200/167876]\n",
            "loss: 0.000752  [112000/167876]\n",
            "loss: 0.000846  [114800/167876]\n",
            "loss: 0.000880  [117600/167876]\n",
            "loss: 0.000917  [120400/167876]\n",
            "loss: 0.001259  [123200/167876]\n",
            "loss: 0.000952  [126000/167876]\n",
            "loss: 0.001507  [128800/167876]\n",
            "loss: 0.000984  [131600/167876]\n",
            "loss: 0.001414  [134400/167876]\n",
            "loss: 0.000655  [137200/167876]\n",
            "loss: 0.001304  [140000/167876]\n",
            "loss: 0.000621  [142800/167876]\n",
            "loss: 0.000711  [145600/167876]\n",
            "loss: 0.000874  [148400/167876]\n",
            "loss: 0.001375  [151200/167876]\n",
            "loss: 0.001061  [154000/167876]\n",
            "loss: 0.000878  [156800/167876]\n",
            "loss: 0.000899  [159600/167876]\n",
            "loss: 0.000929  [162400/167876]\n",
            "loss: 0.000919  [165200/167876]\n",
            "tremolo rate: avg MSE: 0.001986, avg abs error: 0.026\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Finished training\n",
            "tremolo rate: avg MSE: 0.001972, avg abs error: 0.0258\n",
            "Epoch 1\n",
            "loss: 0.059440  [  0/167876]\n",
            "loss: 0.035878  [2800/167876]\n",
            "loss: 0.009871  [5600/167876]\n",
            "loss: 0.006710  [8400/167876]\n",
            "loss: 0.004187  [11200/167876]\n",
            "loss: 0.003075  [14000/167876]\n",
            "loss: 0.003034  [16800/167876]\n",
            "loss: 0.001955  [19600/167876]\n",
            "loss: 0.002849  [22400/167876]\n",
            "loss: 0.001607  [25200/167876]\n",
            "loss: 0.002030  [28000/167876]\n",
            "loss: 0.003512  [30800/167876]\n",
            "loss: 0.001611  [33600/167876]\n",
            "loss: 0.001192  [36400/167876]\n",
            "loss: 0.001037  [39200/167876]\n",
            "loss: 0.001826  [42000/167876]\n",
            "loss: 0.001130  [44800/167876]\n",
            "loss: 0.000989  [47600/167876]\n",
            "loss: 0.002788  [50400/167876]\n",
            "loss: 0.000800  [53200/167876]\n",
            "loss: 0.000905  [56000/167876]\n",
            "loss: 0.003515  [58800/167876]\n",
            "loss: 0.001199  [61600/167876]\n",
            "loss: 0.000760  [64400/167876]\n",
            "loss: 0.001138  [67200/167876]\n",
            "loss: 0.001241  [70000/167876]\n",
            "loss: 0.001077  [72800/167876]\n",
            "loss: 0.000829  [75600/167876]\n",
            "loss: 0.001320  [78400/167876]\n",
            "loss: 0.000918  [81200/167876]\n",
            "loss: 0.001132  [84000/167876]\n",
            "loss: 0.001055  [86800/167876]\n",
            "loss: 0.000858  [89600/167876]\n",
            "loss: 0.000676  [92400/167876]\n",
            "loss: 0.000774  [95200/167876]\n",
            "loss: 0.000948  [98000/167876]\n",
            "loss: 0.001273  [100800/167876]\n",
            "loss: 0.001154  [103600/167876]\n",
            "loss: 0.000872  [106400/167876]\n",
            "loss: 0.001053  [109200/167876]\n",
            "loss: 0.001174  [112000/167876]\n",
            "loss: 0.001044  [114800/167876]\n",
            "loss: 0.000709  [117600/167876]\n",
            "loss: 0.000836  [120400/167876]\n",
            "loss: 0.001049  [123200/167876]\n",
            "loss: 0.001015  [126000/167876]\n",
            "loss: 0.000681  [128800/167876]\n",
            "loss: 0.000629  [131600/167876]\n",
            "loss: 0.001180  [134400/167876]\n",
            "loss: 0.001149  [137200/167876]\n",
            "loss: 0.000473  [140000/167876]\n",
            "loss: 0.000986  [142800/167876]\n",
            "loss: 0.000683  [145600/167876]\n",
            "loss: 0.000777  [148400/167876]\n",
            "loss: 0.001155  [151200/167876]\n",
            "loss: 0.000838  [154000/167876]\n",
            "loss: 0.001141  [156800/167876]\n",
            "loss: 0.000788  [159600/167876]\n",
            "loss: 0.000699  [162400/167876]\n",
            "loss: 0.000964  [165200/167876]\n",
            "delay time: avg MSE: 0.004571, avg abs error: 0.0632\n",
            "learning rate: 0.001000 -> 0.000910\n",
            "---------------------------\n",
            "\n",
            "Epoch 2\n",
            "loss: 0.005504  [  0/167876]\n",
            "loss: 0.001597  [2800/167876]\n",
            "loss: 0.000476  [5600/167876]\n",
            "loss: 0.000566  [8400/167876]\n",
            "loss: 0.000361  [11200/167876]\n",
            "loss: 0.000844  [14000/167876]\n",
            "loss: 0.001776  [16800/167876]\n",
            "loss: 0.000460  [19600/167876]\n",
            "loss: 0.000628  [22400/167876]\n",
            "loss: 0.000592  [25200/167876]\n",
            "loss: 0.000594  [28000/167876]\n",
            "loss: 0.000391  [30800/167876]\n",
            "loss: 0.000513  [33600/167876]\n",
            "loss: 0.003276  [36400/167876]\n",
            "loss: 0.000855  [39200/167876]\n",
            "loss: 0.000956  [42000/167876]\n",
            "loss: 0.000442  [44800/167876]\n",
            "loss: 0.000533  [47600/167876]\n",
            "loss: 0.000548  [50400/167876]\n",
            "loss: 0.000432  [53200/167876]\n",
            "loss: 0.000527  [56000/167876]\n",
            "loss: 0.000565  [58800/167876]\n",
            "loss: 0.000453  [61600/167876]\n",
            "loss: 0.000604  [64400/167876]\n",
            "loss: 0.000396  [67200/167876]\n",
            "loss: 0.000540  [70000/167876]\n",
            "loss: 0.000381  [72800/167876]\n",
            "loss: 0.001183  [75600/167876]\n",
            "loss: 0.000959  [78400/167876]\n",
            "loss: 0.000397  [81200/167876]\n",
            "loss: 0.000920  [84000/167876]\n",
            "loss: 0.000909  [86800/167876]\n",
            "loss: 0.000622  [89600/167876]\n",
            "loss: 0.000461  [92400/167876]\n",
            "loss: 0.000326  [95200/167876]\n",
            "loss: 0.000572  [98000/167876]\n",
            "loss: 0.000387  [100800/167876]\n",
            "loss: 0.000333  [103600/167876]\n",
            "loss: 0.000920  [106400/167876]\n",
            "loss: 0.000437  [109200/167876]\n",
            "loss: 0.000500  [112000/167876]\n",
            "loss: 0.000420  [114800/167876]\n",
            "loss: 0.000549  [117600/167876]\n",
            "loss: 0.000549  [120400/167876]\n",
            "loss: 0.000365  [123200/167876]\n",
            "loss: 0.000799  [126000/167876]\n",
            "loss: 0.000578  [128800/167876]\n",
            "loss: 0.000518  [131600/167876]\n",
            "loss: 0.000298  [134400/167876]\n",
            "loss: 0.000770  [137200/167876]\n",
            "loss: 0.000491  [140000/167876]\n",
            "loss: 0.000625  [142800/167876]\n",
            "loss: 0.000286  [145600/167876]\n",
            "loss: 0.000379  [148400/167876]\n",
            "loss: 0.000766  [151200/167876]\n",
            "loss: 0.000486  [154000/167876]\n",
            "loss: 0.000348  [156800/167876]\n",
            "loss: 0.000618  [159600/167876]\n",
            "loss: 0.000296  [162400/167876]\n",
            "loss: 0.001129  [165200/167876]\n",
            "delay time: avg MSE: 0.000419, avg abs error: 0.0147\n",
            "learning rate: 0.000910 -> 0.000820\n",
            "---------------------------\n",
            "\n",
            "Epoch 3\n",
            "loss: 0.000269  [  0/167876]\n",
            "loss: 0.000292  [2800/167876]\n",
            "loss: 0.000688  [5600/167876]\n",
            "loss: 0.000596  [8400/167876]\n",
            "loss: 0.000270  [11200/167876]\n",
            "loss: 0.000416  [14000/167876]\n",
            "loss: 0.000276  [16800/167876]\n",
            "loss: 0.000343  [19600/167876]\n",
            "loss: 0.000400  [22400/167876]\n",
            "loss: 0.000684  [25200/167876]\n",
            "loss: 0.000361  [28000/167876]\n",
            "loss: 0.000455  [30800/167876]\n",
            "loss: 0.000308  [33600/167876]\n",
            "loss: 0.000210  [36400/167876]\n",
            "loss: 0.000475  [39200/167876]\n",
            "loss: 0.000384  [42000/167876]\n",
            "loss: 0.000268  [44800/167876]\n",
            "loss: 0.000266  [47600/167876]\n",
            "loss: 0.000526  [50400/167876]\n",
            "loss: 0.000929  [53200/167876]\n",
            "loss: 0.000385  [56000/167876]\n",
            "loss: 0.000373  [58800/167876]\n",
            "loss: 0.000328  [61600/167876]\n",
            "loss: 0.000264  [64400/167876]\n",
            "loss: 0.000300  [67200/167876]\n",
            "loss: 0.000599  [70000/167876]\n",
            "loss: 0.000358  [72800/167876]\n",
            "loss: 0.000378  [75600/167876]\n",
            "loss: 0.000300  [78400/167876]\n",
            "loss: 0.000260  [81200/167876]\n",
            "loss: 0.000517  [84000/167876]\n",
            "loss: 0.000659  [86800/167876]\n",
            "loss: 0.000340  [89600/167876]\n",
            "loss: 0.000504  [92400/167876]\n",
            "loss: 0.000493  [95200/167876]\n",
            "loss: 0.000442  [98000/167876]\n",
            "loss: 0.000460  [100800/167876]\n",
            "loss: 0.000418  [103600/167876]\n",
            "loss: 0.000615  [106400/167876]\n",
            "loss: 0.000349  [109200/167876]\n",
            "loss: 0.000457  [112000/167876]\n",
            "loss: 0.000339  [114800/167876]\n",
            "loss: 0.000404  [117600/167876]\n",
            "loss: 0.000338  [120400/167876]\n",
            "loss: 0.000243  [123200/167876]\n",
            "loss: 0.000361  [126000/167876]\n",
            "loss: 0.000262  [128800/167876]\n",
            "loss: 0.000388  [131600/167876]\n",
            "loss: 0.000310  [134400/167876]\n",
            "loss: 0.000334  [137200/167876]\n",
            "loss: 0.000877  [140000/167876]\n",
            "loss: 0.000212  [142800/167876]\n",
            "loss: 0.000322  [145600/167876]\n",
            "loss: 0.000424  [148400/167876]\n",
            "loss: 0.000318  [151200/167876]\n",
            "loss: 0.000549  [154000/167876]\n",
            "loss: 0.000275  [156800/167876]\n",
            "loss: 0.000276  [159600/167876]\n",
            "loss: 0.000455  [162400/167876]\n",
            "loss: 0.000288  [165200/167876]\n",
            "delay time: avg MSE: 0.000500, avg abs error: 0.0177\n",
            "learning rate: 0.000820 -> 0.000730\n",
            "---------------------------\n",
            "\n",
            "Epoch 4\n",
            "loss: 0.000592  [  0/167876]\n",
            "loss: 0.000675  [2800/167876]\n",
            "loss: 0.000689  [5600/167876]\n",
            "loss: 0.000310  [8400/167876]\n",
            "loss: 0.000529  [11200/167876]\n",
            "loss: 0.000475  [14000/167876]\n",
            "loss: 0.000273  [16800/167876]\n",
            "loss: 0.000354  [19600/167876]\n",
            "loss: 0.000531  [22400/167876]\n",
            "loss: 0.000419  [25200/167876]\n",
            "loss: 0.000309  [28000/167876]\n",
            "loss: 0.000228  [30800/167876]\n",
            "loss: 0.000450  [33600/167876]\n",
            "loss: 0.000267  [36400/167876]\n",
            "loss: 0.000336  [39200/167876]\n",
            "loss: 0.000550  [42000/167876]\n",
            "loss: 0.000445  [44800/167876]\n",
            "loss: 0.000362  [47600/167876]\n",
            "loss: 0.000301  [50400/167876]\n",
            "loss: 0.000226  [53200/167876]\n",
            "loss: 0.000251  [56000/167876]\n",
            "loss: 0.000268  [58800/167876]\n",
            "loss: 0.000399  [61600/167876]\n",
            "loss: 0.000339  [64400/167876]\n",
            "loss: 0.000287  [67200/167876]\n",
            "loss: 0.000484  [70000/167876]\n",
            "loss: 0.000200  [72800/167876]\n",
            "loss: 0.000325  [75600/167876]\n",
            "loss: 0.000359  [78400/167876]\n",
            "loss: 0.000355  [81200/167876]\n",
            "loss: 0.000354  [84000/167876]\n",
            "loss: 0.000577  [86800/167876]\n",
            "loss: 0.000490  [89600/167876]\n",
            "loss: 0.000370  [92400/167876]\n",
            "loss: 0.000333  [95200/167876]\n",
            "loss: 0.000546  [98000/167876]\n",
            "loss: 0.000301  [100800/167876]\n",
            "loss: 0.000215  [103600/167876]\n",
            "loss: 0.000273  [106400/167876]\n",
            "loss: 0.000706  [109200/167876]\n",
            "loss: 0.000384  [112000/167876]\n",
            "loss: 0.000238  [114800/167876]\n",
            "loss: 0.000576  [117600/167876]\n",
            "loss: 0.000374  [120400/167876]\n",
            "loss: 0.000337  [123200/167876]\n",
            "loss: 0.000264  [126000/167876]\n",
            "loss: 0.000260  [128800/167876]\n",
            "loss: 0.000322  [131600/167876]\n",
            "loss: 0.000339  [134400/167876]\n",
            "loss: 0.000296  [137200/167876]\n",
            "loss: 0.000388  [140000/167876]\n",
            "loss: 0.000185  [142800/167876]\n",
            "loss: 0.000213  [145600/167876]\n",
            "loss: 0.000204  [148400/167876]\n",
            "loss: 0.000275  [151200/167876]\n",
            "loss: 0.000304  [154000/167876]\n",
            "loss: 0.000303  [156800/167876]\n",
            "loss: 0.000203  [159600/167876]\n",
            "loss: 0.000254  [162400/167876]\n",
            "loss: 0.000418  [165200/167876]\n",
            "delay time: avg MSE: 0.000359, avg abs error: 0.0143\n",
            "learning rate: 0.000730 -> 0.000640\n",
            "---------------------------\n",
            "\n",
            "Epoch 5\n",
            "loss: 0.000376  [  0/167876]\n",
            "loss: 0.000367  [2800/167876]\n",
            "loss: 0.000516  [5600/167876]\n",
            "loss: 0.000304  [8400/167876]\n",
            "loss: 0.000172  [11200/167876]\n",
            "loss: 0.000286  [14000/167876]\n",
            "loss: 0.000280  [16800/167876]\n",
            "loss: 0.000168  [19600/167876]\n",
            "loss: 0.000244  [22400/167876]\n",
            "loss: 0.000280  [25200/167876]\n",
            "loss: 0.000394  [28000/167876]\n",
            "loss: 0.000425  [30800/167876]\n",
            "loss: 0.000205  [33600/167876]\n",
            "loss: 0.000237  [36400/167876]\n",
            "loss: 0.000212  [39200/167876]\n",
            "loss: 0.000378  [42000/167876]\n",
            "loss: 0.000190  [44800/167876]\n",
            "loss: 0.000258  [47600/167876]\n",
            "loss: 0.000572  [50400/167876]\n",
            "loss: 0.000244  [53200/167876]\n",
            "loss: 0.000269  [56000/167876]\n",
            "loss: 0.000347  [58800/167876]\n",
            "loss: 0.000274  [61600/167876]\n",
            "loss: 0.000418  [64400/167876]\n",
            "loss: 0.000226  [67200/167876]\n",
            "loss: 0.000336  [70000/167876]\n",
            "loss: 0.000183  [72800/167876]\n",
            "loss: 0.000423  [75600/167876]\n",
            "loss: 0.000193  [78400/167876]\n",
            "loss: 0.000225  [81200/167876]\n",
            "loss: 0.000300  [84000/167876]\n",
            "loss: 0.000314  [86800/167876]\n",
            "loss: 0.000246  [89600/167876]\n",
            "loss: 0.000175  [92400/167876]\n",
            "loss: 0.000319  [95200/167876]\n",
            "loss: 0.000187  [98000/167876]\n",
            "loss: 0.000172  [100800/167876]\n",
            "loss: 0.000292  [103600/167876]\n",
            "loss: 0.000225  [106400/167876]\n",
            "loss: 0.000331  [109200/167876]\n",
            "loss: 0.000258  [112000/167876]\n",
            "loss: 0.000241  [114800/167876]\n",
            "loss: 0.000227  [117600/167876]\n",
            "loss: 0.000243  [120400/167876]\n",
            "loss: 0.000260  [123200/167876]\n",
            "loss: 0.000294  [126000/167876]\n",
            "loss: 0.000179  [128800/167876]\n",
            "loss: 0.000505  [131600/167876]\n",
            "loss: 0.000457  [134400/167876]\n",
            "loss: 0.000303  [137200/167876]\n",
            "loss: 0.000222  [140000/167876]\n",
            "loss: 0.000210  [142800/167876]\n",
            "loss: 0.000181  [145600/167876]\n",
            "loss: 0.000264  [148400/167876]\n",
            "loss: 0.000186  [151200/167876]\n",
            "loss: 0.000245  [154000/167876]\n",
            "loss: 0.000320  [156800/167876]\n",
            "loss: 0.000155  [159600/167876]\n",
            "loss: 0.000161  [162400/167876]\n",
            "loss: 0.000188  [165200/167876]\n",
            "delay time: avg MSE: 0.000395, avg abs error: 0.0141\n",
            "learning rate: 0.000640 -> 0.000550\n",
            "---------------------------\n",
            "\n",
            "Epoch 6\n",
            "loss: 0.000275  [  0/167876]\n",
            "loss: 0.000245  [2800/167876]\n",
            "loss: 0.000162  [5600/167876]\n",
            "loss: 0.000164  [8400/167876]\n",
            "loss: 0.000236  [11200/167876]\n",
            "loss: 0.000196  [14000/167876]\n",
            "loss: 0.000296  [16800/167876]\n",
            "loss: 0.000225  [19600/167876]\n",
            "loss: 0.000155  [22400/167876]\n",
            "loss: 0.000191  [25200/167876]\n",
            "loss: 0.000226  [28000/167876]\n",
            "loss: 0.000266  [30800/167876]\n",
            "loss: 0.000183  [33600/167876]\n",
            "loss: 0.000210  [36400/167876]\n",
            "loss: 0.000203  [39200/167876]\n",
            "loss: 0.000258  [42000/167876]\n",
            "loss: 0.000280  [44800/167876]\n",
            "loss: 0.000156  [47600/167876]\n",
            "loss: 0.000201  [50400/167876]\n",
            "loss: 0.000287  [53200/167876]\n",
            "loss: 0.000178  [56000/167876]\n",
            "loss: 0.000323  [58800/167876]\n",
            "loss: 0.000178  [61600/167876]\n",
            "loss: 0.000228  [64400/167876]\n",
            "loss: 0.000242  [67200/167876]\n",
            "loss: 0.000303  [70000/167876]\n",
            "loss: 0.000354  [72800/167876]\n",
            "loss: 0.000291  [75600/167876]\n",
            "loss: 0.000254  [78400/167876]\n",
            "loss: 0.000314  [81200/167876]\n",
            "loss: 0.000267  [84000/167876]\n",
            "loss: 0.000231  [86800/167876]\n",
            "loss: 0.000165  [89600/167876]\n",
            "loss: 0.000149  [92400/167876]\n",
            "loss: 0.000174  [95200/167876]\n",
            "loss: 0.000128  [98000/167876]\n",
            "loss: 0.000171  [100800/167876]\n",
            "loss: 0.000134  [103600/167876]\n",
            "loss: 0.000191  [106400/167876]\n",
            "loss: 0.000153  [109200/167876]\n",
            "loss: 0.000163  [112000/167876]\n",
            "loss: 0.000266  [114800/167876]\n",
            "loss: 0.000154  [117600/167876]\n",
            "loss: 0.000204  [120400/167876]\n",
            "loss: 0.000176  [123200/167876]\n",
            "loss: 0.000219  [126000/167876]\n",
            "loss: 0.000278  [128800/167876]\n",
            "loss: 0.000217  [131600/167876]\n",
            "loss: 0.000180  [134400/167876]\n",
            "loss: 0.000161  [137200/167876]\n",
            "loss: 0.000290  [140000/167876]\n",
            "loss: 0.000182  [142800/167876]\n",
            "loss: 0.000292  [145600/167876]\n",
            "loss: 0.000270  [148400/167876]\n",
            "loss: 0.000265  [151200/167876]\n",
            "loss: 0.000241  [154000/167876]\n",
            "loss: 0.000173  [156800/167876]\n",
            "loss: 0.000173  [159600/167876]\n",
            "loss: 0.000192  [162400/167876]\n",
            "loss: 0.000332  [165200/167876]\n",
            "delay time: avg MSE: 0.000414, avg abs error: 0.0161\n",
            "learning rate: 0.000550 -> 0.000460\n",
            "---------------------------\n",
            "\n",
            "Epoch 7\n",
            "loss: 0.000471  [  0/167876]\n",
            "loss: 0.000144  [2800/167876]\n",
            "loss: 0.000311  [5600/167876]\n",
            "loss: 0.000324  [8400/167876]\n",
            "loss: 0.000233  [11200/167876]\n",
            "loss: 0.000204  [14000/167876]\n",
            "loss: 0.000282  [16800/167876]\n",
            "loss: 0.000217  [19600/167876]\n",
            "loss: 0.000120  [22400/167876]\n",
            "loss: 0.000271  [25200/167876]\n",
            "loss: 0.000250  [28000/167876]\n",
            "loss: 0.000187  [30800/167876]\n",
            "loss: 0.000195  [33600/167876]\n",
            "loss: 0.000195  [36400/167876]\n",
            "loss: 0.000150  [39200/167876]\n",
            "loss: 0.000121  [42000/167876]\n",
            "loss: 0.000201  [44800/167876]\n",
            "loss: 0.000334  [47600/167876]\n",
            "loss: 0.000294  [50400/167876]\n",
            "loss: 0.000185  [53200/167876]\n",
            "loss: 0.000134  [56000/167876]\n",
            "loss: 0.000163  [58800/167876]\n",
            "loss: 0.000147  [61600/167876]\n",
            "loss: 0.000135  [64400/167876]\n",
            "loss: 0.000132  [67200/167876]\n",
            "loss: 0.000161  [70000/167876]\n",
            "loss: 0.000168  [72800/167876]\n",
            "loss: 0.000178  [75600/167876]\n",
            "loss: 0.000152  [78400/167876]\n",
            "loss: 0.000337  [81200/167876]\n",
            "loss: 0.000194  [84000/167876]\n",
            "loss: 0.000148  [86800/167876]\n",
            "loss: 0.000206  [89600/167876]\n",
            "loss: 0.000226  [92400/167876]\n",
            "loss: 0.000191  [95200/167876]\n",
            "loss: 0.000241  [98000/167876]\n",
            "loss: 0.000107  [100800/167876]\n",
            "loss: 0.000279  [103600/167876]\n",
            "loss: 0.000237  [106400/167876]\n",
            "loss: 0.000237  [109200/167876]\n",
            "loss: 0.000200  [112000/167876]\n",
            "loss: 0.000209  [114800/167876]\n",
            "loss: 0.000438  [117600/167876]\n",
            "loss: 0.000178  [120400/167876]\n",
            "loss: 0.000159  [123200/167876]\n",
            "loss: 0.000324  [126000/167876]\n",
            "loss: 0.000219  [128800/167876]\n",
            "loss: 0.000156  [131600/167876]\n",
            "loss: 0.000241  [134400/167876]\n",
            "loss: 0.000215  [137200/167876]\n",
            "loss: 0.000160  [140000/167876]\n",
            "loss: 0.000177  [142800/167876]\n",
            "loss: 0.000175  [145600/167876]\n",
            "loss: 0.000212  [148400/167876]\n",
            "loss: 0.000213  [151200/167876]\n",
            "loss: 0.000168  [154000/167876]\n",
            "loss: 0.000200  [156800/167876]\n",
            "loss: 0.000146  [159600/167876]\n",
            "loss: 0.000164  [162400/167876]\n",
            "loss: 0.000247  [165200/167876]\n",
            "delay time: avg MSE: 0.000155, avg abs error: 0.0085\n",
            "learning rate: 0.000460 -> 0.000370\n",
            "---------------------------\n",
            "\n",
            "Epoch 8\n",
            "loss: 0.000148  [  0/167876]\n",
            "loss: 0.000141  [2800/167876]\n",
            "loss: 0.000153  [5600/167876]\n",
            "loss: 0.000176  [8400/167876]\n",
            "loss: 0.000128  [11200/167876]\n",
            "loss: 0.000104  [14000/167876]\n",
            "loss: 0.000077  [16800/167876]\n",
            "loss: 0.000164  [19600/167876]\n",
            "loss: 0.000159  [22400/167876]\n",
            "loss: 0.000180  [25200/167876]\n",
            "loss: 0.000150  [28000/167876]\n",
            "loss: 0.000165  [30800/167876]\n",
            "loss: 0.000189  [33600/167876]\n",
            "loss: 0.000164  [36400/167876]\n",
            "loss: 0.000123  [39200/167876]\n",
            "loss: 0.000194  [42000/167876]\n",
            "loss: 0.000148  [44800/167876]\n",
            "loss: 0.000179  [47600/167876]\n",
            "loss: 0.000116  [50400/167876]\n",
            "loss: 0.000129  [53200/167876]\n",
            "loss: 0.000123  [56000/167876]\n",
            "loss: 0.000107  [58800/167876]\n",
            "loss: 0.000178  [61600/167876]\n",
            "loss: 0.000206  [64400/167876]\n",
            "loss: 0.000216  [67200/167876]\n",
            "loss: 0.000155  [70000/167876]\n",
            "loss: 0.000183  [72800/167876]\n",
            "loss: 0.000180  [75600/167876]\n",
            "loss: 0.000098  [78400/167876]\n",
            "loss: 0.000184  [81200/167876]\n",
            "loss: 0.000124  [84000/167876]\n",
            "loss: 0.000112  [86800/167876]\n",
            "loss: 0.000152  [89600/167876]\n",
            "loss: 0.000139  [92400/167876]\n",
            "loss: 0.000186  [95200/167876]\n",
            "loss: 0.000232  [98000/167876]\n",
            "loss: 0.000109  [100800/167876]\n",
            "loss: 0.000100  [103600/167876]\n",
            "loss: 0.000125  [106400/167876]\n",
            "loss: 0.000116  [109200/167876]\n",
            "loss: 0.000126  [112000/167876]\n",
            "loss: 0.000139  [114800/167876]\n",
            "loss: 0.000106  [117600/167876]\n",
            "loss: 0.000159  [120400/167876]\n",
            "loss: 0.000147  [123200/167876]\n",
            "loss: 0.000109  [126000/167876]\n",
            "loss: 0.000122  [128800/167876]\n",
            "loss: 0.000120  [131600/167876]\n",
            "loss: 0.000163  [134400/167876]\n",
            "loss: 0.000163  [137200/167876]\n",
            "loss: 0.000169  [140000/167876]\n",
            "loss: 0.000141  [142800/167876]\n",
            "loss: 0.000175  [145600/167876]\n",
            "loss: 0.000219  [148400/167876]\n",
            "loss: 0.000266  [151200/167876]\n",
            "loss: 0.000364  [154000/167876]\n",
            "loss: 0.000162  [156800/167876]\n",
            "loss: 0.000158  [159600/167876]\n",
            "loss: 0.000223  [162400/167876]\n",
            "loss: 0.000115  [165200/167876]\n",
            "delay time: avg MSE: 0.000166, avg abs error: 0.0091\n",
            "learning rate: 0.000370 -> 0.000280\n",
            "---------------------------\n",
            "\n",
            "Epoch 9\n",
            "loss: 0.000135  [  0/167876]\n",
            "loss: 0.000100  [2800/167876]\n",
            "loss: 0.000148  [5600/167876]\n",
            "loss: 0.000207  [8400/167876]\n",
            "loss: 0.000112  [11200/167876]\n",
            "loss: 0.000129  [14000/167876]\n",
            "loss: 0.000212  [16800/167876]\n",
            "loss: 0.000192  [19600/167876]\n",
            "loss: 0.000091  [22400/167876]\n",
            "loss: 0.000106  [25200/167876]\n",
            "loss: 0.000140  [28000/167876]\n",
            "loss: 0.000163  [30800/167876]\n",
            "loss: 0.000213  [33600/167876]\n",
            "loss: 0.000162  [36400/167876]\n",
            "loss: 0.000139  [39200/167876]\n",
            "loss: 0.000121  [42000/167876]\n",
            "loss: 0.000237  [44800/167876]\n",
            "loss: 0.000157  [47600/167876]\n",
            "loss: 0.000131  [50400/167876]\n",
            "loss: 0.000171  [53200/167876]\n",
            "loss: 0.000123  [56000/167876]\n",
            "loss: 0.000141  [58800/167876]\n",
            "loss: 0.000141  [61600/167876]\n",
            "loss: 0.000095  [64400/167876]\n",
            "loss: 0.000105  [67200/167876]\n",
            "loss: 0.000124  [70000/167876]\n",
            "loss: 0.000109  [72800/167876]\n",
            "loss: 0.000274  [75600/167876]\n",
            "loss: 0.000163  [78400/167876]\n",
            "loss: 0.000133  [81200/167876]\n",
            "loss: 0.000113  [84000/167876]\n",
            "loss: 0.000136  [86800/167876]\n",
            "loss: 0.000108  [89600/167876]\n",
            "loss: 0.000146  [92400/167876]\n",
            "loss: 0.000135  [95200/167876]\n",
            "loss: 0.000252  [98000/167876]\n",
            "loss: 0.000162  [100800/167876]\n",
            "loss: 0.000168  [103600/167876]\n",
            "loss: 0.000160  [106400/167876]\n",
            "loss: 0.000109  [109200/167876]\n",
            "loss: 0.000095  [112000/167876]\n",
            "loss: 0.000322  [114800/167876]\n",
            "loss: 0.000143  [117600/167876]\n",
            "loss: 0.000113  [120400/167876]\n",
            "loss: 0.000136  [123200/167876]\n",
            "loss: 0.000186  [126000/167876]\n",
            "loss: 0.000152  [128800/167876]\n",
            "loss: 0.000134  [131600/167876]\n",
            "loss: 0.000139  [134400/167876]\n",
            "loss: 0.000225  [137200/167876]\n",
            "loss: 0.000162  [140000/167876]\n",
            "loss: 0.000151  [142800/167876]\n",
            "loss: 0.000190  [145600/167876]\n",
            "loss: 0.000166  [148400/167876]\n",
            "loss: 0.000100  [151200/167876]\n",
            "loss: 0.000138  [154000/167876]\n",
            "loss: 0.000213  [156800/167876]\n",
            "loss: 0.000119  [159600/167876]\n",
            "loss: 0.000193  [162400/167876]\n",
            "loss: 0.000115  [165200/167876]\n",
            "delay time: avg MSE: 0.000169, avg abs error: 0.0092\n",
            "learning rate: 0.000280 -> 0.000190\n",
            "---------------------------\n",
            "\n",
            "Epoch 10\n",
            "loss: 0.000144  [  0/167876]\n",
            "loss: 0.000147  [2800/167876]\n",
            "loss: 0.000105  [5600/167876]\n",
            "loss: 0.000090  [8400/167876]\n",
            "loss: 0.000158  [11200/167876]\n",
            "loss: 0.000186  [14000/167876]\n",
            "loss: 0.000156  [16800/167876]\n",
            "loss: 0.000085  [19600/167876]\n",
            "loss: 0.000162  [22400/167876]\n",
            "loss: 0.000086  [25200/167876]\n",
            "loss: 0.000117  [28000/167876]\n",
            "loss: 0.000126  [30800/167876]\n",
            "loss: 0.000092  [33600/167876]\n",
            "loss: 0.000183  [36400/167876]\n",
            "loss: 0.000103  [39200/167876]\n",
            "loss: 0.000118  [42000/167876]\n",
            "loss: 0.000161  [44800/167876]\n",
            "loss: 0.000151  [47600/167876]\n",
            "loss: 0.000127  [50400/167876]\n",
            "loss: 0.000125  [53200/167876]\n",
            "loss: 0.000194  [56000/167876]\n",
            "loss: 0.000195  [58800/167876]\n",
            "loss: 0.000115  [61600/167876]\n",
            "loss: 0.000122  [64400/167876]\n",
            "loss: 0.000108  [67200/167876]\n",
            "loss: 0.000171  [70000/167876]\n",
            "loss: 0.000107  [72800/167876]\n",
            "loss: 0.000189  [75600/167876]\n",
            "loss: 0.000153  [78400/167876]\n",
            "loss: 0.000091  [81200/167876]\n",
            "loss: 0.000088  [84000/167876]\n",
            "loss: 0.000099  [86800/167876]\n",
            "loss: 0.000165  [89600/167876]\n",
            "loss: 0.000146  [92400/167876]\n",
            "loss: 0.000182  [95200/167876]\n",
            "loss: 0.000097  [98000/167876]\n",
            "loss: 0.000146  [100800/167876]\n",
            "loss: 0.000138  [103600/167876]\n",
            "loss: 0.000113  [106400/167876]\n",
            "loss: 0.000135  [109200/167876]\n",
            "loss: 0.000131  [112000/167876]\n",
            "loss: 0.000151  [114800/167876]\n",
            "loss: 0.000082  [117600/167876]\n",
            "loss: 0.000129  [120400/167876]\n",
            "loss: 0.000076  [123200/167876]\n",
            "loss: 0.000113  [126000/167876]\n",
            "loss: 0.000126  [128800/167876]\n",
            "loss: 0.000138  [131600/167876]\n",
            "loss: 0.000121  [134400/167876]\n",
            "loss: 0.000103  [137200/167876]\n",
            "loss: 0.000143  [140000/167876]\n",
            "loss: 0.000100  [142800/167876]\n",
            "loss: 0.000113  [145600/167876]\n",
            "loss: 0.000122  [148400/167876]\n",
            "loss: 0.000132  [151200/167876]\n",
            "loss: 0.000184  [154000/167876]\n",
            "loss: 0.000136  [156800/167876]\n",
            "loss: 0.000152  [159600/167876]\n",
            "loss: 0.000137  [162400/167876]\n",
            "loss: 0.000097  [165200/167876]\n",
            "delay time: avg MSE: 0.000127, avg abs error: 0.0076\n",
            "learning rate: 0.000190 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 11\n",
            "loss: 0.000120  [  0/167876]\n",
            "loss: 0.000069  [2800/167876]\n",
            "loss: 0.000120  [5600/167876]\n",
            "loss: 0.000074  [8400/167876]\n",
            "loss: 0.000125  [11200/167876]\n",
            "loss: 0.000083  [14000/167876]\n",
            "loss: 0.000091  [16800/167876]\n",
            "loss: 0.000094  [19600/167876]\n",
            "loss: 0.000091  [22400/167876]\n",
            "loss: 0.000093  [25200/167876]\n",
            "loss: 0.000108  [28000/167876]\n",
            "loss: 0.000130  [30800/167876]\n",
            "loss: 0.000113  [33600/167876]\n",
            "loss: 0.000106  [36400/167876]\n",
            "loss: 0.000106  [39200/167876]\n",
            "loss: 0.000071  [42000/167876]\n",
            "loss: 0.000100  [44800/167876]\n",
            "loss: 0.000105  [47600/167876]\n",
            "loss: 0.000117  [50400/167876]\n",
            "loss: 0.000127  [53200/167876]\n",
            "loss: 0.000094  [56000/167876]\n",
            "loss: 0.000070  [58800/167876]\n",
            "loss: 0.000070  [61600/167876]\n",
            "loss: 0.000067  [64400/167876]\n",
            "loss: 0.000098  [67200/167876]\n",
            "loss: 0.000086  [70000/167876]\n",
            "loss: 0.000165  [72800/167876]\n",
            "loss: 0.000137  [75600/167876]\n",
            "loss: 0.000094  [78400/167876]\n",
            "loss: 0.000100  [81200/167876]\n",
            "loss: 0.000069  [84000/167876]\n",
            "loss: 0.000089  [86800/167876]\n",
            "loss: 0.000096  [89600/167876]\n",
            "loss: 0.000150  [92400/167876]\n",
            "loss: 0.000073  [95200/167876]\n",
            "loss: 0.000072  [98000/167876]\n",
            "loss: 0.000079  [100800/167876]\n",
            "loss: 0.000094  [103600/167876]\n",
            "loss: 0.000101  [106400/167876]\n",
            "loss: 0.000146  [109200/167876]\n",
            "loss: 0.000063  [112000/167876]\n",
            "loss: 0.000099  [114800/167876]\n",
            "loss: 0.000097  [117600/167876]\n",
            "loss: 0.000105  [120400/167876]\n",
            "loss: 0.000136  [123200/167876]\n",
            "loss: 0.000085  [126000/167876]\n",
            "loss: 0.000087  [128800/167876]\n",
            "loss: 0.000072  [131600/167876]\n",
            "loss: 0.000112  [134400/167876]\n",
            "loss: 0.000121  [137200/167876]\n",
            "loss: 0.000121  [140000/167876]\n",
            "loss: 0.000136  [142800/167876]\n",
            "loss: 0.000086  [145600/167876]\n",
            "loss: 0.000111  [148400/167876]\n",
            "loss: 0.000116  [151200/167876]\n",
            "loss: 0.000079  [154000/167876]\n",
            "loss: 0.000085  [156800/167876]\n",
            "loss: 0.000090  [159600/167876]\n",
            "loss: 0.000129  [162400/167876]\n",
            "loss: 0.000113  [165200/167876]\n",
            "delay time: avg MSE: 0.000123, avg abs error: 0.0073\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 12\n",
            "loss: 0.000115  [  0/167876]\n",
            "loss: 0.000101  [2800/167876]\n",
            "loss: 0.000055  [5600/167876]\n",
            "loss: 0.000077  [8400/167876]\n",
            "loss: 0.000084  [11200/167876]\n",
            "loss: 0.000073  [14000/167876]\n",
            "loss: 0.000094  [16800/167876]\n",
            "loss: 0.000089  [19600/167876]\n",
            "loss: 0.000100  [22400/167876]\n",
            "loss: 0.000129  [25200/167876]\n",
            "loss: 0.000113  [28000/167876]\n",
            "loss: 0.000086  [30800/167876]\n",
            "loss: 0.000106  [33600/167876]\n",
            "loss: 0.000066  [36400/167876]\n",
            "loss: 0.000123  [39200/167876]\n",
            "loss: 0.000063  [42000/167876]\n",
            "loss: 0.000075  [44800/167876]\n",
            "loss: 0.000142  [47600/167876]\n",
            "loss: 0.000133  [50400/167876]\n",
            "loss: 0.000110  [53200/167876]\n",
            "loss: 0.000102  [56000/167876]\n",
            "loss: 0.000157  [58800/167876]\n",
            "loss: 0.000101  [61600/167876]\n",
            "loss: 0.000130  [64400/167876]\n",
            "loss: 0.000111  [67200/167876]\n",
            "loss: 0.000075  [70000/167876]\n",
            "loss: 0.000118  [72800/167876]\n",
            "loss: 0.000100  [75600/167876]\n",
            "loss: 0.000071  [78400/167876]\n",
            "loss: 0.000091  [81200/167876]\n",
            "loss: 0.000245  [84000/167876]\n",
            "loss: 0.000108  [86800/167876]\n",
            "loss: 0.000149  [89600/167876]\n",
            "loss: 0.000138  [92400/167876]\n",
            "loss: 0.000082  [95200/167876]\n",
            "loss: 0.000103  [98000/167876]\n",
            "loss: 0.000070  [100800/167876]\n",
            "loss: 0.000091  [103600/167876]\n",
            "loss: 0.000096  [106400/167876]\n",
            "loss: 0.000172  [109200/167876]\n",
            "loss: 0.000109  [112000/167876]\n",
            "loss: 0.000073  [114800/167876]\n",
            "loss: 0.000100  [117600/167876]\n",
            "loss: 0.000083  [120400/167876]\n",
            "loss: 0.000057  [123200/167876]\n",
            "loss: 0.000111  [126000/167876]\n",
            "loss: 0.000101  [128800/167876]\n",
            "loss: 0.000080  [131600/167876]\n",
            "loss: 0.000059  [134400/167876]\n",
            "loss: 0.000094  [137200/167876]\n",
            "loss: 0.000098  [140000/167876]\n",
            "loss: 0.000125  [142800/167876]\n",
            "loss: 0.000077  [145600/167876]\n",
            "loss: 0.000112  [148400/167876]\n",
            "loss: 0.000204  [151200/167876]\n",
            "loss: 0.000093  [154000/167876]\n",
            "loss: 0.000103  [156800/167876]\n",
            "loss: 0.000127  [159600/167876]\n",
            "loss: 0.000099  [162400/167876]\n",
            "loss: 0.000115  [165200/167876]\n",
            "delay time: avg MSE: 0.000126, avg abs error: 0.0078\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 13\n",
            "loss: 0.000090  [  0/167876]\n",
            "loss: 0.000140  [2800/167876]\n",
            "loss: 0.000064  [5600/167876]\n",
            "loss: 0.000105  [8400/167876]\n",
            "loss: 0.000085  [11200/167876]\n",
            "loss: 0.000118  [14000/167876]\n",
            "loss: 0.000078  [16800/167876]\n",
            "loss: 0.000088  [19600/167876]\n",
            "loss: 0.000076  [22400/167876]\n",
            "loss: 0.000101  [25200/167876]\n",
            "loss: 0.000097  [28000/167876]\n",
            "loss: 0.000099  [30800/167876]\n",
            "loss: 0.000067  [33600/167876]\n",
            "loss: 0.000202  [36400/167876]\n",
            "loss: 0.000097  [39200/167876]\n",
            "loss: 0.000120  [42000/167876]\n",
            "loss: 0.000091  [44800/167876]\n",
            "loss: 0.000113  [47600/167876]\n",
            "loss: 0.000086  [50400/167876]\n",
            "loss: 0.000121  [53200/167876]\n",
            "loss: 0.000080  [56000/167876]\n",
            "loss: 0.000103  [58800/167876]\n",
            "loss: 0.000145  [61600/167876]\n",
            "loss: 0.000109  [64400/167876]\n",
            "loss: 0.000080  [67200/167876]\n",
            "loss: 0.000163  [70000/167876]\n",
            "loss: 0.000086  [72800/167876]\n",
            "loss: 0.000119  [75600/167876]\n",
            "loss: 0.000085  [78400/167876]\n",
            "loss: 0.000081  [81200/167876]\n",
            "loss: 0.000083  [84000/167876]\n",
            "loss: 0.000077  [86800/167876]\n",
            "loss: 0.000103  [89600/167876]\n",
            "loss: 0.000108  [92400/167876]\n",
            "loss: 0.000115  [95200/167876]\n",
            "loss: 0.000094  [98000/167876]\n",
            "loss: 0.000079  [100800/167876]\n",
            "loss: 0.000079  [103600/167876]\n",
            "loss: 0.000153  [106400/167876]\n",
            "loss: 0.000096  [109200/167876]\n",
            "loss: 0.000101  [112000/167876]\n",
            "loss: 0.000083  [114800/167876]\n",
            "loss: 0.000113  [117600/167876]\n",
            "loss: 0.000089  [120400/167876]\n",
            "loss: 0.000149  [123200/167876]\n",
            "loss: 0.000075  [126000/167876]\n",
            "loss: 0.000096  [128800/167876]\n",
            "loss: 0.000108  [131600/167876]\n",
            "loss: 0.000113  [134400/167876]\n",
            "loss: 0.000161  [137200/167876]\n",
            "loss: 0.000097  [140000/167876]\n",
            "loss: 0.000070  [142800/167876]\n",
            "loss: 0.000136  [145600/167876]\n",
            "loss: 0.000102  [148400/167876]\n",
            "loss: 0.000170  [151200/167876]\n",
            "loss: 0.000092  [154000/167876]\n",
            "loss: 0.000114  [156800/167876]\n",
            "loss: 0.000067  [159600/167876]\n",
            "loss: 0.000092  [162400/167876]\n",
            "loss: 0.000104  [165200/167876]\n",
            "delay time: avg MSE: 0.000132, avg abs error: 0.0079\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 14\n",
            "loss: 0.000192  [  0/167876]\n",
            "loss: 0.000118  [2800/167876]\n",
            "loss: 0.000080  [5600/167876]\n",
            "loss: 0.000092  [8400/167876]\n",
            "loss: 0.000060  [11200/167876]\n",
            "loss: 0.000085  [14000/167876]\n",
            "loss: 0.000089  [16800/167876]\n",
            "loss: 0.000093  [19600/167876]\n",
            "loss: 0.000139  [22400/167876]\n",
            "loss: 0.000136  [25200/167876]\n",
            "loss: 0.000082  [28000/167876]\n",
            "loss: 0.000135  [30800/167876]\n",
            "loss: 0.000075  [33600/167876]\n",
            "loss: 0.000097  [36400/167876]\n",
            "loss: 0.000111  [39200/167876]\n",
            "loss: 0.000167  [42000/167876]\n",
            "loss: 0.000092  [44800/167876]\n",
            "loss: 0.000106  [47600/167876]\n",
            "loss: 0.000070  [50400/167876]\n",
            "loss: 0.000150  [53200/167876]\n",
            "loss: 0.000129  [56000/167876]\n",
            "loss: 0.000110  [58800/167876]\n",
            "loss: 0.000115  [61600/167876]\n",
            "loss: 0.000068  [64400/167876]\n",
            "loss: 0.000131  [67200/167876]\n",
            "loss: 0.000072  [70000/167876]\n",
            "loss: 0.000081  [72800/167876]\n",
            "loss: 0.000067  [75600/167876]\n",
            "loss: 0.000127  [78400/167876]\n",
            "loss: 0.000062  [81200/167876]\n",
            "loss: 0.000088  [84000/167876]\n",
            "loss: 0.000116  [86800/167876]\n",
            "loss: 0.000080  [89600/167876]\n",
            "loss: 0.000096  [92400/167876]\n",
            "loss: 0.000135  [95200/167876]\n",
            "loss: 0.000092  [98000/167876]\n",
            "loss: 0.000133  [100800/167876]\n",
            "loss: 0.000098  [103600/167876]\n",
            "loss: 0.000082  [106400/167876]\n",
            "loss: 0.000115  [109200/167876]\n",
            "loss: 0.000093  [112000/167876]\n",
            "loss: 0.000136  [114800/167876]\n",
            "loss: 0.000101  [117600/167876]\n",
            "loss: 0.000110  [120400/167876]\n",
            "loss: 0.000089  [123200/167876]\n",
            "loss: 0.000185  [126000/167876]\n",
            "loss: 0.000165  [128800/167876]\n",
            "loss: 0.000090  [131600/167876]\n",
            "loss: 0.000087  [134400/167876]\n",
            "loss: 0.000161  [137200/167876]\n",
            "loss: 0.000148  [140000/167876]\n",
            "loss: 0.000098  [142800/167876]\n",
            "loss: 0.000089  [145600/167876]\n",
            "loss: 0.000097  [148400/167876]\n",
            "loss: 0.000065  [151200/167876]\n",
            "loss: 0.000108  [154000/167876]\n",
            "loss: 0.000075  [156800/167876]\n",
            "loss: 0.000086  [159600/167876]\n",
            "loss: 0.000060  [162400/167876]\n",
            "loss: 0.000091  [165200/167876]\n",
            "delay time: avg MSE: 0.000135, avg abs error: 0.0083\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 15\n",
            "loss: 0.000090  [  0/167876]\n",
            "loss: 0.000084  [2800/167876]\n",
            "loss: 0.000109  [5600/167876]\n",
            "loss: 0.000082  [8400/167876]\n",
            "loss: 0.000082  [11200/167876]\n",
            "loss: 0.000117  [14000/167876]\n",
            "loss: 0.000117  [16800/167876]\n",
            "loss: 0.000098  [19600/167876]\n",
            "loss: 0.000043  [22400/167876]\n",
            "loss: 0.000087  [25200/167876]\n",
            "loss: 0.000110  [28000/167876]\n",
            "loss: 0.000115  [30800/167876]\n",
            "loss: 0.000086  [33600/167876]\n",
            "loss: 0.000068  [36400/167876]\n",
            "loss: 0.000072  [39200/167876]\n",
            "loss: 0.000096  [42000/167876]\n",
            "loss: 0.000092  [44800/167876]\n",
            "loss: 0.000101  [47600/167876]\n",
            "loss: 0.000106  [50400/167876]\n",
            "loss: 0.000093  [53200/167876]\n",
            "loss: 0.000108  [56000/167876]\n",
            "loss: 0.000131  [58800/167876]\n",
            "loss: 0.000094  [61600/167876]\n",
            "loss: 0.000159  [64400/167876]\n",
            "loss: 0.000069  [67200/167876]\n",
            "loss: 0.000092  [70000/167876]\n",
            "loss: 0.000080  [72800/167876]\n",
            "loss: 0.000104  [75600/167876]\n",
            "loss: 0.000129  [78400/167876]\n",
            "loss: 0.000091  [81200/167876]\n",
            "loss: 0.000112  [84000/167876]\n",
            "loss: 0.000116  [86800/167876]\n",
            "loss: 0.000121  [89600/167876]\n",
            "loss: 0.000079  [92400/167876]\n",
            "loss: 0.000091  [95200/167876]\n",
            "loss: 0.000094  [98000/167876]\n",
            "loss: 0.000109  [100800/167876]\n",
            "loss: 0.000069  [103600/167876]\n",
            "loss: 0.000098  [106400/167876]\n",
            "loss: 0.000072  [109200/167876]\n",
            "loss: 0.000073  [112000/167876]\n",
            "loss: 0.000098  [114800/167876]\n",
            "loss: 0.000054  [117600/167876]\n",
            "loss: 0.000112  [120400/167876]\n",
            "loss: 0.000139  [123200/167876]\n",
            "loss: 0.000111  [126000/167876]\n",
            "loss: 0.000050  [128800/167876]\n",
            "loss: 0.000078  [131600/167876]\n",
            "loss: 0.000078  [134400/167876]\n",
            "loss: 0.000115  [137200/167876]\n",
            "loss: 0.000083  [140000/167876]\n",
            "loss: 0.000085  [142800/167876]\n",
            "loss: 0.000077  [145600/167876]\n",
            "loss: 0.000098  [148400/167876]\n",
            "loss: 0.000093  [151200/167876]\n",
            "loss: 0.000139  [154000/167876]\n",
            "loss: 0.000062  [156800/167876]\n",
            "loss: 0.000164  [159600/167876]\n",
            "loss: 0.000069  [162400/167876]\n",
            "loss: 0.000122  [165200/167876]\n",
            "delay time: avg MSE: 0.000129, avg abs error: 0.0081\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 16\n",
            "loss: 0.000098  [  0/167876]\n",
            "loss: 0.000093  [2800/167876]\n",
            "loss: 0.000124  [5600/167876]\n",
            "loss: 0.000064  [8400/167876]\n",
            "loss: 0.000064  [11200/167876]\n",
            "loss: 0.000103  [14000/167876]\n",
            "loss: 0.000112  [16800/167876]\n",
            "loss: 0.000133  [19600/167876]\n",
            "loss: 0.000058  [22400/167876]\n",
            "loss: 0.000064  [25200/167876]\n",
            "loss: 0.000098  [28000/167876]\n",
            "loss: 0.000060  [30800/167876]\n",
            "loss: 0.000107  [33600/167876]\n",
            "loss: 0.000071  [36400/167876]\n",
            "loss: 0.000111  [39200/167876]\n",
            "loss: 0.000079  [42000/167876]\n",
            "loss: 0.000092  [44800/167876]\n",
            "loss: 0.000079  [47600/167876]\n",
            "loss: 0.000100  [50400/167876]\n",
            "loss: 0.000113  [53200/167876]\n",
            "loss: 0.000135  [56000/167876]\n",
            "loss: 0.000092  [58800/167876]\n",
            "loss: 0.000096  [61600/167876]\n",
            "loss: 0.000103  [64400/167876]\n",
            "loss: 0.000093  [67200/167876]\n",
            "loss: 0.000099  [70000/167876]\n",
            "loss: 0.000085  [72800/167876]\n",
            "loss: 0.000055  [75600/167876]\n",
            "loss: 0.000062  [78400/167876]\n",
            "loss: 0.000114  [81200/167876]\n",
            "loss: 0.000060  [84000/167876]\n",
            "loss: 0.000073  [86800/167876]\n",
            "loss: 0.000132  [89600/167876]\n",
            "loss: 0.000125  [92400/167876]\n",
            "loss: 0.000109  [95200/167876]\n",
            "loss: 0.000073  [98000/167876]\n",
            "loss: 0.000127  [100800/167876]\n",
            "loss: 0.000097  [103600/167876]\n",
            "loss: 0.000102  [106400/167876]\n",
            "loss: 0.000079  [109200/167876]\n",
            "loss: 0.000051  [112000/167876]\n",
            "loss: 0.000080  [114800/167876]\n",
            "loss: 0.000105  [117600/167876]\n",
            "loss: 0.000061  [120400/167876]\n",
            "loss: 0.000090  [123200/167876]\n",
            "loss: 0.000094  [126000/167876]\n",
            "loss: 0.000074  [128800/167876]\n",
            "loss: 0.000129  [131600/167876]\n",
            "loss: 0.000102  [134400/167876]\n",
            "loss: 0.000097  [137200/167876]\n",
            "loss: 0.000096  [140000/167876]\n",
            "loss: 0.000066  [142800/167876]\n",
            "loss: 0.000089  [145600/167876]\n",
            "loss: 0.000085  [148400/167876]\n",
            "loss: 0.000083  [151200/167876]\n",
            "loss: 0.000085  [154000/167876]\n",
            "loss: 0.000089  [156800/167876]\n",
            "loss: 0.000091  [159600/167876]\n",
            "loss: 0.000090  [162400/167876]\n",
            "loss: 0.000055  [165200/167876]\n",
            "delay time: avg MSE: 0.000101, avg abs error: 0.0064\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 17\n",
            "loss: 0.000060  [  0/167876]\n",
            "loss: 0.000069  [2800/167876]\n",
            "loss: 0.000107  [5600/167876]\n",
            "loss: 0.000073  [8400/167876]\n",
            "loss: 0.000075  [11200/167876]\n",
            "loss: 0.000106  [14000/167876]\n",
            "loss: 0.000124  [16800/167876]\n",
            "loss: 0.000079  [19600/167876]\n",
            "loss: 0.000082  [22400/167876]\n",
            "loss: 0.000064  [25200/167876]\n",
            "loss: 0.000090  [28000/167876]\n",
            "loss: 0.000091  [30800/167876]\n",
            "loss: 0.000084  [33600/167876]\n",
            "loss: 0.000078  [36400/167876]\n",
            "loss: 0.000078  [39200/167876]\n",
            "loss: 0.000110  [42000/167876]\n",
            "loss: 0.000076  [44800/167876]\n",
            "loss: 0.000062  [47600/167876]\n",
            "loss: 0.000076  [50400/167876]\n",
            "loss: 0.000090  [53200/167876]\n",
            "loss: 0.000105  [56000/167876]\n",
            "loss: 0.000096  [58800/167876]\n",
            "loss: 0.000099  [61600/167876]\n",
            "loss: 0.000116  [64400/167876]\n",
            "loss: 0.000094  [67200/167876]\n",
            "loss: 0.000071  [70000/167876]\n",
            "loss: 0.000062  [72800/167876]\n",
            "loss: 0.000085  [75600/167876]\n",
            "loss: 0.000107  [78400/167876]\n",
            "loss: 0.000069  [81200/167876]\n",
            "loss: 0.000158  [84000/167876]\n",
            "loss: 0.000075  [86800/167876]\n",
            "loss: 0.000119  [89600/167876]\n",
            "loss: 0.000066  [92400/167876]\n",
            "loss: 0.000129  [95200/167876]\n",
            "loss: 0.000091  [98000/167876]\n",
            "loss: 0.000126  [100800/167876]\n",
            "loss: 0.000120  [103600/167876]\n",
            "loss: 0.000113  [106400/167876]\n",
            "loss: 0.000074  [109200/167876]\n",
            "loss: 0.000093  [112000/167876]\n",
            "loss: 0.000068  [114800/167876]\n",
            "loss: 0.000096  [117600/167876]\n",
            "loss: 0.000080  [120400/167876]\n",
            "loss: 0.000121  [123200/167876]\n",
            "loss: 0.000102  [126000/167876]\n",
            "loss: 0.000093  [128800/167876]\n",
            "loss: 0.000070  [131600/167876]\n",
            "loss: 0.000059  [134400/167876]\n",
            "loss: 0.000088  [137200/167876]\n",
            "loss: 0.000071  [140000/167876]\n",
            "loss: 0.000066  [142800/167876]\n",
            "loss: 0.000060  [145600/167876]\n",
            "loss: 0.000076  [148400/167876]\n",
            "loss: 0.000093  [151200/167876]\n",
            "loss: 0.000060  [154000/167876]\n",
            "loss: 0.000100  [156800/167876]\n",
            "loss: 0.000087  [159600/167876]\n",
            "loss: 0.000067  [162400/167876]\n",
            "loss: 0.000099  [165200/167876]\n",
            "delay time: avg MSE: 0.000105, avg abs error: 0.0068\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 18\n",
            "loss: 0.000088  [  0/167876]\n",
            "loss: 0.000115  [2800/167876]\n",
            "loss: 0.000133  [5600/167876]\n",
            "loss: 0.000090  [8400/167876]\n",
            "loss: 0.000068  [11200/167876]\n",
            "loss: 0.000082  [14000/167876]\n",
            "loss: 0.000100  [16800/167876]\n",
            "loss: 0.000075  [19600/167876]\n",
            "loss: 0.000067  [22400/167876]\n",
            "loss: 0.000046  [25200/167876]\n",
            "loss: 0.000099  [28000/167876]\n",
            "loss: 0.000118  [30800/167876]\n",
            "loss: 0.000105  [33600/167876]\n",
            "loss: 0.000063  [36400/167876]\n",
            "loss: 0.000073  [39200/167876]\n",
            "loss: 0.000089  [42000/167876]\n",
            "loss: 0.000071  [44800/167876]\n",
            "loss: 0.000088  [47600/167876]\n",
            "loss: 0.000082  [50400/167876]\n",
            "loss: 0.000085  [53200/167876]\n",
            "loss: 0.000082  [56000/167876]\n",
            "loss: 0.000081  [58800/167876]\n",
            "loss: 0.000086  [61600/167876]\n",
            "loss: 0.000107  [64400/167876]\n",
            "loss: 0.000091  [67200/167876]\n",
            "loss: 0.000102  [70000/167876]\n",
            "loss: 0.000121  [72800/167876]\n",
            "loss: 0.000069  [75600/167876]\n",
            "loss: 0.000089  [78400/167876]\n",
            "loss: 0.000063  [81200/167876]\n",
            "loss: 0.000163  [84000/167876]\n",
            "loss: 0.000083  [86800/167876]\n",
            "loss: 0.000085  [89600/167876]\n",
            "loss: 0.000075  [92400/167876]\n",
            "loss: 0.000059  [95200/167876]\n",
            "loss: 0.000074  [98000/167876]\n",
            "loss: 0.000111  [100800/167876]\n",
            "loss: 0.000072  [103600/167876]\n",
            "loss: 0.000106  [106400/167876]\n",
            "loss: 0.000085  [109200/167876]\n",
            "loss: 0.000130  [112000/167876]\n",
            "loss: 0.000090  [114800/167876]\n",
            "loss: 0.000080  [117600/167876]\n",
            "loss: 0.000088  [120400/167876]\n",
            "loss: 0.000112  [123200/167876]\n",
            "loss: 0.000059  [126000/167876]\n",
            "loss: 0.000063  [128800/167876]\n",
            "loss: 0.000072  [131600/167876]\n",
            "loss: 0.000073  [134400/167876]\n",
            "loss: 0.000084  [137200/167876]\n",
            "loss: 0.000076  [140000/167876]\n",
            "loss: 0.000118  [142800/167876]\n",
            "loss: 0.000144  [145600/167876]\n",
            "loss: 0.000067  [148400/167876]\n",
            "loss: 0.000087  [151200/167876]\n",
            "loss: 0.000094  [154000/167876]\n",
            "loss: 0.000103  [156800/167876]\n",
            "loss: 0.000127  [159600/167876]\n",
            "loss: 0.000104  [162400/167876]\n",
            "loss: 0.000081  [165200/167876]\n",
            "delay time: avg MSE: 0.000097, avg abs error: 0.0063\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 19\n",
            "loss: 0.000097  [  0/167876]\n",
            "loss: 0.000070  [2800/167876]\n",
            "loss: 0.000090  [5600/167876]\n",
            "loss: 0.000090  [8400/167876]\n",
            "loss: 0.000076  [11200/167876]\n",
            "loss: 0.000093  [14000/167876]\n",
            "loss: 0.000101  [16800/167876]\n",
            "loss: 0.000081  [19600/167876]\n",
            "loss: 0.000080  [22400/167876]\n",
            "loss: 0.000075  [25200/167876]\n",
            "loss: 0.000141  [28000/167876]\n",
            "loss: 0.000120  [30800/167876]\n",
            "loss: 0.000048  [33600/167876]\n",
            "loss: 0.000107  [36400/167876]\n",
            "loss: 0.000103  [39200/167876]\n",
            "loss: 0.000081  [42000/167876]\n",
            "loss: 0.000088  [44800/167876]\n",
            "loss: 0.000062  [47600/167876]\n",
            "loss: 0.000085  [50400/167876]\n",
            "loss: 0.000109  [53200/167876]\n",
            "loss: 0.000064  [56000/167876]\n",
            "loss: 0.000080  [58800/167876]\n",
            "loss: 0.000049  [61600/167876]\n",
            "loss: 0.000170  [64400/167876]\n",
            "loss: 0.000045  [67200/167876]\n",
            "loss: 0.000066  [70000/167876]\n",
            "loss: 0.000071  [72800/167876]\n",
            "loss: 0.000053  [75600/167876]\n",
            "loss: 0.000087  [78400/167876]\n",
            "loss: 0.000065  [81200/167876]\n",
            "loss: 0.000131  [84000/167876]\n",
            "loss: 0.000081  [86800/167876]\n",
            "loss: 0.000101  [89600/167876]\n",
            "loss: 0.000067  [92400/167876]\n",
            "loss: 0.000066  [95200/167876]\n",
            "loss: 0.000089  [98000/167876]\n",
            "loss: 0.000081  [100800/167876]\n",
            "loss: 0.000139  [103600/167876]\n",
            "loss: 0.000089  [106400/167876]\n",
            "loss: 0.000069  [109200/167876]\n",
            "loss: 0.000079  [112000/167876]\n",
            "loss: 0.000085  [114800/167876]\n",
            "loss: 0.000063  [117600/167876]\n",
            "loss: 0.000070  [120400/167876]\n",
            "loss: 0.000055  [123200/167876]\n",
            "loss: 0.000060  [126000/167876]\n",
            "loss: 0.000077  [128800/167876]\n",
            "loss: 0.000093  [131600/167876]\n",
            "loss: 0.000108  [134400/167876]\n",
            "loss: 0.000090  [137200/167876]\n",
            "loss: 0.000081  [140000/167876]\n",
            "loss: 0.000108  [142800/167876]\n",
            "loss: 0.000086  [145600/167876]\n",
            "loss: 0.000069  [148400/167876]\n",
            "loss: 0.000066  [151200/167876]\n",
            "loss: 0.000072  [154000/167876]\n",
            "loss: 0.000062  [156800/167876]\n",
            "loss: 0.000105  [159600/167876]\n",
            "loss: 0.000098  [162400/167876]\n",
            "loss: 0.000115  [165200/167876]\n",
            "delay time: avg MSE: 0.000095, avg abs error: 0.0061\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 20\n",
            "loss: 0.000075  [  0/167876]\n",
            "loss: 0.000079  [2800/167876]\n",
            "loss: 0.000080  [5600/167876]\n",
            "loss: 0.000065  [8400/167876]\n",
            "loss: 0.000080  [11200/167876]\n",
            "loss: 0.000100  [14000/167876]\n",
            "loss: 0.000163  [16800/167876]\n",
            "loss: 0.000068  [19600/167876]\n",
            "loss: 0.000070  [22400/167876]\n",
            "loss: 0.000051  [25200/167876]\n",
            "loss: 0.000082  [28000/167876]\n",
            "loss: 0.000055  [30800/167876]\n",
            "loss: 0.000057  [33600/167876]\n",
            "loss: 0.000106  [36400/167876]\n",
            "loss: 0.000139  [39200/167876]\n",
            "loss: 0.000092  [42000/167876]\n",
            "loss: 0.000089  [44800/167876]\n",
            "loss: 0.000057  [47600/167876]\n",
            "loss: 0.000051  [50400/167876]\n",
            "loss: 0.000065  [53200/167876]\n",
            "loss: 0.000071  [56000/167876]\n",
            "loss: 0.000102  [58800/167876]\n",
            "loss: 0.000060  [61600/167876]\n",
            "loss: 0.000084  [64400/167876]\n",
            "loss: 0.000069  [67200/167876]\n",
            "loss: 0.000062  [70000/167876]\n",
            "loss: 0.000086  [72800/167876]\n",
            "loss: 0.000096  [75600/167876]\n",
            "loss: 0.000077  [78400/167876]\n",
            "loss: 0.000078  [81200/167876]\n",
            "loss: 0.000162  [84000/167876]\n",
            "loss: 0.000118  [86800/167876]\n",
            "loss: 0.000053  [89600/167876]\n",
            "loss: 0.000067  [92400/167876]\n",
            "loss: 0.000078  [95200/167876]\n",
            "loss: 0.000078  [98000/167876]\n",
            "loss: 0.000093  [100800/167876]\n",
            "loss: 0.000091  [103600/167876]\n",
            "loss: 0.000053  [106400/167876]\n",
            "loss: 0.000067  [109200/167876]\n",
            "loss: 0.000071  [112000/167876]\n",
            "loss: 0.000069  [114800/167876]\n",
            "loss: 0.000088  [117600/167876]\n",
            "loss: 0.000076  [120400/167876]\n",
            "loss: 0.000072  [123200/167876]\n",
            "loss: 0.000105  [126000/167876]\n",
            "loss: 0.000072  [128800/167876]\n",
            "loss: 0.000048  [131600/167876]\n",
            "loss: 0.000066  [134400/167876]\n",
            "loss: 0.000075  [137200/167876]\n",
            "loss: 0.000078  [140000/167876]\n",
            "loss: 0.000062  [142800/167876]\n",
            "loss: 0.000082  [145600/167876]\n",
            "loss: 0.000065  [148400/167876]\n",
            "loss: 0.000072  [151200/167876]\n",
            "loss: 0.000073  [154000/167876]\n",
            "loss: 0.000090  [156800/167876]\n",
            "loss: 0.000089  [159600/167876]\n",
            "loss: 0.000067  [162400/167876]\n",
            "loss: 0.000085  [165200/167876]\n",
            "delay time: avg MSE: 0.000097, avg abs error: 0.0063\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Finished training\n",
            "delay time: avg MSE: 0.000096, avg abs error: 0.0062\n",
            "Epoch 1\n",
            "loss: 0.093211  [  0/167876]\n",
            "loss: 0.326549  [2800/167876]\n",
            "loss: 0.330678  [5600/167876]\n",
            "loss: 0.299790  [8400/167876]\n",
            "loss: 0.287741  [11200/167876]\n",
            "loss: 0.099132  [14000/167876]\n",
            "loss: 0.043677  [16800/167876]\n",
            "loss: 0.036782  [19600/167876]\n",
            "loss: 0.036671  [22400/167876]\n",
            "loss: 0.044651  [25200/167876]\n",
            "loss: 0.028997  [28000/167876]\n",
            "loss: 0.032574  [30800/167876]\n",
            "loss: 0.028140  [33600/167876]\n",
            "loss: 0.023305  [36400/167876]\n",
            "loss: 0.019187  [39200/167876]\n",
            "loss: 0.020513  [42000/167876]\n",
            "loss: 0.026945  [44800/167876]\n",
            "loss: 0.020486  [47600/167876]\n",
            "loss: 0.029414  [50400/167876]\n",
            "loss: 0.025182  [53200/167876]\n",
            "loss: 0.017281  [56000/167876]\n",
            "loss: 0.015493  [58800/167876]\n",
            "loss: 0.017636  [61600/167876]\n",
            "loss: 0.016516  [64400/167876]\n",
            "loss: 0.013778  [67200/167876]\n",
            "loss: 0.013506  [70000/167876]\n",
            "loss: 0.021079  [72800/167876]\n",
            "loss: 0.016319  [75600/167876]\n",
            "loss: 0.018592  [78400/167876]\n",
            "loss: 0.019381  [81200/167876]\n",
            "loss: 0.018598  [84000/167876]\n",
            "loss: 0.015374  [86800/167876]\n",
            "loss: 0.008419  [89600/167876]\n",
            "loss: 0.015640  [92400/167876]\n",
            "loss: 0.013872  [95200/167876]\n",
            "loss: 0.020506  [98000/167876]\n",
            "loss: 0.016111  [100800/167876]\n",
            "loss: 0.010839  [103600/167876]\n",
            "loss: 0.012411  [106400/167876]\n",
            "loss: 0.008323  [109200/167876]\n",
            "loss: 0.009678  [112000/167876]\n",
            "loss: 0.011195  [114800/167876]\n",
            "loss: 0.013977  [117600/167876]\n",
            "loss: 0.013987  [120400/167876]\n",
            "loss: 0.012927  [123200/167876]\n",
            "loss: 0.012091  [126000/167876]\n",
            "loss: 0.011909  [128800/167876]\n",
            "loss: 0.012498  [131600/167876]\n",
            "loss: 0.009402  [134400/167876]\n",
            "loss: 0.009134  [137200/167876]\n",
            "loss: 0.009202  [140000/167876]\n",
            "loss: 0.013756  [142800/167876]\n",
            "loss: 0.009193  [145600/167876]\n",
            "loss: 0.010651  [148400/167876]\n",
            "loss: 0.011439  [151200/167876]\n",
            "loss: 0.006403  [154000/167876]\n",
            "loss: 0.010169  [156800/167876]\n",
            "loss: 0.012417  [159600/167876]\n",
            "loss: 0.014264  [162400/167876]\n",
            "loss: 0.008713  [165200/167876]\n",
            "reverb decay: avg MSE: 0.016617, avg abs error: 0.1056\n",
            "learning rate: 0.001000 -> 0.000910\n",
            "---------------------------\n",
            "\n",
            "Epoch 2\n",
            "loss: 0.019856  [  0/167876]\n",
            "loss: 0.013099  [2800/167876]\n",
            "loss: 0.008320  [5600/167876]\n",
            "loss: 0.010952  [8400/167876]\n",
            "loss: 0.011353  [11200/167876]\n",
            "loss: 0.009367  [14000/167876]\n",
            "loss: 0.009092  [16800/167876]\n",
            "loss: 0.010403  [19600/167876]\n",
            "loss: 0.009869  [22400/167876]\n",
            "loss: 0.010652  [25200/167876]\n",
            "loss: 0.004690  [28000/167876]\n",
            "loss: 0.009864  [30800/167876]\n",
            "loss: 0.007885  [33600/167876]\n",
            "loss: 0.010552  [36400/167876]\n",
            "loss: 0.008246  [39200/167876]\n",
            "loss: 0.007484  [42000/167876]\n",
            "loss: 0.013257  [44800/167876]\n",
            "loss: 0.006649  [47600/167876]\n",
            "loss: 0.010246  [50400/167876]\n",
            "loss: 0.007685  [53200/167876]\n",
            "loss: 0.009720  [56000/167876]\n",
            "loss: 0.013146  [58800/167876]\n",
            "loss: 0.005826  [61600/167876]\n",
            "loss: 0.011652  [64400/167876]\n",
            "loss: 0.010006  [67200/167876]\n",
            "loss: 0.006055  [70000/167876]\n",
            "loss: 0.006643  [72800/167876]\n",
            "loss: 0.007221  [75600/167876]\n",
            "loss: 0.008235  [78400/167876]\n",
            "loss: 0.011399  [81200/167876]\n",
            "loss: 0.003902  [84000/167876]\n",
            "loss: 0.006811  [86800/167876]\n",
            "loss: 0.006492  [89600/167876]\n",
            "loss: 0.007397  [92400/167876]\n",
            "loss: 0.007432  [95200/167876]\n",
            "loss: 0.006430  [98000/167876]\n",
            "loss: 0.007467  [100800/167876]\n",
            "loss: 0.009377  [103600/167876]\n",
            "loss: 0.007401  [106400/167876]\n",
            "loss: 0.008685  [109200/167876]\n",
            "loss: 0.007549  [112000/167876]\n",
            "loss: 0.008405  [114800/167876]\n",
            "loss: 0.007792  [117600/167876]\n",
            "loss: 0.011048  [120400/167876]\n",
            "loss: 0.005838  [123200/167876]\n",
            "loss: 0.015986  [126000/167876]\n",
            "loss: 0.010923  [128800/167876]\n",
            "loss: 0.006241  [131600/167876]\n",
            "loss: 0.007587  [134400/167876]\n",
            "loss: 0.007681  [137200/167876]\n",
            "loss: 0.006600  [140000/167876]\n",
            "loss: 0.006911  [142800/167876]\n",
            "loss: 0.007268  [145600/167876]\n",
            "loss: 0.012145  [148400/167876]\n",
            "loss: 0.007030  [151200/167876]\n",
            "loss: 0.008230  [154000/167876]\n",
            "loss: 0.005196  [156800/167876]\n",
            "loss: 0.007763  [159600/167876]\n",
            "loss: 0.005252  [162400/167876]\n",
            "loss: 0.005098  [165200/167876]\n",
            "reverb decay: avg MSE: 0.006013, avg abs error: 0.0557\n",
            "learning rate: 0.000910 -> 0.000820\n",
            "---------------------------\n",
            "\n",
            "Epoch 3\n",
            "loss: 0.006098  [  0/167876]\n",
            "loss: 0.006430  [2800/167876]\n",
            "loss: 0.008048  [5600/167876]\n",
            "loss: 0.005645  [8400/167876]\n",
            "loss: 0.005133  [11200/167876]\n",
            "loss: 0.007132  [14000/167876]\n",
            "loss: 0.007080  [16800/167876]\n",
            "loss: 0.007278  [19600/167876]\n",
            "loss: 0.007984  [22400/167876]\n",
            "loss: 0.009940  [25200/167876]\n",
            "loss: 0.004661  [28000/167876]\n",
            "loss: 0.005255  [30800/167876]\n",
            "loss: 0.006466  [33600/167876]\n",
            "loss: 0.006794  [36400/167876]\n",
            "loss: 0.006523  [39200/167876]\n",
            "loss: 0.007900  [42000/167876]\n",
            "loss: 0.006035  [44800/167876]\n",
            "loss: 0.007186  [47600/167876]\n",
            "loss: 0.006186  [50400/167876]\n",
            "loss: 0.004950  [53200/167876]\n",
            "loss: 0.005037  [56000/167876]\n",
            "loss: 0.006312  [58800/167876]\n",
            "loss: 0.004771  [61600/167876]\n",
            "loss: 0.004842  [64400/167876]\n",
            "loss: 0.007521  [67200/167876]\n",
            "loss: 0.006991  [70000/167876]\n",
            "loss: 0.006163  [72800/167876]\n",
            "loss: 0.006041  [75600/167876]\n",
            "loss: 0.005986  [78400/167876]\n",
            "loss: 0.007985  [81200/167876]\n",
            "loss: 0.007945  [84000/167876]\n",
            "loss: 0.007316  [86800/167876]\n",
            "loss: 0.004871  [89600/167876]\n",
            "loss: 0.007674  [92400/167876]\n",
            "loss: 0.005894  [95200/167876]\n",
            "loss: 0.008970  [98000/167876]\n",
            "loss: 0.008037  [100800/167876]\n",
            "loss: 0.008872  [103600/167876]\n",
            "loss: 0.006681  [106400/167876]\n",
            "loss: 0.004749  [109200/167876]\n",
            "loss: 0.008803  [112000/167876]\n",
            "loss: 0.004770  [114800/167876]\n",
            "loss: 0.006963  [117600/167876]\n",
            "loss: 0.004542  [120400/167876]\n",
            "loss: 0.006537  [123200/167876]\n",
            "loss: 0.007201  [126000/167876]\n",
            "loss: 0.004637  [128800/167876]\n",
            "loss: 0.005567  [131600/167876]\n",
            "loss: 0.004624  [134400/167876]\n",
            "loss: 0.003961  [137200/167876]\n",
            "loss: 0.005268  [140000/167876]\n",
            "loss: 0.005301  [142800/167876]\n",
            "loss: 0.006512  [145600/167876]\n",
            "loss: 0.004280  [148400/167876]\n",
            "loss: 0.007140  [151200/167876]\n",
            "loss: 0.007826  [154000/167876]\n",
            "loss: 0.004443  [156800/167876]\n",
            "loss: 0.004928  [159600/167876]\n",
            "loss: 0.006829  [162400/167876]\n",
            "loss: 0.010211  [165200/167876]\n",
            "reverb decay: avg MSE: 0.005915, avg abs error: 0.0574\n",
            "learning rate: 0.000820 -> 0.000730\n",
            "---------------------------\n",
            "\n",
            "Epoch 4\n",
            "loss: 0.004757  [  0/167876]\n",
            "loss: 0.003945  [2800/167876]\n",
            "loss: 0.004329  [5600/167876]\n",
            "loss: 0.007489  [8400/167876]\n",
            "loss: 0.003757  [11200/167876]\n",
            "loss: 0.004264  [14000/167876]\n",
            "loss: 0.004580  [16800/167876]\n",
            "loss: 0.005440  [19600/167876]\n",
            "loss: 0.005868  [22400/167876]\n",
            "loss: 0.004711  [25200/167876]\n",
            "loss: 0.005179  [28000/167876]\n",
            "loss: 0.003557  [30800/167876]\n",
            "loss: 0.004134  [33600/167876]\n",
            "loss: 0.006445  [36400/167876]\n",
            "loss: 0.006544  [39200/167876]\n",
            "loss: 0.004094  [42000/167876]\n",
            "loss: 0.005155  [44800/167876]\n",
            "loss: 0.006162  [47600/167876]\n",
            "loss: 0.004329  [50400/167876]\n",
            "loss: 0.005251  [53200/167876]\n",
            "loss: 0.006832  [56000/167876]\n",
            "loss: 0.004456  [58800/167876]\n",
            "loss: 0.012105  [61600/167876]\n",
            "loss: 0.005137  [64400/167876]\n",
            "loss: 0.004888  [67200/167876]\n",
            "loss: 0.005947  [70000/167876]\n",
            "loss: 0.006865  [72800/167876]\n",
            "loss: 0.004032  [75600/167876]\n",
            "loss: 0.005010  [78400/167876]\n",
            "loss: 0.005010  [81200/167876]\n",
            "loss: 0.004500  [84000/167876]\n",
            "loss: 0.004087  [86800/167876]\n",
            "loss: 0.004588  [89600/167876]\n",
            "loss: 0.004350  [92400/167876]\n",
            "loss: 0.006266  [95200/167876]\n",
            "loss: 0.004573  [98000/167876]\n",
            "loss: 0.004788  [100800/167876]\n",
            "loss: 0.004816  [103600/167876]\n",
            "loss: 0.006152  [106400/167876]\n",
            "loss: 0.007961  [109200/167876]\n",
            "loss: 0.005014  [112000/167876]\n",
            "loss: 0.008676  [114800/167876]\n",
            "loss: 0.006270  [117600/167876]\n",
            "loss: 0.005490  [120400/167876]\n",
            "loss: 0.005665  [123200/167876]\n",
            "loss: 0.004838  [126000/167876]\n",
            "loss: 0.006961  [128800/167876]\n",
            "loss: 0.011078  [131600/167876]\n",
            "loss: 0.004955  [134400/167876]\n",
            "loss: 0.004212  [137200/167876]\n",
            "loss: 0.003387  [140000/167876]\n",
            "loss: 0.005261  [142800/167876]\n",
            "loss: 0.003793  [145600/167876]\n",
            "loss: 0.005754  [148400/167876]\n",
            "loss: 0.007539  [151200/167876]\n",
            "loss: 0.005808  [154000/167876]\n",
            "loss: 0.004025  [156800/167876]\n",
            "loss: 0.006722  [159600/167876]\n",
            "loss: 0.005302  [162400/167876]\n",
            "loss: 0.004570  [165200/167876]\n",
            "reverb decay: avg MSE: 0.006053, avg abs error: 0.0552\n",
            "learning rate: 0.000730 -> 0.000640\n",
            "---------------------------\n",
            "\n",
            "Epoch 5\n",
            "loss: 0.004777  [  0/167876]\n",
            "loss: 0.004474  [2800/167876]\n",
            "loss: 0.006587  [5600/167876]\n",
            "loss: 0.007558  [8400/167876]\n",
            "loss: 0.006422  [11200/167876]\n",
            "loss: 0.005301  [14000/167876]\n",
            "loss: 0.004719  [16800/167876]\n",
            "loss: 0.004521  [19600/167876]\n",
            "loss: 0.004110  [22400/167876]\n",
            "loss: 0.003147  [25200/167876]\n",
            "loss: 0.004650  [28000/167876]\n",
            "loss: 0.004062  [30800/167876]\n",
            "loss: 0.003604  [33600/167876]\n",
            "loss: 0.003739  [36400/167876]\n",
            "loss: 0.006581  [39200/167876]\n",
            "loss: 0.004172  [42000/167876]\n",
            "loss: 0.004302  [44800/167876]\n",
            "loss: 0.003854  [47600/167876]\n",
            "loss: 0.004642  [50400/167876]\n",
            "loss: 0.005002  [53200/167876]\n",
            "loss: 0.004842  [56000/167876]\n",
            "loss: 0.004836  [58800/167876]\n",
            "loss: 0.004406  [61600/167876]\n",
            "loss: 0.003763  [64400/167876]\n",
            "loss: 0.003478  [67200/167876]\n",
            "loss: 0.003992  [70000/167876]\n",
            "loss: 0.006221  [72800/167876]\n",
            "loss: 0.003233  [75600/167876]\n",
            "loss: 0.005388  [78400/167876]\n",
            "loss: 0.003945  [81200/167876]\n",
            "loss: 0.004811  [84000/167876]\n",
            "loss: 0.005605  [86800/167876]\n",
            "loss: 0.006416  [89600/167876]\n",
            "loss: 0.003459  [92400/167876]\n",
            "loss: 0.003133  [95200/167876]\n",
            "loss: 0.002530  [98000/167876]\n",
            "loss: 0.002637  [100800/167876]\n",
            "loss: 0.004711  [103600/167876]\n",
            "loss: 0.002845  [106400/167876]\n",
            "loss: 0.005347  [109200/167876]\n",
            "loss: 0.004141  [112000/167876]\n",
            "loss: 0.004491  [114800/167876]\n",
            "loss: 0.003109  [117600/167876]\n",
            "loss: 0.004616  [120400/167876]\n",
            "loss: 0.003577  [123200/167876]\n",
            "loss: 0.005901  [126000/167876]\n",
            "loss: 0.003574  [128800/167876]\n",
            "loss: 0.004564  [131600/167876]\n",
            "loss: 0.003556  [134400/167876]\n",
            "loss: 0.007040  [137200/167876]\n",
            "loss: 0.003919  [140000/167876]\n",
            "loss: 0.004884  [142800/167876]\n",
            "loss: 0.004530  [145600/167876]\n",
            "loss: 0.006983  [148400/167876]\n",
            "loss: 0.003176  [151200/167876]\n",
            "loss: 0.004118  [154000/167876]\n",
            "loss: 0.003993  [156800/167876]\n",
            "loss: 0.003418  [159600/167876]\n",
            "loss: 0.006063  [162400/167876]\n",
            "loss: 0.004523  [165200/167876]\n",
            "reverb decay: avg MSE: 0.004390, avg abs error: 0.0456\n",
            "learning rate: 0.000640 -> 0.000550\n",
            "---------------------------\n",
            "\n",
            "Epoch 6\n",
            "loss: 0.004028  [  0/167876]\n",
            "loss: 0.006507  [2800/167876]\n",
            "loss: 0.004117  [5600/167876]\n",
            "loss: 0.003574  [8400/167876]\n",
            "loss: 0.003144  [11200/167876]\n",
            "loss: 0.002617  [14000/167876]\n",
            "loss: 0.003310  [16800/167876]\n",
            "loss: 0.003293  [19600/167876]\n",
            "loss: 0.004083  [22400/167876]\n",
            "loss: 0.006074  [25200/167876]\n",
            "loss: 0.003493  [28000/167876]\n",
            "loss: 0.002880  [30800/167876]\n",
            "loss: 0.004572  [33600/167876]\n",
            "loss: 0.003775  [36400/167876]\n",
            "loss: 0.003320  [39200/167876]\n",
            "loss: 0.003511  [42000/167876]\n",
            "loss: 0.004064  [44800/167876]\n",
            "loss: 0.003636  [47600/167876]\n",
            "loss: 0.002946  [50400/167876]\n",
            "loss: 0.003844  [53200/167876]\n",
            "loss: 0.003613  [56000/167876]\n",
            "loss: 0.003671  [58800/167876]\n",
            "loss: 0.004630  [61600/167876]\n",
            "loss: 0.002757  [64400/167876]\n",
            "loss: 0.005240  [67200/167876]\n",
            "loss: 0.003182  [70000/167876]\n",
            "loss: 0.003979  [72800/167876]\n",
            "loss: 0.004864  [75600/167876]\n",
            "loss: 0.003567  [78400/167876]\n",
            "loss: 0.003589  [81200/167876]\n",
            "loss: 0.004940  [84000/167876]\n",
            "loss: 0.003527  [86800/167876]\n",
            "loss: 0.004181  [89600/167876]\n",
            "loss: 0.005424  [92400/167876]\n",
            "loss: 0.004779  [95200/167876]\n",
            "loss: 0.002748  [98000/167876]\n",
            "loss: 0.002396  [100800/167876]\n",
            "loss: 0.005100  [103600/167876]\n",
            "loss: 0.004819  [106400/167876]\n",
            "loss: 0.004225  [109200/167876]\n",
            "loss: 0.004371  [112000/167876]\n",
            "loss: 0.004671  [114800/167876]\n",
            "loss: 0.004579  [117600/167876]\n",
            "loss: 0.004752  [120400/167876]\n",
            "loss: 0.004480  [123200/167876]\n",
            "loss: 0.002225  [126000/167876]\n",
            "loss: 0.004053  [128800/167876]\n",
            "loss: 0.004310  [131600/167876]\n",
            "loss: 0.004191  [134400/167876]\n",
            "loss: 0.003424  [137200/167876]\n",
            "loss: 0.002449  [140000/167876]\n",
            "loss: 0.003998  [142800/167876]\n",
            "loss: 0.004712  [145600/167876]\n",
            "loss: 0.003430  [148400/167876]\n",
            "loss: 0.005739  [151200/167876]\n",
            "loss: 0.003857  [154000/167876]\n",
            "loss: 0.003857  [156800/167876]\n",
            "loss: 0.005744  [159600/167876]\n",
            "loss: 0.004209  [162400/167876]\n",
            "loss: 0.005323  [165200/167876]\n",
            "reverb decay: avg MSE: 0.004542, avg abs error: 0.0484\n",
            "learning rate: 0.000550 -> 0.000460\n",
            "---------------------------\n",
            "\n",
            "Epoch 7\n",
            "loss: 0.003131  [  0/167876]\n",
            "loss: 0.004737  [2800/167876]\n",
            "loss: 0.003204  [5600/167876]\n",
            "loss: 0.002472  [8400/167876]\n",
            "loss: 0.002790  [11200/167876]\n",
            "loss: 0.004037  [14000/167876]\n",
            "loss: 0.003616  [16800/167876]\n",
            "loss: 0.003427  [19600/167876]\n",
            "loss: 0.003180  [22400/167876]\n",
            "loss: 0.004185  [25200/167876]\n",
            "loss: 0.002842  [28000/167876]\n",
            "loss: 0.004900  [30800/167876]\n",
            "loss: 0.005338  [33600/167876]\n",
            "loss: 0.003700  [36400/167876]\n",
            "loss: 0.004899  [39200/167876]\n",
            "loss: 0.002235  [42000/167876]\n",
            "loss: 0.003713  [44800/167876]\n",
            "loss: 0.002909  [47600/167876]\n",
            "loss: 0.003598  [50400/167876]\n",
            "loss: 0.003868  [53200/167876]\n",
            "loss: 0.003930  [56000/167876]\n",
            "loss: 0.004011  [58800/167876]\n",
            "loss: 0.003073  [61600/167876]\n",
            "loss: 0.002338  [64400/167876]\n",
            "loss: 0.003232  [67200/167876]\n",
            "loss: 0.001926  [70000/167876]\n",
            "loss: 0.002693  [72800/167876]\n",
            "loss: 0.003307  [75600/167876]\n",
            "loss: 0.002715  [78400/167876]\n",
            "loss: 0.004798  [81200/167876]\n",
            "loss: 0.002293  [84000/167876]\n",
            "loss: 0.003620  [86800/167876]\n",
            "loss: 0.003269  [89600/167876]\n",
            "loss: 0.005027  [92400/167876]\n",
            "loss: 0.002751  [95200/167876]\n",
            "loss: 0.003387  [98000/167876]\n",
            "loss: 0.003434  [100800/167876]\n",
            "loss: 0.003292  [103600/167876]\n",
            "loss: 0.003732  [106400/167876]\n",
            "loss: 0.005924  [109200/167876]\n",
            "loss: 0.003920  [112000/167876]\n",
            "loss: 0.003111  [114800/167876]\n",
            "loss: 0.002994  [117600/167876]\n",
            "loss: 0.003765  [120400/167876]\n",
            "loss: 0.003666  [123200/167876]\n",
            "loss: 0.003129  [126000/167876]\n",
            "loss: 0.003043  [128800/167876]\n",
            "loss: 0.004359  [131600/167876]\n",
            "loss: 0.002993  [134400/167876]\n",
            "loss: 0.003882  [137200/167876]\n",
            "loss: 0.003285  [140000/167876]\n",
            "loss: 0.003571  [142800/167876]\n",
            "loss: 0.003160  [145600/167876]\n",
            "loss: 0.002934  [148400/167876]\n",
            "loss: 0.004049  [151200/167876]\n",
            "loss: 0.003506  [154000/167876]\n",
            "loss: 0.003781  [156800/167876]\n",
            "loss: 0.002463  [159600/167876]\n",
            "loss: 0.003464  [162400/167876]\n",
            "loss: 0.004451  [165200/167876]\n",
            "reverb decay: avg MSE: 0.003860, avg abs error: 0.0426\n",
            "learning rate: 0.000460 -> 0.000370\n",
            "---------------------------\n",
            "\n",
            "Epoch 8\n",
            "loss: 0.003362  [  0/167876]\n",
            "loss: 0.003512  [2800/167876]\n",
            "loss: 0.002077  [5600/167876]\n",
            "loss: 0.002452  [8400/167876]\n",
            "loss: 0.003965  [11200/167876]\n",
            "loss: 0.002399  [14000/167876]\n",
            "loss: 0.004046  [16800/167876]\n",
            "loss: 0.002780  [19600/167876]\n",
            "loss: 0.002324  [22400/167876]\n",
            "loss: 0.003352  [25200/167876]\n",
            "loss: 0.003393  [28000/167876]\n",
            "loss: 0.002973  [30800/167876]\n",
            "loss: 0.002679  [33600/167876]\n",
            "loss: 0.002665  [36400/167876]\n",
            "loss: 0.003895  [39200/167876]\n",
            "loss: 0.002432  [42000/167876]\n",
            "loss: 0.003192  [44800/167876]\n",
            "loss: 0.003170  [47600/167876]\n",
            "loss: 0.003433  [50400/167876]\n",
            "loss: 0.002827  [53200/167876]\n",
            "loss: 0.002812  [56000/167876]\n",
            "loss: 0.002887  [58800/167876]\n",
            "loss: 0.002967  [61600/167876]\n",
            "loss: 0.004177  [64400/167876]\n",
            "loss: 0.003191  [67200/167876]\n",
            "loss: 0.003409  [70000/167876]\n",
            "loss: 0.002636  [72800/167876]\n",
            "loss: 0.003388  [75600/167876]\n",
            "loss: 0.002679  [78400/167876]\n",
            "loss: 0.003621  [81200/167876]\n",
            "loss: 0.003041  [84000/167876]\n",
            "loss: 0.002618  [86800/167876]\n",
            "loss: 0.002309  [89600/167876]\n",
            "loss: 0.004088  [92400/167876]\n",
            "loss: 0.003380  [95200/167876]\n",
            "loss: 0.003622  [98000/167876]\n",
            "loss: 0.003268  [100800/167876]\n",
            "loss: 0.003615  [103600/167876]\n",
            "loss: 0.004056  [106400/167876]\n",
            "loss: 0.003879  [109200/167876]\n",
            "loss: 0.002863  [112000/167876]\n",
            "loss: 0.002800  [114800/167876]\n",
            "loss: 0.002979  [117600/167876]\n",
            "loss: 0.002528  [120400/167876]\n",
            "loss: 0.002329  [123200/167876]\n",
            "loss: 0.002430  [126000/167876]\n",
            "loss: 0.002879  [128800/167876]\n",
            "loss: 0.002456  [131600/167876]\n",
            "loss: 0.003278  [134400/167876]\n",
            "loss: 0.002521  [137200/167876]\n",
            "loss: 0.002196  [140000/167876]\n",
            "loss: 0.004402  [142800/167876]\n",
            "loss: 0.002226  [145600/167876]\n",
            "loss: 0.001881  [148400/167876]\n",
            "loss: 0.003031  [151200/167876]\n",
            "loss: 0.005953  [154000/167876]\n",
            "loss: 0.002771  [156800/167876]\n",
            "loss: 0.002649  [159600/167876]\n",
            "loss: 0.001599  [162400/167876]\n",
            "loss: 0.004073  [165200/167876]\n",
            "reverb decay: avg MSE: 0.003648, avg abs error: 0.0428\n",
            "learning rate: 0.000370 -> 0.000280\n",
            "---------------------------\n",
            "\n",
            "Epoch 9\n",
            "loss: 0.004227  [  0/167876]\n",
            "loss: 0.002212  [2800/167876]\n",
            "loss: 0.003295  [5600/167876]\n",
            "loss: 0.003608  [8400/167876]\n",
            "loss: 0.001886  [11200/167876]\n",
            "loss: 0.004204  [14000/167876]\n",
            "loss: 0.002075  [16800/167876]\n",
            "loss: 0.003831  [19600/167876]\n",
            "loss: 0.001516  [22400/167876]\n",
            "loss: 0.002343  [25200/167876]\n",
            "loss: 0.004571  [28000/167876]\n",
            "loss: 0.002406  [30800/167876]\n",
            "loss: 0.002789  [33600/167876]\n",
            "loss: 0.002420  [36400/167876]\n",
            "loss: 0.002370  [39200/167876]\n",
            "loss: 0.003226  [42000/167876]\n",
            "loss: 0.003518  [44800/167876]\n",
            "loss: 0.002446  [47600/167876]\n",
            "loss: 0.002175  [50400/167876]\n",
            "loss: 0.002160  [53200/167876]\n",
            "loss: 0.002575  [56000/167876]\n",
            "loss: 0.002780  [58800/167876]\n",
            "loss: 0.002439  [61600/167876]\n",
            "loss: 0.003078  [64400/167876]\n",
            "loss: 0.002404  [67200/167876]\n",
            "loss: 0.003679  [70000/167876]\n",
            "loss: 0.002418  [72800/167876]\n",
            "loss: 0.001853  [75600/167876]\n",
            "loss: 0.002263  [78400/167876]\n",
            "loss: 0.002691  [81200/167876]\n",
            "loss: 0.002529  [84000/167876]\n",
            "loss: 0.002461  [86800/167876]\n",
            "loss: 0.003409  [89600/167876]\n",
            "loss: 0.002531  [92400/167876]\n",
            "loss: 0.002054  [95200/167876]\n",
            "loss: 0.002820  [98000/167876]\n",
            "loss: 0.002702  [100800/167876]\n",
            "loss: 0.002998  [103600/167876]\n",
            "loss: 0.002872  [106400/167876]\n",
            "loss: 0.001649  [109200/167876]\n",
            "loss: 0.002856  [112000/167876]\n",
            "loss: 0.003764  [114800/167876]\n",
            "loss: 0.002373  [117600/167876]\n",
            "loss: 0.002300  [120400/167876]\n",
            "loss: 0.002278  [123200/167876]\n",
            "loss: 0.002427  [126000/167876]\n",
            "loss: 0.002920  [128800/167876]\n",
            "loss: 0.002870  [131600/167876]\n",
            "loss: 0.002583  [134400/167876]\n",
            "loss: 0.001869  [137200/167876]\n",
            "loss: 0.002335  [140000/167876]\n",
            "loss: 0.002200  [142800/167876]\n",
            "loss: 0.002712  [145600/167876]\n",
            "loss: 0.002625  [148400/167876]\n",
            "loss: 0.002619  [151200/167876]\n",
            "loss: 0.002658  [154000/167876]\n",
            "loss: 0.005520  [156800/167876]\n",
            "loss: 0.001554  [159600/167876]\n",
            "loss: 0.001905  [162400/167876]\n",
            "loss: 0.002340  [165200/167876]\n",
            "reverb decay: avg MSE: 0.003323, avg abs error: 0.0396\n",
            "learning rate: 0.000280 -> 0.000190\n",
            "---------------------------\n",
            "\n",
            "Epoch 10\n",
            "loss: 0.002356  [  0/167876]\n",
            "loss: 0.002120  [2800/167876]\n",
            "loss: 0.003073  [5600/167876]\n",
            "loss: 0.002285  [8400/167876]\n",
            "loss: 0.002158  [11200/167876]\n",
            "loss: 0.002232  [14000/167876]\n",
            "loss: 0.001521  [16800/167876]\n",
            "loss: 0.001637  [19600/167876]\n",
            "loss: 0.001877  [22400/167876]\n",
            "loss: 0.002989  [25200/167876]\n",
            "loss: 0.002823  [28000/167876]\n",
            "loss: 0.002233  [30800/167876]\n",
            "loss: 0.001826  [33600/167876]\n",
            "loss: 0.003311  [36400/167876]\n",
            "loss: 0.002311  [39200/167876]\n",
            "loss: 0.002969  [42000/167876]\n",
            "loss: 0.002605  [44800/167876]\n",
            "loss: 0.002282  [47600/167876]\n",
            "loss: 0.003059  [50400/167876]\n",
            "loss: 0.002711  [53200/167876]\n",
            "loss: 0.003102  [56000/167876]\n",
            "loss: 0.001777  [58800/167876]\n",
            "loss: 0.003174  [61600/167876]\n",
            "loss: 0.002860  [64400/167876]\n",
            "loss: 0.002933  [67200/167876]\n",
            "loss: 0.003477  [70000/167876]\n",
            "loss: 0.002022  [72800/167876]\n",
            "loss: 0.003077  [75600/167876]\n",
            "loss: 0.002171  [78400/167876]\n",
            "loss: 0.003263  [81200/167876]\n",
            "loss: 0.002081  [84000/167876]\n",
            "loss: 0.002564  [86800/167876]\n",
            "loss: 0.002906  [89600/167876]\n",
            "loss: 0.001757  [92400/167876]\n",
            "loss: 0.002684  [95200/167876]\n",
            "loss: 0.003656  [98000/167876]\n",
            "loss: 0.001907  [100800/167876]\n",
            "loss: 0.001951  [103600/167876]\n",
            "loss: 0.002790  [106400/167876]\n",
            "loss: 0.003063  [109200/167876]\n",
            "loss: 0.002400  [112000/167876]\n",
            "loss: 0.002599  [114800/167876]\n",
            "loss: 0.001885  [117600/167876]\n",
            "loss: 0.003142  [120400/167876]\n",
            "loss: 0.002288  [123200/167876]\n",
            "loss: 0.002503  [126000/167876]\n",
            "loss: 0.002183  [128800/167876]\n",
            "loss: 0.001979  [131600/167876]\n",
            "loss: 0.002180  [134400/167876]\n",
            "loss: 0.002936  [137200/167876]\n",
            "loss: 0.002680  [140000/167876]\n",
            "loss: 0.002068  [142800/167876]\n",
            "loss: 0.002383  [145600/167876]\n",
            "loss: 0.003283  [148400/167876]\n",
            "loss: 0.002222  [151200/167876]\n",
            "loss: 0.002455  [154000/167876]\n",
            "loss: 0.002627  [156800/167876]\n",
            "loss: 0.002839  [159600/167876]\n",
            "loss: 0.002058  [162400/167876]\n",
            "loss: 0.002925  [165200/167876]\n",
            "reverb decay: avg MSE: 0.003059, avg abs error: 0.0381\n",
            "learning rate: 0.000190 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 11\n",
            "loss: 0.002109  [  0/167876]\n",
            "loss: 0.002445  [2800/167876]\n",
            "loss: 0.001750  [5600/167876]\n",
            "loss: 0.002762  [8400/167876]\n",
            "loss: 0.001772  [11200/167876]\n",
            "loss: 0.002468  [14000/167876]\n",
            "loss: 0.002547  [16800/167876]\n",
            "loss: 0.002678  [19600/167876]\n",
            "loss: 0.002220  [22400/167876]\n",
            "loss: 0.001890  [25200/167876]\n",
            "loss: 0.002237  [28000/167876]\n",
            "loss: 0.001732  [30800/167876]\n",
            "loss: 0.002175  [33600/167876]\n",
            "loss: 0.002535  [36400/167876]\n",
            "loss: 0.002778  [39200/167876]\n",
            "loss: 0.001869  [42000/167876]\n",
            "loss: 0.001730  [44800/167876]\n",
            "loss: 0.001896  [47600/167876]\n",
            "loss: 0.001959  [50400/167876]\n",
            "loss: 0.002167  [53200/167876]\n",
            "loss: 0.002178  [56000/167876]\n",
            "loss: 0.001963  [58800/167876]\n",
            "loss: 0.002268  [61600/167876]\n",
            "loss: 0.002750  [64400/167876]\n",
            "loss: 0.002169  [67200/167876]\n",
            "loss: 0.001958  [70000/167876]\n",
            "loss: 0.001582  [72800/167876]\n",
            "loss: 0.001798  [75600/167876]\n",
            "loss: 0.001936  [78400/167876]\n",
            "loss: 0.002432  [81200/167876]\n",
            "loss: 0.002479  [84000/167876]\n",
            "loss: 0.001280  [86800/167876]\n",
            "loss: 0.001454  [89600/167876]\n",
            "loss: 0.001665  [92400/167876]\n",
            "loss: 0.001871  [95200/167876]\n",
            "loss: 0.002591  [98000/167876]\n",
            "loss: 0.002028  [100800/167876]\n",
            "loss: 0.002309  [103600/167876]\n",
            "loss: 0.001532  [106400/167876]\n",
            "loss: 0.001789  [109200/167876]\n",
            "loss: 0.002283  [112000/167876]\n",
            "loss: 0.002501  [114800/167876]\n",
            "loss: 0.002005  [117600/167876]\n",
            "loss: 0.002196  [120400/167876]\n",
            "loss: 0.002602  [123200/167876]\n",
            "loss: 0.001824  [126000/167876]\n",
            "loss: 0.002146  [128800/167876]\n",
            "loss: 0.002742  [131600/167876]\n",
            "loss: 0.003607  [134400/167876]\n",
            "loss: 0.002251  [137200/167876]\n",
            "loss: 0.001594  [140000/167876]\n",
            "loss: 0.001886  [142800/167876]\n",
            "loss: 0.002171  [145600/167876]\n",
            "loss: 0.001700  [148400/167876]\n",
            "loss: 0.002649  [151200/167876]\n",
            "loss: 0.001898  [154000/167876]\n",
            "loss: 0.002062  [156800/167876]\n",
            "loss: 0.002200  [159600/167876]\n",
            "loss: 0.002567  [162400/167876]\n",
            "loss: 0.003373  [165200/167876]\n",
            "reverb decay: avg MSE: 0.002987, avg abs error: 0.0372\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 12\n",
            "loss: 0.001242  [  0/167876]\n",
            "loss: 0.002178  [2800/167876]\n",
            "loss: 0.001751  [5600/167876]\n",
            "loss: 0.001828  [8400/167876]\n",
            "loss: 0.001948  [11200/167876]\n",
            "loss: 0.001911  [14000/167876]\n",
            "loss: 0.002172  [16800/167876]\n",
            "loss: 0.001580  [19600/167876]\n",
            "loss: 0.001457  [22400/167876]\n",
            "loss: 0.002265  [25200/167876]\n",
            "loss: 0.001757  [28000/167876]\n",
            "loss: 0.002108  [30800/167876]\n",
            "loss: 0.002623  [33600/167876]\n",
            "loss: 0.002304  [36400/167876]\n",
            "loss: 0.001988  [39200/167876]\n",
            "loss: 0.002224  [42000/167876]\n",
            "loss: 0.002373  [44800/167876]\n",
            "loss: 0.002235  [47600/167876]\n",
            "loss: 0.001525  [50400/167876]\n",
            "loss: 0.002237  [53200/167876]\n",
            "loss: 0.002267  [56000/167876]\n",
            "loss: 0.001634  [58800/167876]\n",
            "loss: 0.001917  [61600/167876]\n",
            "loss: 0.001793  [64400/167876]\n",
            "loss: 0.002174  [67200/167876]\n",
            "loss: 0.001950  [70000/167876]\n",
            "loss: 0.001944  [72800/167876]\n",
            "loss: 0.002203  [75600/167876]\n",
            "loss: 0.001786  [78400/167876]\n",
            "loss: 0.001888  [81200/167876]\n",
            "loss: 0.002195  [84000/167876]\n",
            "loss: 0.002170  [86800/167876]\n",
            "loss: 0.002323  [89600/167876]\n",
            "loss: 0.002036  [92400/167876]\n",
            "loss: 0.002438  [95200/167876]\n",
            "loss: 0.002285  [98000/167876]\n",
            "loss: 0.003167  [100800/167876]\n",
            "loss: 0.002036  [103600/167876]\n",
            "loss: 0.001796  [106400/167876]\n",
            "loss: 0.002651  [109200/167876]\n",
            "loss: 0.001377  [112000/167876]\n",
            "loss: 0.002716  [114800/167876]\n",
            "loss: 0.002053  [117600/167876]\n",
            "loss: 0.001847  [120400/167876]\n",
            "loss: 0.002292  [123200/167876]\n",
            "loss: 0.001982  [126000/167876]\n",
            "loss: 0.002051  [128800/167876]\n",
            "loss: 0.003180  [131600/167876]\n",
            "loss: 0.003195  [134400/167876]\n",
            "loss: 0.002476  [137200/167876]\n",
            "loss: 0.001313  [140000/167876]\n",
            "loss: 0.002184  [142800/167876]\n",
            "loss: 0.002479  [145600/167876]\n",
            "loss: 0.003883  [148400/167876]\n",
            "loss: 0.001923  [151200/167876]\n",
            "loss: 0.002417  [154000/167876]\n",
            "loss: 0.002429  [156800/167876]\n",
            "loss: 0.001866  [159600/167876]\n",
            "loss: 0.002353  [162400/167876]\n",
            "loss: 0.001709  [165200/167876]\n",
            "reverb decay: avg MSE: 0.002938, avg abs error: 0.0369\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 13\n",
            "loss: 0.001309  [  0/167876]\n",
            "loss: 0.002513  [2800/167876]\n",
            "loss: 0.002513  [5600/167876]\n",
            "loss: 0.002306  [8400/167876]\n",
            "loss: 0.002048  [11200/167876]\n",
            "loss: 0.001910  [14000/167876]\n",
            "loss: 0.002263  [16800/167876]\n",
            "loss: 0.001793  [19600/167876]\n",
            "loss: 0.001213  [22400/167876]\n",
            "loss: 0.002093  [25200/167876]\n",
            "loss: 0.003294  [28000/167876]\n",
            "loss: 0.002883  [30800/167876]\n",
            "loss: 0.002402  [33600/167876]\n",
            "loss: 0.002177  [36400/167876]\n",
            "loss: 0.002739  [39200/167876]\n",
            "loss: 0.001637  [42000/167876]\n",
            "loss: 0.002582  [44800/167876]\n",
            "loss: 0.002195  [47600/167876]\n",
            "loss: 0.001739  [50400/167876]\n",
            "loss: 0.002111  [53200/167876]\n",
            "loss: 0.002275  [56000/167876]\n",
            "loss: 0.001899  [58800/167876]\n",
            "loss: 0.001989  [61600/167876]\n",
            "loss: 0.001658  [64400/167876]\n",
            "loss: 0.002387  [67200/167876]\n",
            "loss: 0.002752  [70000/167876]\n",
            "loss: 0.001506  [72800/167876]\n",
            "loss: 0.001364  [75600/167876]\n",
            "loss: 0.001686  [78400/167876]\n",
            "loss: 0.002651  [81200/167876]\n",
            "loss: 0.002245  [84000/167876]\n",
            "loss: 0.002119  [86800/167876]\n",
            "loss: 0.002188  [89600/167876]\n",
            "loss: 0.002162  [92400/167876]\n",
            "loss: 0.002217  [95200/167876]\n",
            "loss: 0.001758  [98000/167876]\n",
            "loss: 0.001863  [100800/167876]\n",
            "loss: 0.002040  [103600/167876]\n",
            "loss: 0.002780  [106400/167876]\n",
            "loss: 0.002655  [109200/167876]\n",
            "loss: 0.001878  [112000/167876]\n",
            "loss: 0.002304  [114800/167876]\n",
            "loss: 0.001938  [117600/167876]\n",
            "loss: 0.002151  [120400/167876]\n",
            "loss: 0.001821  [123200/167876]\n",
            "loss: 0.001968  [126000/167876]\n",
            "loss: 0.001847  [128800/167876]\n",
            "loss: 0.001596  [131600/167876]\n",
            "loss: 0.001954  [134400/167876]\n",
            "loss: 0.001586  [137200/167876]\n",
            "loss: 0.002744  [140000/167876]\n",
            "loss: 0.002648  [142800/167876]\n",
            "loss: 0.002086  [145600/167876]\n",
            "loss: 0.002564  [148400/167876]\n",
            "loss: 0.001393  [151200/167876]\n",
            "loss: 0.002434  [154000/167876]\n",
            "loss: 0.001796  [156800/167876]\n",
            "loss: 0.002186  [159600/167876]\n",
            "loss: 0.001806  [162400/167876]\n",
            "loss: 0.001438  [165200/167876]\n",
            "reverb decay: avg MSE: 0.002995, avg abs error: 0.037\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 14\n",
            "loss: 0.001696  [  0/167876]\n",
            "loss: 0.001738  [2800/167876]\n",
            "loss: 0.001967  [5600/167876]\n",
            "loss: 0.002157  [8400/167876]\n",
            "loss: 0.001421  [11200/167876]\n",
            "loss: 0.001889  [14000/167876]\n",
            "loss: 0.002056  [16800/167876]\n",
            "loss: 0.002705  [19600/167876]\n",
            "loss: 0.001817  [22400/167876]\n",
            "loss: 0.001893  [25200/167876]\n",
            "loss: 0.002063  [28000/167876]\n",
            "loss: 0.002009  [30800/167876]\n",
            "loss: 0.002239  [33600/167876]\n",
            "loss: 0.002117  [36400/167876]\n",
            "loss: 0.001571  [39200/167876]\n",
            "loss: 0.001759  [42000/167876]\n",
            "loss: 0.002051  [44800/167876]\n",
            "loss: 0.002136  [47600/167876]\n",
            "loss: 0.001581  [50400/167876]\n",
            "loss: 0.002301  [53200/167876]\n",
            "loss: 0.001857  [56000/167876]\n",
            "loss: 0.001857  [58800/167876]\n",
            "loss: 0.001977  [61600/167876]\n",
            "loss: 0.002162  [64400/167876]\n",
            "loss: 0.001293  [67200/167876]\n",
            "loss: 0.001860  [70000/167876]\n",
            "loss: 0.001826  [72800/167876]\n",
            "loss: 0.001722  [75600/167876]\n",
            "loss: 0.001866  [78400/167876]\n",
            "loss: 0.002134  [81200/167876]\n",
            "loss: 0.002490  [84000/167876]\n",
            "loss: 0.001921  [86800/167876]\n",
            "loss: 0.002608  [89600/167876]\n",
            "loss: 0.001669  [92400/167876]\n",
            "loss: 0.001790  [95200/167876]\n",
            "loss: 0.002199  [98000/167876]\n",
            "loss: 0.001810  [100800/167876]\n",
            "loss: 0.001729  [103600/167876]\n",
            "loss: 0.001667  [106400/167876]\n",
            "loss: 0.001566  [109200/167876]\n",
            "loss: 0.001938  [112000/167876]\n",
            "loss: 0.002693  [114800/167876]\n",
            "loss: 0.001889  [117600/167876]\n",
            "loss: 0.001931  [120400/167876]\n",
            "loss: 0.002807  [123200/167876]\n",
            "loss: 0.002005  [126000/167876]\n",
            "loss: 0.002154  [128800/167876]\n",
            "loss: 0.001487  [131600/167876]\n",
            "loss: 0.001611  [134400/167876]\n",
            "loss: 0.001470  [137200/167876]\n",
            "loss: 0.002803  [140000/167876]\n",
            "loss: 0.002376  [142800/167876]\n",
            "loss: 0.001969  [145600/167876]\n",
            "loss: 0.001983  [148400/167876]\n",
            "loss: 0.002105  [151200/167876]\n",
            "loss: 0.001888  [154000/167876]\n",
            "loss: 0.002174  [156800/167876]\n",
            "loss: 0.001640  [159600/167876]\n",
            "loss: 0.002572  [162400/167876]\n",
            "loss: 0.001946  [165200/167876]\n",
            "reverb decay: avg MSE: 0.002894, avg abs error: 0.037\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 15\n",
            "loss: 0.002251  [  0/167876]\n",
            "loss: 0.001705  [2800/167876]\n",
            "loss: 0.001832  [5600/167876]\n",
            "loss: 0.001731  [8400/167876]\n",
            "loss: 0.001331  [11200/167876]\n",
            "loss: 0.002580  [14000/167876]\n",
            "loss: 0.001857  [16800/167876]\n",
            "loss: 0.001668  [19600/167876]\n",
            "loss: 0.002213  [22400/167876]\n",
            "loss: 0.001790  [25200/167876]\n",
            "loss: 0.002415  [28000/167876]\n",
            "loss: 0.002087  [30800/167876]\n",
            "loss: 0.002572  [33600/167876]\n",
            "loss: 0.001695  [36400/167876]\n",
            "loss: 0.001233  [39200/167876]\n",
            "loss: 0.001558  [42000/167876]\n",
            "loss: 0.001873  [44800/167876]\n",
            "loss: 0.001692  [47600/167876]\n",
            "loss: 0.001702  [50400/167876]\n",
            "loss: 0.002736  [53200/167876]\n",
            "loss: 0.001554  [56000/167876]\n",
            "loss: 0.001871  [58800/167876]\n",
            "loss: 0.002065  [61600/167876]\n",
            "loss: 0.001817  [64400/167876]\n",
            "loss: 0.002016  [67200/167876]\n",
            "loss: 0.001342  [70000/167876]\n",
            "loss: 0.002562  [72800/167876]\n",
            "loss: 0.002006  [75600/167876]\n",
            "loss: 0.001805  [78400/167876]\n",
            "loss: 0.001497  [81200/167876]\n",
            "loss: 0.001478  [84000/167876]\n",
            "loss: 0.002509  [86800/167876]\n",
            "loss: 0.002607  [89600/167876]\n",
            "loss: 0.001997  [92400/167876]\n",
            "loss: 0.002349  [95200/167876]\n",
            "loss: 0.001840  [98000/167876]\n",
            "loss: 0.002274  [100800/167876]\n",
            "loss: 0.003118  [103600/167876]\n",
            "loss: 0.004009  [106400/167876]\n",
            "loss: 0.002665  [109200/167876]\n",
            "loss: 0.002201  [112000/167876]\n",
            "loss: 0.003054  [114800/167876]\n",
            "loss: 0.001159  [117600/167876]\n",
            "loss: 0.002358  [120400/167876]\n",
            "loss: 0.001353  [123200/167876]\n",
            "loss: 0.001424  [126000/167876]\n",
            "loss: 0.002119  [128800/167876]\n",
            "loss: 0.002370  [131600/167876]\n",
            "loss: 0.001641  [134400/167876]\n",
            "loss: 0.001551  [137200/167876]\n",
            "loss: 0.002095  [140000/167876]\n",
            "loss: 0.002180  [142800/167876]\n",
            "loss: 0.001868  [145600/167876]\n",
            "loss: 0.002069  [148400/167876]\n",
            "loss: 0.001855  [151200/167876]\n",
            "loss: 0.002919  [154000/167876]\n",
            "loss: 0.002877  [156800/167876]\n",
            "loss: 0.001683  [159600/167876]\n",
            "loss: 0.002657  [162400/167876]\n",
            "loss: 0.002326  [165200/167876]\n",
            "reverb decay: avg MSE: 0.002980, avg abs error: 0.0371\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 16\n",
            "loss: 0.002387  [  0/167876]\n",
            "loss: 0.001453  [2800/167876]\n",
            "loss: 0.002046  [5600/167876]\n",
            "loss: 0.001576  [8400/167876]\n",
            "loss: 0.001831  [11200/167876]\n",
            "loss: 0.001610  [14000/167876]\n",
            "loss: 0.001989  [16800/167876]\n",
            "loss: 0.002509  [19600/167876]\n",
            "loss: 0.002019  [22400/167876]\n",
            "loss: 0.001714  [25200/167876]\n",
            "loss: 0.001887  [28000/167876]\n",
            "loss: 0.002168  [30800/167876]\n",
            "loss: 0.001772  [33600/167876]\n",
            "loss: 0.001702  [36400/167876]\n",
            "loss: 0.001782  [39200/167876]\n",
            "loss: 0.002127  [42000/167876]\n",
            "loss: 0.001651  [44800/167876]\n",
            "loss: 0.001921  [47600/167876]\n",
            "loss: 0.002001  [50400/167876]\n",
            "loss: 0.001683  [53200/167876]\n",
            "loss: 0.001834  [56000/167876]\n",
            "loss: 0.001362  [58800/167876]\n",
            "loss: 0.001988  [61600/167876]\n",
            "loss: 0.001317  [64400/167876]\n",
            "loss: 0.002167  [67200/167876]\n",
            "loss: 0.001971  [70000/167876]\n",
            "loss: 0.001851  [72800/167876]\n",
            "loss: 0.001341  [75600/167876]\n",
            "loss: 0.002382  [78400/167876]\n",
            "loss: 0.001450  [81200/167876]\n",
            "loss: 0.001727  [84000/167876]\n",
            "loss: 0.001817  [86800/167876]\n",
            "loss: 0.001611  [89600/167876]\n",
            "loss: 0.002059  [92400/167876]\n",
            "loss: 0.002306  [95200/167876]\n",
            "loss: 0.002160  [98000/167876]\n",
            "loss: 0.001366  [100800/167876]\n",
            "loss: 0.002171  [103600/167876]\n",
            "loss: 0.002057  [106400/167876]\n",
            "loss: 0.001578  [109200/167876]\n",
            "loss: 0.002468  [112000/167876]\n",
            "loss: 0.002540  [114800/167876]\n",
            "loss: 0.001292  [117600/167876]\n",
            "loss: 0.001918  [120400/167876]\n",
            "loss: 0.002036  [123200/167876]\n",
            "loss: 0.001790  [126000/167876]\n",
            "loss: 0.001687  [128800/167876]\n",
            "loss: 0.001368  [131600/167876]\n",
            "loss: 0.001833  [134400/167876]\n",
            "loss: 0.001797  [137200/167876]\n",
            "loss: 0.001863  [140000/167876]\n",
            "loss: 0.001719  [142800/167876]\n",
            "loss: 0.001717  [145600/167876]\n",
            "loss: 0.001633  [148400/167876]\n",
            "loss: 0.001752  [151200/167876]\n",
            "loss: 0.002098  [154000/167876]\n",
            "loss: 0.002326  [156800/167876]\n",
            "loss: 0.001641  [159600/167876]\n",
            "loss: 0.001688  [162400/167876]\n",
            "loss: 0.001817  [165200/167876]\n",
            "reverb decay: avg MSE: 0.003085, avg abs error: 0.0387\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 17\n",
            "loss: 0.002853  [  0/167876]\n",
            "loss: 0.001802  [2800/167876]\n",
            "loss: 0.001423  [5600/167876]\n",
            "loss: 0.001729  [8400/167876]\n",
            "loss: 0.001965  [11200/167876]\n",
            "loss: 0.002556  [14000/167876]\n",
            "loss: 0.002314  [16800/167876]\n",
            "loss: 0.002313  [19600/167876]\n",
            "loss: 0.002363  [22400/167876]\n",
            "loss: 0.001620  [25200/167876]\n",
            "loss: 0.001425  [28000/167876]\n",
            "loss: 0.001647  [30800/167876]\n",
            "loss: 0.001629  [33600/167876]\n",
            "loss: 0.001261  [36400/167876]\n",
            "loss: 0.001492  [39200/167876]\n",
            "loss: 0.002146  [42000/167876]\n",
            "loss: 0.001380  [44800/167876]\n",
            "loss: 0.001470  [47600/167876]\n",
            "loss: 0.002559  [50400/167876]\n",
            "loss: 0.001836  [53200/167876]\n",
            "loss: 0.001923  [56000/167876]\n",
            "loss: 0.001765  [58800/167876]\n",
            "loss: 0.002453  [61600/167876]\n",
            "loss: 0.001311  [64400/167876]\n",
            "loss: 0.001547  [67200/167876]\n",
            "loss: 0.002255  [70000/167876]\n",
            "loss: 0.001953  [72800/167876]\n",
            "loss: 0.001599  [75600/167876]\n",
            "loss: 0.001747  [78400/167876]\n",
            "loss: 0.001346  [81200/167876]\n",
            "loss: 0.002271  [84000/167876]\n",
            "loss: 0.002728  [86800/167876]\n",
            "loss: 0.001463  [89600/167876]\n",
            "loss: 0.002453  [92400/167876]\n",
            "loss: 0.001259  [95200/167876]\n",
            "loss: 0.001668  [98000/167876]\n",
            "loss: 0.002005  [100800/167876]\n",
            "loss: 0.001879  [103600/167876]\n",
            "loss: 0.001372  [106400/167876]\n",
            "loss: 0.002232  [109200/167876]\n",
            "loss: 0.002151  [112000/167876]\n",
            "loss: 0.001957  [114800/167876]\n",
            "loss: 0.002484  [117600/167876]\n",
            "loss: 0.002010  [120400/167876]\n",
            "loss: 0.002400  [123200/167876]\n",
            "loss: 0.001882  [126000/167876]\n",
            "loss: 0.001830  [128800/167876]\n",
            "loss: 0.001519  [131600/167876]\n",
            "loss: 0.001913  [134400/167876]\n",
            "loss: 0.001602  [137200/167876]\n",
            "loss: 0.001346  [140000/167876]\n",
            "loss: 0.002203  [142800/167876]\n",
            "loss: 0.001841  [145600/167876]\n",
            "loss: 0.002290  [148400/167876]\n",
            "loss: 0.001446  [151200/167876]\n",
            "loss: 0.001748  [154000/167876]\n",
            "loss: 0.001775  [156800/167876]\n",
            "loss: 0.001774  [159600/167876]\n",
            "loss: 0.001987  [162400/167876]\n",
            "loss: 0.001976  [165200/167876]\n",
            "reverb decay: avg MSE: 0.003306, avg abs error: 0.0404\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 18\n",
            "loss: 0.002768  [  0/167876]\n",
            "loss: 0.001697  [2800/167876]\n",
            "loss: 0.001539  [5600/167876]\n",
            "loss: 0.001391  [8400/167876]\n",
            "loss: 0.001848  [11200/167876]\n",
            "loss: 0.002224  [14000/167876]\n",
            "loss: 0.001367  [16800/167876]\n",
            "loss: 0.001633  [19600/167876]\n",
            "loss: 0.001581  [22400/167876]\n",
            "loss: 0.001219  [25200/167876]\n",
            "loss: 0.001503  [28000/167876]\n",
            "loss: 0.002489  [30800/167876]\n",
            "loss: 0.001915  [33600/167876]\n",
            "loss: 0.001411  [36400/167876]\n",
            "loss: 0.001544  [39200/167876]\n",
            "loss: 0.001203  [42000/167876]\n",
            "loss: 0.001940  [44800/167876]\n",
            "loss: 0.002074  [47600/167876]\n",
            "loss: 0.001854  [50400/167876]\n",
            "loss: 0.002224  [53200/167876]\n",
            "loss: 0.001822  [56000/167876]\n",
            "loss: 0.001819  [58800/167876]\n",
            "loss: 0.002362  [61600/167876]\n",
            "loss: 0.002498  [64400/167876]\n",
            "loss: 0.001670  [67200/167876]\n",
            "loss: 0.002534  [70000/167876]\n",
            "loss: 0.001963  [72800/167876]\n",
            "loss: 0.001517  [75600/167876]\n",
            "loss: 0.002054  [78400/167876]\n",
            "loss: 0.001859  [81200/167876]\n",
            "loss: 0.001277  [84000/167876]\n",
            "loss: 0.001547  [86800/167876]\n",
            "loss: 0.001738  [89600/167876]\n",
            "loss: 0.002097  [92400/167876]\n",
            "loss: 0.001496  [95200/167876]\n",
            "loss: 0.001634  [98000/167876]\n",
            "loss: 0.001364  [100800/167876]\n",
            "loss: 0.001899  [103600/167876]\n",
            "loss: 0.002705  [106400/167876]\n",
            "loss: 0.002022  [109200/167876]\n",
            "loss: 0.001216  [112000/167876]\n",
            "loss: 0.001367  [114800/167876]\n",
            "loss: 0.002023  [117600/167876]\n",
            "loss: 0.001114  [120400/167876]\n",
            "loss: 0.002895  [123200/167876]\n",
            "loss: 0.002323  [126000/167876]\n",
            "loss: 0.001580  [128800/167876]\n",
            "loss: 0.001421  [131600/167876]\n",
            "loss: 0.001646  [134400/167876]\n",
            "loss: 0.002090  [137200/167876]\n",
            "loss: 0.003323  [140000/167876]\n",
            "loss: 0.001402  [142800/167876]\n",
            "loss: 0.001757  [145600/167876]\n",
            "loss: 0.001830  [148400/167876]\n",
            "loss: 0.001182  [151200/167876]\n",
            "loss: 0.001737  [154000/167876]\n",
            "loss: 0.002605  [156800/167876]\n",
            "loss: 0.001672  [159600/167876]\n",
            "loss: 0.001432  [162400/167876]\n",
            "loss: 0.001378  [165200/167876]\n",
            "reverb decay: avg MSE: 0.002841, avg abs error: 0.0364\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 19\n",
            "loss: 0.001398  [  0/167876]\n",
            "loss: 0.001464  [2800/167876]\n",
            "loss: 0.001032  [5600/167876]\n",
            "loss: 0.001796  [8400/167876]\n",
            "loss: 0.001302  [11200/167876]\n",
            "loss: 0.001645  [14000/167876]\n",
            "loss: 0.001222  [16800/167876]\n",
            "loss: 0.001733  [19600/167876]\n",
            "loss: 0.002706  [22400/167876]\n",
            "loss: 0.002497  [25200/167876]\n",
            "loss: 0.001463  [28000/167876]\n",
            "loss: 0.002540  [30800/167876]\n",
            "loss: 0.001401  [33600/167876]\n",
            "loss: 0.002273  [36400/167876]\n",
            "loss: 0.001942  [39200/167876]\n",
            "loss: 0.001472  [42000/167876]\n",
            "loss: 0.001373  [44800/167876]\n",
            "loss: 0.001389  [47600/167876]\n",
            "loss: 0.001154  [50400/167876]\n",
            "loss: 0.001855  [53200/167876]\n",
            "loss: 0.001619  [56000/167876]\n",
            "loss: 0.001626  [58800/167876]\n",
            "loss: 0.001794  [61600/167876]\n",
            "loss: 0.001953  [64400/167876]\n",
            "loss: 0.001852  [67200/167876]\n",
            "loss: 0.001590  [70000/167876]\n",
            "loss: 0.001911  [72800/167876]\n",
            "loss: 0.002206  [75600/167876]\n",
            "loss: 0.002215  [78400/167876]\n",
            "loss: 0.001215  [81200/167876]\n",
            "loss: 0.001598  [84000/167876]\n",
            "loss: 0.001144  [86800/167876]\n",
            "loss: 0.001991  [89600/167876]\n",
            "loss: 0.001639  [92400/167876]\n",
            "loss: 0.001451  [95200/167876]\n",
            "loss: 0.001626  [98000/167876]\n",
            "loss: 0.001819  [100800/167876]\n",
            "loss: 0.001470  [103600/167876]\n",
            "loss: 0.002068  [106400/167876]\n",
            "loss: 0.001938  [109200/167876]\n",
            "loss: 0.001467  [112000/167876]\n",
            "loss: 0.002151  [114800/167876]\n",
            "loss: 0.002135  [117600/167876]\n",
            "loss: 0.001518  [120400/167876]\n",
            "loss: 0.002404  [123200/167876]\n",
            "loss: 0.001962  [126000/167876]\n",
            "loss: 0.001794  [128800/167876]\n",
            "loss: 0.001958  [131600/167876]\n",
            "loss: 0.001685  [134400/167876]\n",
            "loss: 0.001826  [137200/167876]\n",
            "loss: 0.001520  [140000/167876]\n",
            "loss: 0.002152  [142800/167876]\n",
            "loss: 0.001695  [145600/167876]\n",
            "loss: 0.001857  [148400/167876]\n",
            "loss: 0.001725  [151200/167876]\n",
            "loss: 0.001536  [154000/167876]\n",
            "loss: 0.001292  [156800/167876]\n",
            "loss: 0.001577  [159600/167876]\n",
            "loss: 0.001474  [162400/167876]\n",
            "loss: 0.001472  [165200/167876]\n",
            "reverb decay: avg MSE: 0.002820, avg abs error: 0.0363\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 20\n",
            "loss: 0.001257  [  0/167876]\n",
            "loss: 0.001524  [2800/167876]\n",
            "loss: 0.001773  [5600/167876]\n",
            "loss: 0.001576  [8400/167876]\n",
            "loss: 0.001519  [11200/167876]\n",
            "loss: 0.002032  [14000/167876]\n",
            "loss: 0.002639  [16800/167876]\n",
            "loss: 0.001879  [19600/167876]\n",
            "loss: 0.001782  [22400/167876]\n",
            "loss: 0.001160  [25200/167876]\n",
            "loss: 0.001879  [28000/167876]\n",
            "loss: 0.001699  [30800/167876]\n",
            "loss: 0.002195  [33600/167876]\n",
            "loss: 0.001248  [36400/167876]\n",
            "loss: 0.001584  [39200/167876]\n",
            "loss: 0.001594  [42000/167876]\n",
            "loss: 0.001589  [44800/167876]\n",
            "loss: 0.001497  [47600/167876]\n",
            "loss: 0.001722  [50400/167876]\n",
            "loss: 0.001684  [53200/167876]\n",
            "loss: 0.001301  [56000/167876]\n",
            "loss: 0.002342  [58800/167876]\n",
            "loss: 0.001541  [61600/167876]\n",
            "loss: 0.002253  [64400/167876]\n",
            "loss: 0.001059  [67200/167876]\n",
            "loss: 0.001678  [70000/167876]\n",
            "loss: 0.002120  [72800/167876]\n",
            "loss: 0.001375  [75600/167876]\n",
            "loss: 0.001885  [78400/167876]\n",
            "loss: 0.001377  [81200/167876]\n",
            "loss: 0.001893  [84000/167876]\n",
            "loss: 0.001452  [86800/167876]\n",
            "loss: 0.001720  [89600/167876]\n",
            "loss: 0.002220  [92400/167876]\n",
            "loss: 0.001915  [95200/167876]\n",
            "loss: 0.001286  [98000/167876]\n",
            "loss: 0.001681  [100800/167876]\n",
            "loss: 0.001677  [103600/167876]\n",
            "loss: 0.001683  [106400/167876]\n",
            "loss: 0.001814  [109200/167876]\n",
            "loss: 0.001375  [112000/167876]\n",
            "loss: 0.001804  [114800/167876]\n",
            "loss: 0.001487  [117600/167876]\n",
            "loss: 0.001571  [120400/167876]\n",
            "loss: 0.001343  [123200/167876]\n",
            "loss: 0.001654  [126000/167876]\n",
            "loss: 0.001632  [128800/167876]\n",
            "loss: 0.001454  [131600/167876]\n",
            "loss: 0.001154  [134400/167876]\n",
            "loss: 0.001944  [137200/167876]\n",
            "loss: 0.001474  [140000/167876]\n",
            "loss: 0.001295  [142800/167876]\n",
            "loss: 0.001943  [145600/167876]\n",
            "loss: 0.001668  [148400/167876]\n",
            "loss: 0.001796  [151200/167876]\n",
            "loss: 0.001388  [154000/167876]\n",
            "loss: 0.002276  [156800/167876]\n",
            "loss: 0.001602  [159600/167876]\n",
            "loss: 0.001793  [162400/167876]\n",
            "loss: 0.001548  [165200/167876]\n",
            "reverb decay: avg MSE: 0.002878, avg abs error: 0.0361\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Finished training\n",
            "reverb decay: avg MSE: 0.002893, avg abs error: 0.0362\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x800 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from src.util import plot_box\n",
        "import numpy as np\n",
        "\n",
        "WEIGHTS_DIR = \"_weights/\"\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 20\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device {device}\")\n",
        "\n",
        "error = [[], [], [], [], []]\n",
        "\n",
        "for fx in range(5):\n",
        "\n",
        "    WEIGHTS_PATH = os.path.join(WEIGHTS_DIR, EXPERIMENT_NAME + \"_\" + str(fx))\n",
        "\n",
        "    if not os.path.exists('%s' % WEIGHTS_DIR):\n",
        "        os.makedirs('%s' % WEIGHTS_DIR)\n",
        "\n",
        "    fxData = load_data(fx)\n",
        "    train_dataloader, test_dataloader, val_dataloader = split_data(fxData)\n",
        "    \n",
        "    # construct model and assign it to device\n",
        "    cnn = model.Extractor().to(device)\n",
        "    \n",
        "    if fx == 0:\n",
        "        signal, _, _, _, _ = fxData[0]\n",
        "        print(f\"There are {len(fxData)} samples in the dataset.\")\n",
        "        print(f\"Shape of signal: {signal.shape}\")\n",
        "\n",
        "        print(\"input feature:\")\n",
        "        log_writer.add_figure(\"Input Feature\", plot_spectrogram(signal[0], title=\"MFCC\"))\n",
        "        log_writer.add_graph(cnn, signal.unsqueeze_(0))\n",
        "\n",
        "    # initialise loss funtion + optimiser\n",
        "    loss_fn = nn.MSELoss(reduction='mean')\n",
        "    optimiser = torch.optim.Adam(cnn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "    # train model\n",
        "    train.train(cnn,\n",
        "                train_dataloader,\n",
        "                test_dataloader,\n",
        "                loss_fn,\n",
        "                optimiser,\n",
        "                device,\n",
        "                log_writer,\n",
        "                EPOCHS,\n",
        "                WEIGHTS_PATH,\n",
        "                effect=fx)\n",
        "\n",
        "    _, _, log = train.test(cnn, val_dataloader, device, effect=fx)\n",
        "    for _, data in enumerate(log):\n",
        "        error[fx].append(data[3])\n",
        "\n",
        "arr = np.array(error)\n",
        "np.save(EVU_DIR + EXPERIMENT_NAME + \"_evaluation.npy\", arr)\n",
        "\n",
        "log_writer.add_figure(\"Error Box\", \n",
        "                      plot_box(error, title=\"Error\", labels=EFFECT_MAP, ylabel=\"paramter value\"))\n",
        "\n",
        "log_writer.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "torchaudio-tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
