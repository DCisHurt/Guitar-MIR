{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parameter extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMiPTdSA8C2F",
        "outputId": "a297ba92-a5c6-4015-d92d-1e9487daf896"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchaudio\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gIdrZtW_JYf",
        "outputId": "c1bb6969-15a9-44eb-8cdd-8d79c89208f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cpu\n",
            "There are 5220 samples in the dataset.\n",
            "Shape of signal: torch.Size([1, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "from src.gtfxdataset import GtFxDataset\n",
        "\n",
        "AUDIO_DIR = \"_assets/DATASET/GT-FX-C51/\"\n",
        "ANNOTATIONS_FILE = os.path.join(AUDIO_DIR, \"annotation.csv\")\n",
        "\n",
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050*3\n",
        "\n",
        "EFFECT_MAP = [\"distortion\", \"chorus\", \"tremolo\", \"delay\", \"reverb\"]\n",
        "EFFECT = 1\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device {device}\")\n",
        "\n",
        "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SAMPLE_RATE,\n",
        "    n_fft=512,\n",
        "    hop_length=1050,\n",
        "    n_mels=64,\n",
        "    # power=2\n",
        ")\n",
        "\n",
        "spectrogram = torchaudio.transforms.Spectrogram(\n",
        "    power=2,\n",
        "    n_fft=127,\n",
        "    win_length= 127,\n",
        "    hop_length= 1040,\n",
        "    normalized=True\n",
        ")\n",
        "\n",
        "mfcc = torchaudio.transforms.MFCC(\n",
        "    sample_rate = SAMPLE_RATE, \n",
        "    n_mfcc = 64,\n",
        "    melkwargs = {\n",
        "        \"n_fft\": 1024,\n",
        "        \"hop_length\": 1030,\n",
        "        \"n_mels\": 64,\n",
        "        \"center\": False})\n",
        "\n",
        "fxData = GtFxDataset(ANNOTATIONS_FILE,\n",
        "                        AUDIO_DIR,\n",
        "                        mfcc,\n",
        "                        SAMPLE_RATE,\n",
        "                        NUM_SAMPLES,\n",
        "                        device,\n",
        "                        EFFECT_MAP[EFFECT])\n",
        "\n",
        "signal, _, _, _, _ = fxData[0]\n",
        "print(f\"There are {len(fxData)} samples in the dataset.\")\n",
        "print(f\"Shape of signal: {signal.shape}\")\n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Split dataset into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.extrector import train\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "split_ratio = [0.8, 0.1, 0.1]\n",
        "train_set, test_set, val_set = torch.utils.data.random_split(fxData, lengths=split_ratio)\n",
        "\n",
        "train_dataloader = train.create_data_loader(train_set, BATCH_SIZE)\n",
        "test_dataloader = train.create_data_loader(test_set, BATCH_SIZE)\n",
        "val_dataloader = train.create_data_loader(val_set, BATCH_SIZE)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8O9r5WI2zuI",
        "outputId": "ed2a4ebd-f644-4e35-8fda-86170924dfe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "_weights/c51_parameter_1.pth\n"
          ]
        }
      ],
      "source": [
        "from src.extrector import model\n",
        "\n",
        "LEARNING_RATE = 0.0005\n",
        "EPOCHS = 10\n",
        "\n",
        "WEIGHTS_DIR = \"_weights/\"\n",
        "WEIGHTS_FILE = os.path.join(WEIGHTS_DIR, \"c51_parameter_\" + str(EFFECT) + \".pth\")\n",
        "\n",
        "if not os.path.exists('%s' % WEIGHTS_DIR):\n",
        "    os.makedirs('%s' % WEIGHTS_DIR)\n",
        "\n",
        "# construct model and assign it to device\n",
        "cnn = model.Extractor().to(device)\n",
        "\n",
        "# initialise loss funtion + optimiser\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "optimiser = torch.optim.Adam(cnn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# train model\n",
        "train.train(cnn, train_dataloader, test_dataloader, loss_fn, optimiser, device, EPOCHS, effect=EFFECT)\n",
        "\n",
        "# save model\n",
        "torch.save(cnn.state_dict(), WEIGHTS_FILE)\n",
        "print(\"Trained feed forward net saved at %s\" %(WEIGHTS_FILE))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg MSE: 0.007226\n",
            "['c51_325_cs_1', 0.25, 0.29]\n",
            "['c51_222_cs_2', 0.53, 0.51]\n",
            "['c51_384_cs_4', 0.83, 0.88]\n",
            "['c51_530_cs_2', 0.35, 0.46]\n",
            "['c51_989_cs_2', 0.49, 0.53]\n",
            "['c51_495_cs_1', 0.19, 0.32]\n",
            "['c51_339_cs_4', 0.88, 0.91]\n",
            "['c51_318_cs_2', 0.48, 0.53]\n",
            "['c51_536_cs_2', 0.47, 0.5]\n",
            "['c51_523_cs_1', 0.24, 0.32]\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "cnn = model.Extractor().to(device)\n",
        "\n",
        "state_dict = torch.load(WEIGHTS_FILE)\n",
        "cnn.load_state_dict(state_dict)\n",
        "\n",
        "log = train.test(cnn, val_dataloader, device, effect=EFFECT)\n",
        "\n",
        "for i in range(10):\n",
        "    print(log[i])\n",
        "\n",
        "# file = open('report.csv', 'w+', newline ='')\n",
        "\n",
        "# # writing the data into the file\n",
        "# with file:   \n",
        "#     write = csv.writer(file)\n",
        "#     write.writerows(log)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "torchaudio-tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
