{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Parameter extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMiPTdSA8C2F",
        "outputId": "a297ba92-a5c6-4015-d92d-1e9487daf896"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchaudio\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gIdrZtW_JYf",
        "outputId": "c1bb6969-15a9-44eb-8cdd-8d79c89208f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cpu\n",
            "There are 16896 samples in the dataset.\n",
            "Shape of signal: torch.Size([1, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "from src.gtfxdataset import GtFxDataset\n",
        "\n",
        "AUDIO_DIR = \"_assets/DATASET/GT-FX-ALL/\"\n",
        "ANNOTATIONS_FILE = os.path.join(AUDIO_DIR, \"annotation.csv\")\n",
        "\n",
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050*3\n",
        "\n",
        "EFFECT_MAP = [\"distortion\", \"chorus\", \"tremolo\", \"delay\", \"reverb\"]\n",
        "EFFECT = 4\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device {device}\")\n",
        "\n",
        "mel_spectrogram = torchaudio.transforms.MelSpectrogram(\n",
        "    sample_rate=SAMPLE_RATE,\n",
        "    n_fft=512,\n",
        "    hop_length=1050,\n",
        "    n_mels=64,\n",
        "    # power=2\n",
        ")\n",
        "\n",
        "spectrogram = torchaudio.transforms.Spectrogram(\n",
        "    power=2,\n",
        "    n_fft=127,\n",
        "    win_length= 127,\n",
        "    hop_length= 1040,\n",
        "    normalized=True\n",
        ")\n",
        "\n",
        "mfcc = torchaudio.transforms.MFCC(\n",
        "    sample_rate = SAMPLE_RATE, \n",
        "    n_mfcc = 64,\n",
        "    melkwargs = {\n",
        "        \"n_fft\": 1024,\n",
        "        \"hop_length\": 1030,\n",
        "        \"n_mels\": 64,\n",
        "        \"center\": False})\n",
        "\n",
        "fxData = GtFxDataset(ANNOTATIONS_FILE,\n",
        "                        AUDIO_DIR,\n",
        "                        mfcc,\n",
        "                        SAMPLE_RATE,\n",
        "                        NUM_SAMPLES,\n",
        "                        device,\n",
        "                        EFFECT_MAP[EFFECT])\n",
        "\n",
        "signal, _, _, _, _ = fxData[0]\n",
        "print(f\"There are {len(fxData)} samples in the dataset.\")\n",
        "print(f\"Shape of signal: {signal.shape}\")\n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Split dataset into train and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.extrector import train\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "split_ratio = [0.8, 0.1, 0.1]\n",
        "train_set, test_set, val_set = torch.utils.data.random_split(fxData, lengths=split_ratio)\n",
        "\n",
        "train_dataloader = train.create_data_loader(train_set, BATCH_SIZE)\n",
        "test_dataloader = train.create_data_loader(test_set, BATCH_SIZE)\n",
        "val_dataloader = train.create_data_loader(val_set, BATCH_SIZE)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8O9r5WI2zuI",
        "outputId": "ed2a4ebd-f644-4e35-8fda-86170924dfe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "loss: 0.082270  [  0/13568]\n",
            "loss: 0.052145  [1280/13568]\n",
            "loss: 0.049215  [2560/13568]\n",
            "loss: 0.039952  [3840/13568]\n",
            "loss: 0.043157  [5120/13568]\n",
            "loss: 0.044093  [6400/13568]\n",
            "loss: 0.045471  [7680/13568]\n",
            "loss: 0.027066  [8960/13568]\n",
            "loss: 0.031229  [10240/13568]\n",
            "loss: 0.030402  [11520/13568]\n",
            "loss: 0.042724  [12800/13568]\n",
            "avg MSE: 0.035681\n",
            "---------------------------\n",
            "Epoch 2\n",
            "loss: 0.026128  [  0/13568]\n",
            "loss: 0.029098  [1280/13568]\n",
            "loss: 0.031910  [2560/13568]\n",
            "loss: 0.029922  [3840/13568]\n",
            "loss: 0.029470  [5120/13568]\n",
            "loss: 0.021193  [6400/13568]\n",
            "loss: 0.040507  [7680/13568]\n",
            "loss: 0.025790  [8960/13568]\n",
            "loss: 0.022867  [10240/13568]\n",
            "loss: 0.021869  [11520/13568]\n",
            "loss: 0.022812  [12800/13568]\n",
            "avg MSE: 0.025411\n",
            "---------------------------\n",
            "Epoch 3\n",
            "loss: 0.036826  [  0/13568]\n",
            "loss: 0.025664  [1280/13568]\n",
            "loss: 0.026879  [2560/13568]\n",
            "loss: 0.015964  [3840/13568]\n",
            "loss: 0.017132  [5120/13568]\n",
            "loss: 0.027433  [6400/13568]\n",
            "loss: 0.018404  [7680/13568]\n",
            "loss: 0.019370  [8960/13568]\n",
            "loss: 0.022327  [10240/13568]\n",
            "loss: 0.017898  [11520/13568]\n",
            "loss: 0.026803  [12800/13568]\n",
            "avg MSE: 0.020814\n",
            "---------------------------\n",
            "Epoch 4\n",
            "loss: 0.020131  [  0/13568]\n",
            "loss: 0.014721  [1280/13568]\n",
            "loss: 0.029731  [2560/13568]\n",
            "loss: 0.015920  [3840/13568]\n",
            "loss: 0.016551  [5120/13568]\n",
            "loss: 0.017579  [6400/13568]\n",
            "loss: 0.020249  [7680/13568]\n",
            "loss: 0.008562  [8960/13568]\n",
            "loss: 0.017514  [10240/13568]\n",
            "loss: 0.027035  [11520/13568]\n",
            "loss: 0.017813  [12800/13568]\n",
            "avg MSE: 0.018405\n",
            "---------------------------\n",
            "Epoch 5\n",
            "loss: 0.021427  [  0/13568]\n",
            "loss: 0.021677  [1280/13568]\n",
            "loss: 0.013802  [2560/13568]\n",
            "loss: 0.012892  [3840/13568]\n",
            "loss: 0.012181  [5120/13568]\n",
            "loss: 0.017416  [6400/13568]\n",
            "loss: 0.013599  [7680/13568]\n",
            "loss: 0.025170  [8960/13568]\n",
            "loss: 0.021307  [10240/13568]\n",
            "loss: 0.023695  [11520/13568]\n",
            "loss: 0.015204  [12800/13568]\n",
            "avg MSE: 0.020835\n",
            "---------------------------\n",
            "Finished training\n",
            "Trained feed forward net saved at _weights/c55_parameter_4.pth\n"
          ]
        }
      ],
      "source": [
        "from src.extrector import model\n",
        "\n",
        "LEARNING_RATE = 0.0003\n",
        "EPOCHS = 5\n",
        "\n",
        "WEIGHTS_DIR = \"_weights/\"\n",
        "WEIGHTS_FILE = os.path.join(WEIGHTS_DIR, \"c55_parameter_\" + str(EFFECT) + \".pth\")\n",
        "\n",
        "if not os.path.exists('%s' % WEIGHTS_DIR):\n",
        "    os.makedirs('%s' % WEIGHTS_DIR)\n",
        "\n",
        "# construct model and assign it to device\n",
        "cnn = model.Extractor().to(device)\n",
        "\n",
        "# initialise loss funtion + optimiser\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "optimiser = torch.optim.Adam(cnn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# train model\n",
        "train.train(cnn, train_dataloader, test_dataloader, loss_fn, optimiser, device, EPOCHS, effect=EFFECT)\n",
        "\n",
        "# save model\n",
        "torch.save(cnn.state_dict(), WEIGHTS_FILE)\n",
        "print(\"Trained feed forward net saved at %s\" %(WEIGHTS_FILE))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "avg MSE: 0.019669\n",
            "['c55_7_od_0_cs_1_tr_1_dl_0_rv_0', 0.37, 0.18]\n",
            "['c52_20_od_1_rv_1', 0.59, 0.51]\n",
            "['c54_21_cs_2_tr_0_dl_0_rv_2', 0.77, 0.81]\n",
            "['c54_8_od_0_cs_1_tr_2_rv_1', 0.48, 0.53]\n",
            "['c54_16_od_2_cs_2_tr_0_rv_1', 0.56, 0.56]\n",
            "['c53_15_od_1_tr_1_rv_1', 0.48, 0.56]\n",
            "['c54_8_od_2_tr_1_dl_1_rv_0', 0.42, 0.15]\n",
            "['c54_4_od_1_tr_2_dl_1_rv_0', 0.13, 0.14]\n",
            "['c54_12_cs_1_tr_1_dl_0_rv_1', 0.58, 0.55]\n",
            "['c52_2_od_2_rv_2', 0.89, 0.79]\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "\n",
        "cnn = model.Extractor().to(device)\n",
        "\n",
        "state_dict = torch.load(WEIGHTS_FILE)\n",
        "cnn.load_state_dict(state_dict)\n",
        "\n",
        "log = train.test(cnn, val_dataloader, device, effect=EFFECT)\n",
        "\n",
        "for i in range(10):\n",
        "    print(log[i])\n",
        "\n",
        "# file = open('report.csv', 'w+', newline ='')\n",
        "\n",
        "# # writing the data into the file\n",
        "# with file:   \n",
        "#     write = csv.writer(file)\n",
        "#     write.writerows(log)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "torchaudio-tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
