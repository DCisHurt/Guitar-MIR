{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Signal Chain Reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMiPTdSA8C2F",
        "outputId": "a297ba92-a5c6-4015-d92d-1e9487daf896"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchaudio\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gIdrZtW_JYf",
        "outputId": "c1bb6969-15a9-44eb-8cdd-8d79c89208f2"
      },
      "outputs": [],
      "source": [
        "from src.gtfxdataset import GtFxDataset\n",
        "\n",
        "AUDIO_DIR = \"_assets/DATASET/GT-FX-C53/\"\n",
        "EVU_ANNOTATIONS_FILE = os.path.join(AUDIO_DIR, \"evaluation.csv\")\n",
        "\n",
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050*3\n",
        "\n",
        "EFFECT_MAP = [\"distortion\", \"chorus\", \"tremolo\", \"delay\", \"reverb\"]\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device {device}\")\n",
        "\n",
        "mfcc = torchaudio.transforms.MFCC(\n",
        "    sample_rate = SAMPLE_RATE, \n",
        "    n_mfcc = 64,\n",
        "    melkwargs = {\n",
        "        \"n_fft\": 1024,\n",
        "        \"hop_length\": 1024,\n",
        "        \"n_mels\": 64,\n",
        "        \"center\": False})\n",
        "\n",
        "fxData = GtFxDataset(EVU_ANNOTATIONS_FILE,\n",
        "                        AUDIO_DIR,\n",
        "                        mfcc,\n",
        "                        SAMPLE_RATE,\n",
        "                        NUM_SAMPLES,\n",
        "                        device)\n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Reconstruct Singal Chain (singal)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.classifier import model as cModel\n",
        "from src.classifier import train as cTrain\n",
        "from src.extrector import model as eModel\n",
        "from src.extrector import train as eTrain\n",
        "from src.effectapplier import EffectApplier\n",
        "from src.util import play_audio, similarity_percentage\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "_WEIGHTS_DIR = \"_weights/Legacy/\"\n",
        "_DRY_SAMPLE_DIR = \"_assets/DATASET/GT-FX-DRY\"\n",
        "\n",
        "CLASSIFY_WEIGHTS = os.path.join(_WEIGHTS_DIR, \"c53_classify_15.pth\")\n",
        "\n",
        "index = random.randint(0, len(fxData))\n",
        "test_data = fxData[index]\n",
        "\n",
        "sample_file = fxData.get_audio_sample_filename(index) + \".wav\"\n",
        "dry_file =  \"C50-\" + sample_file[4:8] + \".wav\"\n",
        "\n",
        "classify = cModel.Classifier().to(device)\n",
        "classify.load_state_dict(torch.load(CLASSIFY_WEIGHTS))\n",
        "\n",
        "log = cTrain.test_single(classify, test_data)\n",
        "\n",
        "extract = eModel.Extractor().to(device)\n",
        "eff = EffectApplier(_DRY_SAMPLE_DIR, dry_file)\n",
        "\n",
        "table = []\n",
        "para_list = ['Gain', 'Depth', 'Rate', 'Delay Time', 'Decay']\n",
        "for fx, state in enumerate(log):\n",
        "    table.append([EFFECT_MAP[fx],\n",
        "                 \"State\",\n",
        "                 \"Activate\" if test_data[1][fx] > 0.0 else \"Bypass\",\n",
        "                 \"Activate\" if state == 1 else \"Bypass\"])\n",
        "\n",
        "    if state == 1:\n",
        "        WEIGHTS_FILE = os.path.join(_WEIGHTS_DIR, \"c53_parameter_\" + str(fx) + \"_20.pth\")\n",
        "        extract.load_state_dict(torch.load(WEIGHTS_FILE))\n",
        "        val = eTrain.test_single(extract, test_data, fx)\n",
        "        eff.addEffect(fx, val, mode=\"manual\")\n",
        "    else:\n",
        "        val = -1.0\n",
        "\n",
        "    table.append([EFFECT_MAP[fx],\n",
        "                para_list[fx],\n",
        "                \"N/A\" if test_data[1][fx] < 1 else round(test_data[3][fx].item(), 2),\n",
        "                \"N/A\" if val < 0 else val])\n",
        "remix = eff.generate()\n",
        "waveform, sr = torchaudio.load(os.path.join(AUDIO_DIR, sample_file))\n",
        "\n",
        "print(f\"Test file: {sample_file}\")\n",
        "play_audio(waveform, sr)\n",
        "\n",
        "print(f\"Remix by CNN Model:\")\n",
        "play_audio(remix, sr)\n",
        "\n",
        "resampler = torchaudio.transforms.Resample(44100, 22050)\n",
        "\n",
        "waveform = resampler(waveform)\n",
        "waveform = mfcc(waveform).reshape(64, 64)\n",
        "remix = resampler(remix)\n",
        "remix = mfcc(remix).reshape(64, 64)\n",
        "\n",
        "similarity = similarity_percentage(waveform, remix)\n",
        "\n",
        "print(f\"Similarity Percentage: {similarity:.2f}%\")\n",
        "\n",
        "df = pd.DataFrame(table)\n",
        "df.columns = [\"Effect\", \"Item\", \"Expected\", \"Predicted\"]\n",
        "df2 = df.groupby(['Effect', \"Item\"])\n",
        "df2.first()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Evaluate Signal Chain Reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.classifier import model as cModel\n",
        "from src.classifier import train as cTrain\n",
        "from src.extrector import model as eModel\n",
        "from src.extrector import train as eTrain\n",
        "from src.effectapplier import EffectApplier\n",
        "from src.util import similarity_percentage\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "EXPERIMENT_NAME = \"c53_signal_chain\"\n",
        "EVU_DIR = \"_log/Legacy/Evaluation/\"\n",
        "\n",
        "_WEIGHTS_DIR = \"_weights/Legacy/\"\n",
        "_DRY_SAMPLE_DIR = \"_assets/DATASET/GT-FX-DRY\"\n",
        "CLASSIFY_WEIGHTS = os.path.join(_WEIGHTS_DIR, \"c53_classify_15.pth\")\n",
        "\n",
        "if not os.path.exists('%s' % EVU_DIR):\n",
        "    os.makedirs('%s' % EVU_DIR)\n",
        "\n",
        "table = []\n",
        "\n",
        "for index in range(len(fxData)):  \n",
        "    test_data = fxData[index]\n",
        "\n",
        "    sample_file = fxData.get_audio_sample_filename(index) + \".wav\"\n",
        "    dry_file =  \"C50-\" + sample_file[4:8] + \".wav\"\n",
        "\n",
        "    classify = cModel.Classifier().to(device)\n",
        "    classify.load_state_dict(torch.load(CLASSIFY_WEIGHTS))\n",
        "\n",
        "    classify_log = cTrain.test_single(classify, test_data)\n",
        "\n",
        "    extract = eModel.Extractor().to(device)\n",
        "    eff = EffectApplier(_DRY_SAMPLE_DIR, dry_file)\n",
        "\n",
        "\n",
        "    log = [sample_file[:-4]]\n",
        "    for fx, pred_state in enumerate(classify_log):\n",
        "        \n",
        "        state = test_data[1][fx]\n",
        "        log.append(state * pred_state)\n",
        "\n",
        "        if state > 0:\n",
        "            WEIGHTS_FILE = os.path.join(_WEIGHTS_DIR, \"c53_parameter_\" + str(fx) + \"_20.pth\")\n",
        "            extract.load_state_dict(torch.load(WEIGHTS_FILE))\n",
        "            pred_val = eTrain.test_single(extract, test_data, fx)\n",
        "            eff.addEffect(fx, val, mode=\"manual\")\n",
        "            val = round(test_data[3][fx].item(), 2)\n",
        "            error = round(pred_val - val, 2)\n",
        "        else:\n",
        "            error = -1.0\n",
        "        log.append(error)\n",
        "\n",
        "    remix = eff.generate()\n",
        "    waveform, _ = torchaudio.load(os.path.join(AUDIO_DIR, sample_file))\n",
        "    resampler = torchaudio.transforms.Resample(44100, 22050)\n",
        "\n",
        "    waveform = resampler(waveform)\n",
        "    waveform = mfcc(waveform).reshape(64, 64)\n",
        "    remix = resampler(remix)\n",
        "    remix = mfcc(remix).reshape(64, 64)\n",
        "    similarity = similarity_percentage(waveform, remix)\n",
        "\n",
        "    log.append(round(similarity, 1))\n",
        "    table.append(log)\n",
        "    \n",
        "\n",
        "arr = np.array(table)\n",
        "np.save(EVU_DIR + EXPERIMENT_NAME + \"_evaluation.npy\", arr)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "torchaudio-tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
