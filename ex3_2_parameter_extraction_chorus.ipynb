{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Distortion Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMiPTdSA8C2F",
        "outputId": "a297ba92-a5c6-4015-d92d-1e9487daf896"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import os"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gIdrZtW_JYf",
        "outputId": "c1bb6969-15a9-44eb-8cdd-8d79c89208f2"
      },
      "outputs": [],
      "source": [
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050*3\n",
        "\n",
        "mfcc = torchaudio.transforms.MFCC(\n",
        "    sample_rate = SAMPLE_RATE, \n",
        "    n_mfcc = 64,\n",
        "    melkwargs = {\n",
        "        \"n_fft\": 1024,\n",
        "        \"hop_length\": 1024,\n",
        "        \"n_mels\": 64,\n",
        "        \"center\": False})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Functions for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.gtfxdataset import GtFxDataset\n",
        "from src.util import plot_spectrogram\n",
        "from src.extrector import train\n",
        "from src.extrector import model\n",
        "from torch import nn\n",
        "\n",
        "AUDIO_DIR = \"_assets/DATASET/GT-FX-C53/\"\n",
        "ANNOTATIONS_FILE = os.path.join(AUDIO_DIR, \"train.csv\")\n",
        "EVU_ANNOTATIONS_FILE = os.path.join(AUDIO_DIR, \"evaluation.csv\")\n",
        "EFFECT_MAP = [\"distortion\", \"chorus\", \"tremolo\", \"delay\", \"reverb\"]\n",
        "\n",
        "def load_train_data(effect):\n",
        "    \n",
        "    fxData = GtFxDataset(ANNOTATIONS_FILE,\n",
        "                        AUDIO_DIR,\n",
        "                        mfcc,\n",
        "                        SAMPLE_RATE,\n",
        "                        NUM_SAMPLES,\n",
        "                        device,\n",
        "                        effect=EFFECT_MAP[effect])\n",
        "    return fxData\n",
        "\n",
        "def load_evaluation_data(effect):\n",
        "\n",
        "    evuData = GtFxDataset(EVU_ANNOTATIONS_FILE,\n",
        "                        AUDIO_DIR,\n",
        "                        mfcc,\n",
        "                        SAMPLE_RATE,\n",
        "                        NUM_SAMPLES,\n",
        "                        device,\n",
        "                        effect=EFFECT_MAP[effect])\n",
        "\n",
        "    BATCH_SIZE = round(len(evuData) / 1500)\n",
        "    val_dataloader = train.create_data_loader(evuData, BATCH_SIZE)\n",
        "    return val_dataloader\n",
        "\n",
        "def split_data(data):\n",
        "\n",
        "    BATCH_SIZE = round(len(data) / 1500)\n",
        "\n",
        "    split_ratio = [0.9, 0.1]\n",
        "    train_set, test_set = torch.utils.data.random_split(data, lengths=split_ratio)\n",
        "\n",
        "    train_dataloader = train.create_data_loader(train_set, BATCH_SIZE)\n",
        "    test_dataloader = train.create_data_loader(test_set, BATCH_SIZE)\n",
        "\n",
        "    return train_dataloader, test_dataloader   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Add Tensorboard to record data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "EXPERIMENT_NAME = \"c53_parameter\"\n",
        "LOG_DIR = \"_log/\" + EXPERIMENT_NAME\n",
        "EVU_DIR = \"_log/Evaluation/\"\n",
        "\n",
        "if not os.path.exists('%s' % LOG_DIR):\n",
        "    os.makedirs('%s' % LOG_DIR)\n",
        "\n",
        "if not os.path.exists('%s' % EVU_DIR):\n",
        "    os.makedirs('%s' % EVU_DIR)\n",
        "\n",
        "log_writer = SummaryWriter(LOG_DIR)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8O9r5WI2zuI",
        "outputId": "ed2a4ebd-f644-4e35-8fda-86170924dfe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cpu\n",
            "Epoch 1\n",
            "loss: 0.088109  [  0/170046]\n",
            "loss: 0.306298  [2520/170046]\n",
            "loss: 0.087883  [5040/170046]\n",
            "loss: 0.064516  [7560/170046]\n",
            "loss: 0.052269  [10080/170046]\n",
            "loss: 0.062175  [12600/170046]\n",
            "loss: 0.044581  [15120/170046]\n",
            "loss: 0.045116  [17640/170046]\n",
            "loss: 0.042697  [20160/170046]\n",
            "loss: 0.041405  [22680/170046]\n",
            "loss: 0.035540  [25200/170046]\n",
            "loss: 0.028419  [27720/170046]\n",
            "loss: 0.035338  [30240/170046]\n",
            "loss: 0.024014  [32760/170046]\n",
            "loss: 0.030263  [35280/170046]\n",
            "loss: 0.024098  [37800/170046]\n",
            "loss: 0.031755  [40320/170046]\n",
            "loss: 0.025749  [42840/170046]\n",
            "loss: 0.027428  [45360/170046]\n",
            "loss: 0.017318  [47880/170046]\n",
            "loss: 0.035831  [50400/170046]\n",
            "loss: 0.018005  [52920/170046]\n",
            "loss: 0.016661  [55440/170046]\n",
            "loss: 0.017286  [57960/170046]\n",
            "loss: 0.018772  [60480/170046]\n",
            "loss: 0.029149  [63000/170046]\n",
            "loss: 0.018979  [65520/170046]\n",
            "loss: 0.020311  [68040/170046]\n",
            "loss: 0.019099  [70560/170046]\n",
            "loss: 0.016035  [73080/170046]\n",
            "loss: 0.019770  [75600/170046]\n",
            "loss: 0.014793  [78120/170046]\n",
            "loss: 0.017823  [80640/170046]\n",
            "loss: 0.018181  [83160/170046]\n",
            "loss: 0.020985  [85680/170046]\n",
            "loss: 0.011821  [88200/170046]\n",
            "loss: 0.012025  [90720/170046]\n",
            "loss: 0.020005  [93240/170046]\n",
            "loss: 0.013040  [95760/170046]\n",
            "loss: 0.010232  [98280/170046]\n",
            "loss: 0.015081  [100800/170046]\n",
            "loss: 0.016931  [103320/170046]\n",
            "loss: 0.013536  [105840/170046]\n",
            "loss: 0.016654  [108360/170046]\n",
            "loss: 0.013427  [110880/170046]\n",
            "loss: 0.014256  [113400/170046]\n",
            "loss: 0.018496  [115920/170046]\n",
            "loss: 0.012883  [118440/170046]\n",
            "loss: 0.012421  [120960/170046]\n",
            "loss: 0.012120  [123480/170046]\n",
            "loss: 0.012642  [126000/170046]\n",
            "loss: 0.010804  [128520/170046]\n",
            "loss: 0.013179  [131040/170046]\n",
            "loss: 0.019284  [133560/170046]\n",
            "loss: 0.015540  [136080/170046]\n",
            "loss: 0.012277  [138600/170046]\n",
            "loss: 0.013549  [141120/170046]\n",
            "loss: 0.012677  [143640/170046]\n",
            "loss: 0.010458  [146160/170046]\n",
            "loss: 0.012084  [148680/170046]\n",
            "loss: 0.010581  [151200/170046]\n",
            "loss: 0.011346  [153720/170046]\n",
            "loss: 0.018275  [156240/170046]\n",
            "loss: 0.011740  [158760/170046]\n",
            "loss: 0.009339  [161280/170046]\n",
            "loss: 0.013319  [163800/170046]\n",
            "loss: 0.016818  [166320/170046]\n",
            "loss: 0.011065  [168840/170046]\n",
            "chorus depth: avg MSE: 0.011643, avg abs error: 0.0815\n",
            "learning rate: 0.001000 -> 0.000910\n",
            "---------------------------\n",
            "\n",
            "Epoch 2\n",
            "loss: 0.011205  [  0/170046]\n",
            "loss: 0.010073  [2520/170046]\n",
            "loss: 0.011304  [5040/170046]\n",
            "loss: 0.010428  [7560/170046]\n",
            "loss: 0.010478  [10080/170046]\n",
            "loss: 0.011583  [12600/170046]\n",
            "loss: 0.013597  [15120/170046]\n",
            "loss: 0.011709  [17640/170046]\n",
            "loss: 0.010665  [20160/170046]\n",
            "loss: 0.010257  [22680/170046]\n",
            "loss: 0.014799  [25200/170046]\n",
            "loss: 0.012656  [27720/170046]\n",
            "loss: 0.008841  [30240/170046]\n",
            "loss: 0.010710  [32760/170046]\n",
            "loss: 0.009025  [35280/170046]\n",
            "loss: 0.011746  [37800/170046]\n",
            "loss: 0.011131  [40320/170046]\n",
            "loss: 0.012922  [42840/170046]\n",
            "loss: 0.013246  [45360/170046]\n",
            "loss: 0.011296  [47880/170046]\n",
            "loss: 0.009530  [50400/170046]\n",
            "loss: 0.008448  [52920/170046]\n",
            "loss: 0.009130  [55440/170046]\n",
            "loss: 0.010107  [57960/170046]\n",
            "loss: 0.015311  [60480/170046]\n",
            "loss: 0.012285  [63000/170046]\n",
            "loss: 0.010675  [65520/170046]\n",
            "loss: 0.012136  [68040/170046]\n",
            "loss: 0.013373  [70560/170046]\n",
            "loss: 0.008928  [73080/170046]\n",
            "loss: 0.016452  [75600/170046]\n",
            "loss: 0.008947  [78120/170046]\n",
            "loss: 0.016580  [80640/170046]\n",
            "loss: 0.009058  [83160/170046]\n",
            "loss: 0.007101  [85680/170046]\n",
            "loss: 0.012280  [88200/170046]\n",
            "loss: 0.008417  [90720/170046]\n",
            "loss: 0.008058  [93240/170046]\n",
            "loss: 0.013963  [95760/170046]\n",
            "loss: 0.009663  [98280/170046]\n",
            "loss: 0.011085  [100800/170046]\n",
            "loss: 0.008060  [103320/170046]\n",
            "loss: 0.009056  [105840/170046]\n",
            "loss: 0.014847  [108360/170046]\n",
            "loss: 0.009605  [110880/170046]\n",
            "loss: 0.012889  [113400/170046]\n",
            "loss: 0.010494  [115920/170046]\n",
            "loss: 0.009437  [118440/170046]\n",
            "loss: 0.007593  [120960/170046]\n",
            "loss: 0.008134  [123480/170046]\n",
            "loss: 0.007509  [126000/170046]\n",
            "loss: 0.013411  [128520/170046]\n",
            "loss: 0.009537  [131040/170046]\n",
            "loss: 0.012769  [133560/170046]\n",
            "loss: 0.011579  [136080/170046]\n",
            "loss: 0.008792  [138600/170046]\n",
            "loss: 0.010624  [141120/170046]\n",
            "loss: 0.008775  [143640/170046]\n",
            "loss: 0.006760  [146160/170046]\n",
            "loss: 0.013169  [148680/170046]\n",
            "loss: 0.014152  [151200/170046]\n",
            "loss: 0.010100  [153720/170046]\n",
            "loss: 0.008370  [156240/170046]\n",
            "loss: 0.010817  [158760/170046]\n",
            "loss: 0.008386  [161280/170046]\n",
            "loss: 0.007746  [163800/170046]\n",
            "loss: 0.007212  [166320/170046]\n",
            "loss: 0.009366  [168840/170046]\n",
            "chorus depth: avg MSE: 0.011508, avg abs error: 0.0797\n",
            "learning rate: 0.000910 -> 0.000820\n",
            "---------------------------\n",
            "\n",
            "Epoch 3\n",
            "loss: 0.010903  [  0/170046]\n",
            "loss: 0.007133  [2520/170046]\n",
            "loss: 0.008554  [5040/170046]\n",
            "loss: 0.006888  [7560/170046]\n",
            "loss: 0.009201  [10080/170046]\n",
            "loss: 0.009209  [12600/170046]\n",
            "loss: 0.009909  [15120/170046]\n",
            "loss: 0.008489  [17640/170046]\n",
            "loss: 0.013363  [20160/170046]\n",
            "loss: 0.007101  [22680/170046]\n",
            "loss: 0.010364  [25200/170046]\n",
            "loss: 0.006159  [27720/170046]\n",
            "loss: 0.006017  [30240/170046]\n",
            "loss: 0.009167  [32760/170046]\n",
            "loss: 0.006550  [35280/170046]\n",
            "loss: 0.009140  [37800/170046]\n",
            "loss: 0.007241  [40320/170046]\n",
            "loss: 0.006665  [42840/170046]\n",
            "loss: 0.011072  [45360/170046]\n",
            "loss: 0.006415  [47880/170046]\n",
            "loss: 0.009733  [50400/170046]\n",
            "loss: 0.010807  [52920/170046]\n",
            "loss: 0.006452  [55440/170046]\n",
            "loss: 0.007895  [57960/170046]\n",
            "loss: 0.006827  [60480/170046]\n",
            "loss: 0.007329  [63000/170046]\n",
            "loss: 0.011550  [65520/170046]\n",
            "loss: 0.008102  [68040/170046]\n",
            "loss: 0.009265  [70560/170046]\n",
            "loss: 0.008764  [73080/170046]\n",
            "loss: 0.008813  [75600/170046]\n",
            "loss: 0.010129  [78120/170046]\n",
            "loss: 0.007269  [80640/170046]\n",
            "loss: 0.012501  [83160/170046]\n",
            "loss: 0.008695  [85680/170046]\n",
            "loss: 0.007566  [88200/170046]\n",
            "loss: 0.006241  [90720/170046]\n",
            "loss: 0.010063  [93240/170046]\n",
            "loss: 0.011725  [95760/170046]\n",
            "loss: 0.008525  [98280/170046]\n",
            "loss: 0.006691  [100800/170046]\n",
            "loss: 0.006799  [103320/170046]\n",
            "loss: 0.007166  [105840/170046]\n",
            "loss: 0.006835  [108360/170046]\n",
            "loss: 0.008383  [110880/170046]\n",
            "loss: 0.006757  [113400/170046]\n",
            "loss: 0.008238  [115920/170046]\n",
            "loss: 0.007556  [118440/170046]\n",
            "loss: 0.005359  [120960/170046]\n",
            "loss: 0.008411  [123480/170046]\n",
            "loss: 0.011485  [126000/170046]\n",
            "loss: 0.008473  [128520/170046]\n",
            "loss: 0.007902  [131040/170046]\n",
            "loss: 0.006426  [133560/170046]\n",
            "loss: 0.005435  [136080/170046]\n",
            "loss: 0.005285  [138600/170046]\n",
            "loss: 0.007569  [141120/170046]\n",
            "loss: 0.006844  [143640/170046]\n",
            "loss: 0.009192  [146160/170046]\n",
            "loss: 0.008159  [148680/170046]\n",
            "loss: 0.006600  [151200/170046]\n",
            "loss: 0.012642  [153720/170046]\n",
            "loss: 0.007979  [156240/170046]\n",
            "loss: 0.009112  [158760/170046]\n",
            "loss: 0.005862  [161280/170046]\n",
            "loss: 0.005673  [163800/170046]\n",
            "loss: 0.009350  [166320/170046]\n",
            "loss: 0.010568  [168840/170046]\n",
            "chorus depth: avg MSE: 0.007417, avg abs error: 0.0623\n",
            "learning rate: 0.000820 -> 0.000730\n",
            "---------------------------\n",
            "\n",
            "Epoch 4\n",
            "loss: 0.007798  [  0/170046]\n",
            "loss: 0.011618  [2520/170046]\n",
            "loss: 0.006738  [5040/170046]\n",
            "loss: 0.005971  [7560/170046]\n",
            "loss: 0.007409  [10080/170046]\n",
            "loss: 0.007943  [12600/170046]\n",
            "loss: 0.005123  [15120/170046]\n",
            "loss: 0.005450  [17640/170046]\n",
            "loss: 0.009643  [20160/170046]\n",
            "loss: 0.007381  [22680/170046]\n",
            "loss: 0.008104  [25200/170046]\n",
            "loss: 0.007897  [27720/170046]\n",
            "loss: 0.007514  [30240/170046]\n",
            "loss: 0.008573  [32760/170046]\n",
            "loss: 0.007223  [35280/170046]\n",
            "loss: 0.005999  [37800/170046]\n",
            "loss: 0.006351  [40320/170046]\n",
            "loss: 0.006497  [42840/170046]\n",
            "loss: 0.005685  [45360/170046]\n",
            "loss: 0.006158  [47880/170046]\n",
            "loss: 0.006959  [50400/170046]\n",
            "loss: 0.007103  [52920/170046]\n",
            "loss: 0.007362  [55440/170046]\n",
            "loss: 0.008621  [57960/170046]\n",
            "loss: 0.007019  [60480/170046]\n",
            "loss: 0.005177  [63000/170046]\n",
            "loss: 0.009476  [65520/170046]\n",
            "loss: 0.005380  [68040/170046]\n",
            "loss: 0.010357  [70560/170046]\n",
            "loss: 0.006546  [73080/170046]\n",
            "loss: 0.005530  [75600/170046]\n",
            "loss: 0.005468  [78120/170046]\n",
            "loss: 0.006222  [80640/170046]\n",
            "loss: 0.005283  [83160/170046]\n",
            "loss: 0.007211  [85680/170046]\n",
            "loss: 0.005739  [88200/170046]\n",
            "loss: 0.007221  [90720/170046]\n",
            "loss: 0.005346  [93240/170046]\n",
            "loss: 0.004482  [95760/170046]\n",
            "loss: 0.010919  [98280/170046]\n",
            "loss: 0.006993  [100800/170046]\n",
            "loss: 0.009087  [103320/170046]\n",
            "loss: 0.006201  [105840/170046]\n",
            "loss: 0.005371  [108360/170046]\n",
            "loss: 0.006960  [110880/170046]\n",
            "loss: 0.005897  [113400/170046]\n",
            "loss: 0.008004  [115920/170046]\n",
            "loss: 0.004634  [118440/170046]\n",
            "loss: 0.008118  [120960/170046]\n",
            "loss: 0.005944  [123480/170046]\n",
            "loss: 0.005860  [126000/170046]\n",
            "loss: 0.006679  [128520/170046]\n",
            "loss: 0.006759  [131040/170046]\n",
            "loss: 0.005156  [133560/170046]\n",
            "loss: 0.006830  [136080/170046]\n",
            "loss: 0.004397  [138600/170046]\n",
            "loss: 0.005691  [141120/170046]\n",
            "loss: 0.005203  [143640/170046]\n",
            "loss: 0.006131  [146160/170046]\n",
            "loss: 0.006080  [148680/170046]\n",
            "loss: 0.007861  [151200/170046]\n",
            "loss: 0.010643  [153720/170046]\n",
            "loss: 0.008635  [156240/170046]\n",
            "loss: 0.006079  [158760/170046]\n",
            "loss: 0.007655  [161280/170046]\n",
            "loss: 0.012595  [163800/170046]\n",
            "loss: 0.008448  [166320/170046]\n",
            "loss: 0.004960  [168840/170046]\n",
            "chorus depth: avg MSE: 0.006333, avg abs error: 0.0577\n",
            "learning rate: 0.000730 -> 0.000640\n",
            "---------------------------\n",
            "\n",
            "Epoch 5\n",
            "loss: 0.004627  [  0/170046]\n",
            "loss: 0.005069  [2520/170046]\n",
            "loss: 0.005056  [5040/170046]\n",
            "loss: 0.004229  [7560/170046]\n",
            "loss: 0.007296  [10080/170046]\n",
            "loss: 0.004034  [12600/170046]\n",
            "loss: 0.006239  [15120/170046]\n",
            "loss: 0.008617  [17640/170046]\n",
            "loss: 0.006879  [20160/170046]\n",
            "loss: 0.005359  [22680/170046]\n",
            "loss: 0.005390  [25200/170046]\n",
            "loss: 0.005936  [27720/170046]\n",
            "loss: 0.004523  [30240/170046]\n",
            "loss: 0.006242  [32760/170046]\n",
            "loss: 0.006540  [35280/170046]\n",
            "loss: 0.004924  [37800/170046]\n",
            "loss: 0.007641  [40320/170046]\n",
            "loss: 0.003988  [42840/170046]\n",
            "loss: 0.004698  [45360/170046]\n",
            "loss: 0.005907  [47880/170046]\n",
            "loss: 0.005546  [50400/170046]\n",
            "loss: 0.005876  [52920/170046]\n",
            "loss: 0.006033  [55440/170046]\n",
            "loss: 0.005578  [57960/170046]\n",
            "loss: 0.007348  [60480/170046]\n",
            "loss: 0.004639  [63000/170046]\n",
            "loss: 0.013808  [65520/170046]\n",
            "loss: 0.005179  [68040/170046]\n",
            "loss: 0.007122  [70560/170046]\n",
            "loss: 0.006330  [73080/170046]\n",
            "loss: 0.006662  [75600/170046]\n",
            "loss: 0.004388  [78120/170046]\n",
            "loss: 0.004212  [80640/170046]\n",
            "loss: 0.004590  [83160/170046]\n",
            "loss: 0.005523  [85680/170046]\n",
            "loss: 0.007135  [88200/170046]\n",
            "loss: 0.007177  [90720/170046]\n",
            "loss: 0.007391  [93240/170046]\n",
            "loss: 0.004946  [95760/170046]\n",
            "loss: 0.008322  [98280/170046]\n",
            "loss: 0.005640  [100800/170046]\n",
            "loss: 0.005565  [103320/170046]\n",
            "loss: 0.004867  [105840/170046]\n",
            "loss: 0.004400  [108360/170046]\n",
            "loss: 0.006316  [110880/170046]\n",
            "loss: 0.006954  [113400/170046]\n",
            "loss: 0.005472  [115920/170046]\n",
            "loss: 0.005713  [118440/170046]\n",
            "loss: 0.006111  [120960/170046]\n",
            "loss: 0.004398  [123480/170046]\n",
            "loss: 0.006176  [126000/170046]\n",
            "loss: 0.005329  [128520/170046]\n",
            "loss: 0.006445  [131040/170046]\n",
            "loss: 0.005252  [133560/170046]\n",
            "loss: 0.006328  [136080/170046]\n",
            "loss: 0.009728  [138600/170046]\n",
            "loss: 0.006485  [141120/170046]\n",
            "loss: 0.004739  [143640/170046]\n",
            "loss: 0.004644  [146160/170046]\n",
            "loss: 0.005216  [148680/170046]\n",
            "loss: 0.004890  [151200/170046]\n",
            "loss: 0.003205  [153720/170046]\n",
            "loss: 0.005905  [156240/170046]\n",
            "loss: 0.005865  [158760/170046]\n",
            "loss: 0.006328  [161280/170046]\n",
            "loss: 0.005668  [163800/170046]\n",
            "loss: 0.004751  [166320/170046]\n",
            "loss: 0.007031  [168840/170046]\n",
            "chorus depth: avg MSE: 0.007571, avg abs error: 0.0655\n",
            "learning rate: 0.000640 -> 0.000550\n",
            "---------------------------\n",
            "\n",
            "Epoch 6\n",
            "loss: 0.003406  [  0/170046]\n",
            "loss: 0.005627  [2520/170046]\n",
            "loss: 0.005151  [5040/170046]\n",
            "loss: 0.005793  [7560/170046]\n",
            "loss: 0.004766  [10080/170046]\n",
            "loss: 0.004998  [12600/170046]\n",
            "loss: 0.005175  [15120/170046]\n",
            "loss: 0.004484  [17640/170046]\n",
            "loss: 0.004782  [20160/170046]\n",
            "loss: 0.005600  [22680/170046]\n",
            "loss: 0.004553  [25200/170046]\n",
            "loss: 0.005338  [27720/170046]\n",
            "loss: 0.005478  [30240/170046]\n",
            "loss: 0.005979  [32760/170046]\n",
            "loss: 0.004744  [35280/170046]\n",
            "loss: 0.004680  [37800/170046]\n",
            "loss: 0.008044  [40320/170046]\n",
            "loss: 0.005650  [42840/170046]\n",
            "loss: 0.004407  [45360/170046]\n",
            "loss: 0.008358  [47880/170046]\n",
            "loss: 0.005299  [50400/170046]\n",
            "loss: 0.004291  [52920/170046]\n",
            "loss: 0.003807  [55440/170046]\n",
            "loss: 0.008808  [57960/170046]\n",
            "loss: 0.004168  [60480/170046]\n",
            "loss: 0.006584  [63000/170046]\n",
            "loss: 0.003844  [65520/170046]\n",
            "loss: 0.005335  [68040/170046]\n",
            "loss: 0.006500  [70560/170046]\n",
            "loss: 0.006750  [73080/170046]\n",
            "loss: 0.006548  [75600/170046]\n",
            "loss: 0.004662  [78120/170046]\n",
            "loss: 0.004017  [80640/170046]\n",
            "loss: 0.004324  [83160/170046]\n",
            "loss: 0.004230  [85680/170046]\n",
            "loss: 0.007339  [88200/170046]\n",
            "loss: 0.006063  [90720/170046]\n",
            "loss: 0.005072  [93240/170046]\n",
            "loss: 0.005330  [95760/170046]\n",
            "loss: 0.004338  [98280/170046]\n",
            "loss: 0.004215  [100800/170046]\n",
            "loss: 0.005031  [103320/170046]\n",
            "loss: 0.009759  [105840/170046]\n",
            "loss: 0.004072  [108360/170046]\n",
            "loss: 0.004878  [110880/170046]\n",
            "loss: 0.005192  [113400/170046]\n",
            "loss: 0.005021  [115920/170046]\n",
            "loss: 0.005103  [118440/170046]\n",
            "loss: 0.004945  [120960/170046]\n",
            "loss: 0.004949  [123480/170046]\n",
            "loss: 0.004770  [126000/170046]\n",
            "loss: 0.004234  [128520/170046]\n",
            "loss: 0.003611  [131040/170046]\n",
            "loss: 0.005756  [133560/170046]\n",
            "loss: 0.003694  [136080/170046]\n",
            "loss: 0.005866  [138600/170046]\n",
            "loss: 0.005178  [141120/170046]\n",
            "loss: 0.005943  [143640/170046]\n",
            "loss: 0.005252  [146160/170046]\n",
            "loss: 0.003854  [148680/170046]\n",
            "loss: 0.004770  [151200/170046]\n",
            "loss: 0.004554  [153720/170046]\n",
            "loss: 0.005130  [156240/170046]\n",
            "loss: 0.004263  [158760/170046]\n",
            "loss: 0.003480  [161280/170046]\n",
            "loss: 0.005915  [163800/170046]\n",
            "loss: 0.008024  [166320/170046]\n",
            "loss: 0.005272  [168840/170046]\n",
            "chorus depth: avg MSE: 0.005836, avg abs error: 0.0555\n",
            "learning rate: 0.000550 -> 0.000460\n",
            "---------------------------\n",
            "\n",
            "Epoch 7\n",
            "loss: 0.003323  [  0/170046]\n",
            "loss: 0.004413  [2520/170046]\n",
            "loss: 0.004925  [5040/170046]\n",
            "loss: 0.004390  [7560/170046]\n",
            "loss: 0.004943  [10080/170046]\n",
            "loss: 0.005127  [12600/170046]\n",
            "loss: 0.003992  [15120/170046]\n",
            "loss: 0.006066  [17640/170046]\n",
            "loss: 0.004057  [20160/170046]\n",
            "loss: 0.005059  [22680/170046]\n",
            "loss: 0.004746  [25200/170046]\n",
            "loss: 0.004848  [27720/170046]\n",
            "loss: 0.004197  [30240/170046]\n",
            "loss: 0.003534  [32760/170046]\n",
            "loss: 0.004234  [35280/170046]\n",
            "loss: 0.004904  [37800/170046]\n",
            "loss: 0.003200  [40320/170046]\n",
            "loss: 0.003939  [42840/170046]\n",
            "loss: 0.004635  [45360/170046]\n",
            "loss: 0.004759  [47880/170046]\n",
            "loss: 0.003889  [50400/170046]\n",
            "loss: 0.007283  [52920/170046]\n",
            "loss: 0.004262  [55440/170046]\n",
            "loss: 0.005317  [57960/170046]\n",
            "loss: 0.004209  [60480/170046]\n",
            "loss: 0.003535  [63000/170046]\n",
            "loss: 0.005343  [65520/170046]\n",
            "loss: 0.004473  [68040/170046]\n",
            "loss: 0.003475  [70560/170046]\n",
            "loss: 0.004737  [73080/170046]\n",
            "loss: 0.004291  [75600/170046]\n",
            "loss: 0.004720  [78120/170046]\n",
            "loss: 0.005179  [80640/170046]\n",
            "loss: 0.004619  [83160/170046]\n",
            "loss: 0.003565  [85680/170046]\n",
            "loss: 0.004435  [88200/170046]\n",
            "loss: 0.005808  [90720/170046]\n",
            "loss: 0.005108  [93240/170046]\n",
            "loss: 0.005010  [95760/170046]\n",
            "loss: 0.005828  [98280/170046]\n",
            "loss: 0.004820  [100800/170046]\n",
            "loss: 0.003470  [103320/170046]\n",
            "loss: 0.003716  [105840/170046]\n",
            "loss: 0.006372  [108360/170046]\n",
            "loss: 0.005606  [110880/170046]\n",
            "loss: 0.004226  [113400/170046]\n",
            "loss: 0.003455  [115920/170046]\n",
            "loss: 0.005585  [118440/170046]\n",
            "loss: 0.005679  [120960/170046]\n",
            "loss: 0.004343  [123480/170046]\n",
            "loss: 0.007339  [126000/170046]\n",
            "loss: 0.003283  [128520/170046]\n",
            "loss: 0.004656  [131040/170046]\n",
            "loss: 0.005005  [133560/170046]\n",
            "loss: 0.005033  [136080/170046]\n",
            "loss: 0.004873  [138600/170046]\n",
            "loss: 0.005331  [141120/170046]\n",
            "loss: 0.005260  [143640/170046]\n",
            "loss: 0.004922  [146160/170046]\n",
            "loss: 0.006653  [148680/170046]\n",
            "loss: 0.003646  [151200/170046]\n",
            "loss: 0.004934  [153720/170046]\n",
            "loss: 0.004729  [156240/170046]\n",
            "loss: 0.003512  [158760/170046]\n",
            "loss: 0.003679  [161280/170046]\n",
            "loss: 0.006308  [163800/170046]\n",
            "loss: 0.004695  [166320/170046]\n",
            "loss: 0.003824  [168840/170046]\n",
            "chorus depth: avg MSE: 0.005884, avg abs error: 0.0557\n",
            "learning rate: 0.000460 -> 0.000370\n",
            "---------------------------\n",
            "\n",
            "Epoch 8\n",
            "loss: 0.004728  [  0/170046]\n",
            "loss: 0.004198  [2520/170046]\n",
            "loss: 0.004296  [5040/170046]\n",
            "loss: 0.005188  [7560/170046]\n",
            "loss: 0.003101  [10080/170046]\n",
            "loss: 0.004051  [12600/170046]\n",
            "loss: 0.002777  [15120/170046]\n",
            "loss: 0.003620  [17640/170046]\n",
            "loss: 0.003854  [20160/170046]\n",
            "loss: 0.004695  [22680/170046]\n",
            "loss: 0.003373  [25200/170046]\n",
            "loss: 0.003751  [27720/170046]\n",
            "loss: 0.004069  [30240/170046]\n",
            "loss: 0.003710  [32760/170046]\n",
            "loss: 0.003448  [35280/170046]\n",
            "loss: 0.004538  [37800/170046]\n",
            "loss: 0.003649  [40320/170046]\n",
            "loss: 0.004385  [42840/170046]\n",
            "loss: 0.005508  [45360/170046]\n",
            "loss: 0.005104  [47880/170046]\n",
            "loss: 0.003971  [50400/170046]\n",
            "loss: 0.003953  [52920/170046]\n",
            "loss: 0.003785  [55440/170046]\n",
            "loss: 0.003494  [57960/170046]\n",
            "loss: 0.004222  [60480/170046]\n",
            "loss: 0.004682  [63000/170046]\n",
            "loss: 0.002637  [65520/170046]\n",
            "loss: 0.003204  [68040/170046]\n",
            "loss: 0.004512  [70560/170046]\n",
            "loss: 0.004733  [73080/170046]\n",
            "loss: 0.005293  [75600/170046]\n",
            "loss: 0.003404  [78120/170046]\n",
            "loss: 0.004977  [80640/170046]\n",
            "loss: 0.004235  [83160/170046]\n",
            "loss: 0.003458  [85680/170046]\n",
            "loss: 0.004289  [88200/170046]\n",
            "loss: 0.003264  [90720/170046]\n",
            "loss: 0.003454  [93240/170046]\n",
            "loss: 0.004042  [95760/170046]\n",
            "loss: 0.004964  [98280/170046]\n",
            "loss: 0.005499  [100800/170046]\n",
            "loss: 0.003903  [103320/170046]\n",
            "loss: 0.004238  [105840/170046]\n",
            "loss: 0.004776  [108360/170046]\n",
            "loss: 0.003681  [110880/170046]\n",
            "loss: 0.003039  [113400/170046]\n",
            "loss: 0.004431  [115920/170046]\n",
            "loss: 0.005171  [118440/170046]\n",
            "loss: 0.004364  [120960/170046]\n",
            "loss: 0.003740  [123480/170046]\n",
            "loss: 0.004187  [126000/170046]\n",
            "loss: 0.004061  [128520/170046]\n",
            "loss: 0.004335  [131040/170046]\n",
            "loss: 0.005472  [133560/170046]\n",
            "loss: 0.003660  [136080/170046]\n",
            "loss: 0.003746  [138600/170046]\n",
            "loss: 0.002600  [141120/170046]\n",
            "loss: 0.003962  [143640/170046]\n",
            "loss: 0.005458  [146160/170046]\n",
            "loss: 0.004546  [148680/170046]\n",
            "loss: 0.004933  [151200/170046]\n",
            "loss: 0.002802  [153720/170046]\n",
            "loss: 0.004315  [156240/170046]\n",
            "loss: 0.005354  [158760/170046]\n",
            "loss: 0.004867  [161280/170046]\n",
            "loss: 0.004407  [163800/170046]\n",
            "loss: 0.003182  [166320/170046]\n",
            "loss: 0.004010  [168840/170046]\n",
            "chorus depth: avg MSE: 0.005004, avg abs error: 0.0505\n",
            "learning rate: 0.000370 -> 0.000280\n",
            "---------------------------\n",
            "\n",
            "Epoch 9\n",
            "loss: 0.004950  [  0/170046]\n",
            "loss: 0.002871  [2520/170046]\n",
            "loss: 0.002535  [5040/170046]\n",
            "loss: 0.003657  [7560/170046]\n",
            "loss: 0.002731  [10080/170046]\n",
            "loss: 0.004520  [12600/170046]\n",
            "loss: 0.002754  [15120/170046]\n",
            "loss: 0.003579  [17640/170046]\n",
            "loss: 0.003946  [20160/170046]\n",
            "loss: 0.002838  [22680/170046]\n",
            "loss: 0.003261  [25200/170046]\n",
            "loss: 0.004040  [27720/170046]\n",
            "loss: 0.003318  [30240/170046]\n",
            "loss: 0.004280  [32760/170046]\n",
            "loss: 0.003648  [35280/170046]\n",
            "loss: 0.003188  [37800/170046]\n",
            "loss: 0.004500  [40320/170046]\n",
            "loss: 0.004024  [42840/170046]\n",
            "loss: 0.004904  [45360/170046]\n",
            "loss: 0.003351  [47880/170046]\n",
            "loss: 0.003850  [50400/170046]\n",
            "loss: 0.003982  [52920/170046]\n",
            "loss: 0.003910  [55440/170046]\n",
            "loss: 0.002684  [57960/170046]\n",
            "loss: 0.004041  [60480/170046]\n",
            "loss: 0.002397  [63000/170046]\n",
            "loss: 0.002987  [65520/170046]\n",
            "loss: 0.003672  [68040/170046]\n",
            "loss: 0.004369  [70560/170046]\n",
            "loss: 0.004744  [73080/170046]\n",
            "loss: 0.003823  [75600/170046]\n",
            "loss: 0.003203  [78120/170046]\n",
            "loss: 0.003625  [80640/170046]\n",
            "loss: 0.005231  [83160/170046]\n",
            "loss: 0.003142  [85680/170046]\n",
            "loss: 0.004055  [88200/170046]\n",
            "loss: 0.005396  [90720/170046]\n",
            "loss: 0.003278  [93240/170046]\n",
            "loss: 0.003725  [95760/170046]\n",
            "loss: 0.003349  [98280/170046]\n",
            "loss: 0.003761  [100800/170046]\n",
            "loss: 0.003453  [103320/170046]\n",
            "loss: 0.003910  [105840/170046]\n",
            "loss: 0.005760  [108360/170046]\n",
            "loss: 0.003717  [110880/170046]\n",
            "loss: 0.004048  [113400/170046]\n",
            "loss: 0.004015  [115920/170046]\n",
            "loss: 0.003916  [118440/170046]\n",
            "loss: 0.003776  [120960/170046]\n",
            "loss: 0.005329  [123480/170046]\n",
            "loss: 0.003590  [126000/170046]\n",
            "loss: 0.002661  [128520/170046]\n",
            "loss: 0.004826  [131040/170046]\n",
            "loss: 0.004139  [133560/170046]\n",
            "loss: 0.004232  [136080/170046]\n",
            "loss: 0.003667  [138600/170046]\n",
            "loss: 0.003284  [141120/170046]\n",
            "loss: 0.003396  [143640/170046]\n",
            "loss: 0.003677  [146160/170046]\n",
            "loss: 0.004715  [148680/170046]\n",
            "loss: 0.004156  [151200/170046]\n",
            "loss: 0.006043  [153720/170046]\n",
            "loss: 0.003135  [156240/170046]\n",
            "loss: 0.004455  [158760/170046]\n",
            "loss: 0.003630  [161280/170046]\n",
            "loss: 0.004490  [163800/170046]\n",
            "loss: 0.003518  [166320/170046]\n",
            "loss: 0.004020  [168840/170046]\n",
            "chorus depth: avg MSE: 0.004683, avg abs error: 0.0493\n",
            "learning rate: 0.000280 -> 0.000190\n",
            "---------------------------\n",
            "\n",
            "Epoch 10\n",
            "loss: 0.003393  [  0/170046]\n",
            "loss: 0.003976  [2520/170046]\n",
            "loss: 0.002531  [5040/170046]\n",
            "loss: 0.004149  [7560/170046]\n",
            "loss: 0.003922  [10080/170046]\n",
            "loss: 0.003846  [12600/170046]\n",
            "loss: 0.003344  [15120/170046]\n",
            "loss: 0.002694  [17640/170046]\n",
            "loss: 0.003690  [20160/170046]\n",
            "loss: 0.003053  [22680/170046]\n",
            "loss: 0.003564  [25200/170046]\n",
            "loss: 0.003630  [27720/170046]\n",
            "loss: 0.004802  [30240/170046]\n",
            "loss: 0.002893  [32760/170046]\n",
            "loss: 0.003200  [35280/170046]\n",
            "loss: 0.003043  [37800/170046]\n",
            "loss: 0.003454  [40320/170046]\n",
            "loss: 0.005419  [42840/170046]\n",
            "loss: 0.002741  [45360/170046]\n",
            "loss: 0.003191  [47880/170046]\n",
            "loss: 0.003754  [50400/170046]\n",
            "loss: 0.002777  [52920/170046]\n",
            "loss: 0.003395  [55440/170046]\n",
            "loss: 0.002308  [57960/170046]\n",
            "loss: 0.003947  [60480/170046]\n",
            "loss: 0.003449  [63000/170046]\n",
            "loss: 0.003788  [65520/170046]\n",
            "loss: 0.003179  [68040/170046]\n",
            "loss: 0.002964  [70560/170046]\n",
            "loss: 0.004117  [73080/170046]\n",
            "loss: 0.003160  [75600/170046]\n",
            "loss: 0.002663  [78120/170046]\n",
            "loss: 0.004609  [80640/170046]\n",
            "loss: 0.004089  [83160/170046]\n",
            "loss: 0.004477  [85680/170046]\n",
            "loss: 0.002731  [88200/170046]\n",
            "loss: 0.002915  [90720/170046]\n",
            "loss: 0.003072  [93240/170046]\n",
            "loss: 0.003576  [95760/170046]\n",
            "loss: 0.003737  [98280/170046]\n",
            "loss: 0.004312  [100800/170046]\n",
            "loss: 0.002439  [103320/170046]\n",
            "loss: 0.003325  [105840/170046]\n",
            "loss: 0.003569  [108360/170046]\n",
            "loss: 0.003272  [110880/170046]\n",
            "loss: 0.002949  [113400/170046]\n",
            "loss: 0.003091  [115920/170046]\n",
            "loss: 0.002769  [118440/170046]\n",
            "loss: 0.004930  [120960/170046]\n",
            "loss: 0.003606  [123480/170046]\n",
            "loss: 0.003281  [126000/170046]\n",
            "loss: 0.002184  [128520/170046]\n",
            "loss: 0.003275  [131040/170046]\n",
            "loss: 0.003261  [133560/170046]\n",
            "loss: 0.002656  [136080/170046]\n",
            "loss: 0.002752  [138600/170046]\n",
            "loss: 0.004507  [141120/170046]\n",
            "loss: 0.002975  [143640/170046]\n",
            "loss: 0.004251  [146160/170046]\n",
            "loss: 0.002914  [148680/170046]\n",
            "loss: 0.003067  [151200/170046]\n",
            "loss: 0.002833  [153720/170046]\n",
            "loss: 0.003457  [156240/170046]\n",
            "loss: 0.003030  [158760/170046]\n",
            "loss: 0.004094  [161280/170046]\n",
            "loss: 0.002289  [163800/170046]\n",
            "loss: 0.002337  [166320/170046]\n",
            "loss: 0.003463  [168840/170046]\n",
            "chorus depth: avg MSE: 0.005864, avg abs error: 0.0563\n",
            "learning rate: 0.000190 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 11\n",
            "loss: 0.004356  [  0/170046]\n",
            "loss: 0.002171  [2520/170046]\n",
            "loss: 0.002832  [5040/170046]\n",
            "loss: 0.003197  [7560/170046]\n",
            "loss: 0.003126  [10080/170046]\n",
            "loss: 0.002401  [12600/170046]\n",
            "loss: 0.004130  [15120/170046]\n",
            "loss: 0.002409  [17640/170046]\n",
            "loss: 0.002692  [20160/170046]\n",
            "loss: 0.002375  [22680/170046]\n",
            "loss: 0.002734  [25200/170046]\n",
            "loss: 0.003549  [27720/170046]\n",
            "loss: 0.003299  [30240/170046]\n",
            "loss: 0.002282  [32760/170046]\n",
            "loss: 0.003227  [35280/170046]\n",
            "loss: 0.003809  [37800/170046]\n",
            "loss: 0.002872  [40320/170046]\n",
            "loss: 0.002861  [42840/170046]\n",
            "loss: 0.003368  [45360/170046]\n",
            "loss: 0.003164  [47880/170046]\n",
            "loss: 0.003295  [50400/170046]\n",
            "loss: 0.003392  [52920/170046]\n",
            "loss: 0.003247  [55440/170046]\n",
            "loss: 0.002694  [57960/170046]\n",
            "loss: 0.003710  [60480/170046]\n",
            "loss: 0.002954  [63000/170046]\n",
            "loss: 0.002686  [65520/170046]\n",
            "loss: 0.002807  [68040/170046]\n",
            "loss: 0.002934  [70560/170046]\n",
            "loss: 0.002926  [73080/170046]\n",
            "loss: 0.003336  [75600/170046]\n",
            "loss: 0.002567  [78120/170046]\n",
            "loss: 0.003203  [80640/170046]\n",
            "loss: 0.003062  [83160/170046]\n",
            "loss: 0.003307  [85680/170046]\n",
            "loss: 0.003358  [88200/170046]\n",
            "loss: 0.002605  [90720/170046]\n",
            "loss: 0.002878  [93240/170046]\n",
            "loss: 0.002399  [95760/170046]\n",
            "loss: 0.002890  [98280/170046]\n",
            "loss: 0.003037  [100800/170046]\n",
            "loss: 0.002845  [103320/170046]\n",
            "loss: 0.002257  [105840/170046]\n",
            "loss: 0.002820  [108360/170046]\n",
            "loss: 0.002201  [110880/170046]\n",
            "loss: 0.003854  [113400/170046]\n",
            "loss: 0.004271  [115920/170046]\n",
            "loss: 0.002855  [118440/170046]\n",
            "loss: 0.002497  [120960/170046]\n",
            "loss: 0.002528  [123480/170046]\n",
            "loss: 0.002660  [126000/170046]\n",
            "loss: 0.002552  [128520/170046]\n",
            "loss: 0.002773  [131040/170046]\n",
            "loss: 0.003394  [133560/170046]\n",
            "loss: 0.002677  [136080/170046]\n",
            "loss: 0.003412  [138600/170046]\n",
            "loss: 0.003043  [141120/170046]\n",
            "loss: 0.002756  [143640/170046]\n",
            "loss: 0.002926  [146160/170046]\n",
            "loss: 0.003109  [148680/170046]\n",
            "loss: 0.002722  [151200/170046]\n",
            "loss: 0.003082  [153720/170046]\n",
            "loss: 0.002280  [156240/170046]\n",
            "loss: 0.002868  [158760/170046]\n",
            "loss: 0.002169  [161280/170046]\n",
            "loss: 0.002736  [163800/170046]\n",
            "loss: 0.003352  [166320/170046]\n",
            "loss: 0.003091  [168840/170046]\n",
            "chorus depth: avg MSE: 0.004667, avg abs error: 0.0493\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 12\n",
            "loss: 0.002788  [  0/170046]\n",
            "loss: 0.002047  [2520/170046]\n",
            "loss: 0.003704  [5040/170046]\n",
            "loss: 0.003337  [7560/170046]\n",
            "loss: 0.002253  [10080/170046]\n",
            "loss: 0.003404  [12600/170046]\n",
            "loss: 0.003499  [15120/170046]\n",
            "loss: 0.002958  [17640/170046]\n",
            "loss: 0.002804  [20160/170046]\n",
            "loss: 0.003268  [22680/170046]\n",
            "loss: 0.002570  [25200/170046]\n",
            "loss: 0.002731  [27720/170046]\n",
            "loss: 0.003229  [30240/170046]\n",
            "loss: 0.003216  [32760/170046]\n",
            "loss: 0.002290  [35280/170046]\n",
            "loss: 0.002081  [37800/170046]\n",
            "loss: 0.002363  [40320/170046]\n",
            "loss: 0.002325  [42840/170046]\n",
            "loss: 0.003247  [45360/170046]\n",
            "loss: 0.003129  [47880/170046]\n",
            "loss: 0.002218  [50400/170046]\n",
            "loss: 0.002842  [52920/170046]\n",
            "loss: 0.002605  [55440/170046]\n",
            "loss: 0.004381  [57960/170046]\n",
            "loss: 0.002902  [60480/170046]\n",
            "loss: 0.003083  [63000/170046]\n",
            "loss: 0.003049  [65520/170046]\n",
            "loss: 0.003469  [68040/170046]\n",
            "loss: 0.002579  [70560/170046]\n",
            "loss: 0.003216  [73080/170046]\n",
            "loss: 0.002217  [75600/170046]\n",
            "loss: 0.003413  [78120/170046]\n",
            "loss: 0.002127  [80640/170046]\n",
            "loss: 0.002805  [83160/170046]\n",
            "loss: 0.002635  [85680/170046]\n",
            "loss: 0.001650  [88200/170046]\n",
            "loss: 0.002473  [90720/170046]\n",
            "loss: 0.003145  [93240/170046]\n",
            "loss: 0.002414  [95760/170046]\n",
            "loss: 0.002778  [98280/170046]\n",
            "loss: 0.002860  [100800/170046]\n",
            "loss: 0.003200  [103320/170046]\n",
            "loss: 0.002932  [105840/170046]\n",
            "loss: 0.002587  [108360/170046]\n",
            "loss: 0.002688  [110880/170046]\n",
            "loss: 0.003643  [113400/170046]\n",
            "loss: 0.003232  [115920/170046]\n",
            "loss: 0.001724  [118440/170046]\n",
            "loss: 0.003485  [120960/170046]\n",
            "loss: 0.003005  [123480/170046]\n",
            "loss: 0.003800  [126000/170046]\n",
            "loss: 0.003087  [128520/170046]\n",
            "loss: 0.003034  [131040/170046]\n",
            "loss: 0.002089  [133560/170046]\n",
            "loss: 0.003302  [136080/170046]\n",
            "loss: 0.002959  [138600/170046]\n",
            "loss: 0.002589  [141120/170046]\n",
            "loss: 0.003262  [143640/170046]\n",
            "loss: 0.003184  [146160/170046]\n",
            "loss: 0.002488  [148680/170046]\n",
            "loss: 0.002252  [151200/170046]\n",
            "loss: 0.003550  [153720/170046]\n",
            "loss: 0.003179  [156240/170046]\n",
            "loss: 0.002873  [158760/170046]\n",
            "loss: 0.002341  [161280/170046]\n",
            "loss: 0.002371  [163800/170046]\n",
            "loss: 0.003169  [166320/170046]\n",
            "loss: 0.002302  [168840/170046]\n",
            "chorus depth: avg MSE: 0.004479, avg abs error: 0.048\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 13\n",
            "loss: 0.003056  [  0/170046]\n",
            "loss: 0.003412  [2520/170046]\n",
            "loss: 0.002981  [5040/170046]\n",
            "loss: 0.002525  [7560/170046]\n",
            "loss: 0.002698  [10080/170046]\n",
            "loss: 0.002011  [12600/170046]\n",
            "loss: 0.002616  [15120/170046]\n",
            "loss: 0.001924  [17640/170046]\n",
            "loss: 0.003220  [20160/170046]\n",
            "loss: 0.002381  [22680/170046]\n",
            "loss: 0.002576  [25200/170046]\n",
            "loss: 0.002438  [27720/170046]\n",
            "loss: 0.002673  [30240/170046]\n",
            "loss: 0.002763  [32760/170046]\n",
            "loss: 0.001741  [35280/170046]\n",
            "loss: 0.002342  [37800/170046]\n",
            "loss: 0.002132  [40320/170046]\n",
            "loss: 0.002507  [42840/170046]\n",
            "loss: 0.002096  [45360/170046]\n",
            "loss: 0.003016  [47880/170046]\n",
            "loss: 0.003202  [50400/170046]\n",
            "loss: 0.002719  [52920/170046]\n",
            "loss: 0.003016  [55440/170046]\n",
            "loss: 0.003341  [57960/170046]\n",
            "loss: 0.001527  [60480/170046]\n",
            "loss: 0.002952  [63000/170046]\n",
            "loss: 0.002431  [65520/170046]\n",
            "loss: 0.002686  [68040/170046]\n",
            "loss: 0.003713  [70560/170046]\n",
            "loss: 0.002476  [73080/170046]\n",
            "loss: 0.003151  [75600/170046]\n",
            "loss: 0.002306  [78120/170046]\n",
            "loss: 0.003040  [80640/170046]\n",
            "loss: 0.002288  [83160/170046]\n",
            "loss: 0.002280  [85680/170046]\n",
            "loss: 0.003133  [88200/170046]\n",
            "loss: 0.002935  [90720/170046]\n",
            "loss: 0.002305  [93240/170046]\n",
            "loss: 0.002364  [95760/170046]\n",
            "loss: 0.002122  [98280/170046]\n",
            "loss: 0.002703  [100800/170046]\n",
            "loss: 0.002564  [103320/170046]\n",
            "loss: 0.002889  [105840/170046]\n",
            "loss: 0.002831  [108360/170046]\n",
            "loss: 0.002683  [110880/170046]\n",
            "loss: 0.001953  [113400/170046]\n",
            "loss: 0.002522  [115920/170046]\n",
            "loss: 0.002404  [118440/170046]\n",
            "loss: 0.002358  [120960/170046]\n",
            "loss: 0.002143  [123480/170046]\n",
            "loss: 0.002950  [126000/170046]\n",
            "loss: 0.003655  [128520/170046]\n",
            "loss: 0.003004  [131040/170046]\n",
            "loss: 0.002302  [133560/170046]\n",
            "loss: 0.002579  [136080/170046]\n",
            "loss: 0.002523  [138600/170046]\n",
            "loss: 0.001885  [141120/170046]\n",
            "loss: 0.003022  [143640/170046]\n",
            "loss: 0.002707  [146160/170046]\n",
            "loss: 0.002876  [148680/170046]\n",
            "loss: 0.002900  [151200/170046]\n",
            "loss: 0.003590  [153720/170046]\n",
            "loss: 0.002250  [156240/170046]\n",
            "loss: 0.003798  [158760/170046]\n",
            "loss: 0.003184  [161280/170046]\n",
            "loss: 0.002282  [163800/170046]\n",
            "loss: 0.003068  [166320/170046]\n",
            "loss: 0.002917  [168840/170046]\n",
            "chorus depth: avg MSE: 0.004489, avg abs error: 0.0482\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 14\n",
            "loss: 0.002068  [  0/170046]\n",
            "loss: 0.002123  [2520/170046]\n",
            "loss: 0.001966  [5040/170046]\n",
            "loss: 0.002104  [7560/170046]\n",
            "loss: 0.002347  [10080/170046]\n",
            "loss: 0.002077  [12600/170046]\n",
            "loss: 0.003074  [15120/170046]\n",
            "loss: 0.002328  [17640/170046]\n",
            "loss: 0.002718  [20160/170046]\n",
            "loss: 0.003108  [22680/170046]\n",
            "loss: 0.001705  [25200/170046]\n",
            "loss: 0.002780  [27720/170046]\n",
            "loss: 0.002538  [30240/170046]\n",
            "loss: 0.002355  [32760/170046]\n",
            "loss: 0.001989  [35280/170046]\n",
            "loss: 0.002590  [37800/170046]\n",
            "loss: 0.003816  [40320/170046]\n",
            "loss: 0.003239  [42840/170046]\n",
            "loss: 0.003129  [45360/170046]\n",
            "loss: 0.002773  [47880/170046]\n",
            "loss: 0.002738  [50400/170046]\n",
            "loss: 0.002358  [52920/170046]\n",
            "loss: 0.002496  [55440/170046]\n",
            "loss: 0.002818  [57960/170046]\n",
            "loss: 0.002603  [60480/170046]\n",
            "loss: 0.002468  [63000/170046]\n",
            "loss: 0.003443  [65520/170046]\n",
            "loss: 0.002562  [68040/170046]\n",
            "loss: 0.002611  [70560/170046]\n",
            "loss: 0.002738  [73080/170046]\n",
            "loss: 0.003590  [75600/170046]\n",
            "loss: 0.002809  [78120/170046]\n",
            "loss: 0.002010  [80640/170046]\n",
            "loss: 0.003111  [83160/170046]\n",
            "loss: 0.002421  [85680/170046]\n",
            "loss: 0.003302  [88200/170046]\n",
            "loss: 0.003718  [90720/170046]\n",
            "loss: 0.003349  [93240/170046]\n",
            "loss: 0.002562  [95760/170046]\n",
            "loss: 0.003711  [98280/170046]\n",
            "loss: 0.002581  [100800/170046]\n",
            "loss: 0.002366  [103320/170046]\n",
            "loss: 0.004048  [105840/170046]\n",
            "loss: 0.002636  [108360/170046]\n",
            "loss: 0.003025  [110880/170046]\n",
            "loss: 0.002454  [113400/170046]\n",
            "loss: 0.002378  [115920/170046]\n",
            "loss: 0.003369  [118440/170046]\n",
            "loss: 0.003033  [120960/170046]\n",
            "loss: 0.002972  [123480/170046]\n",
            "loss: 0.002246  [126000/170046]\n",
            "loss: 0.003042  [128520/170046]\n",
            "loss: 0.002539  [131040/170046]\n",
            "loss: 0.002922  [133560/170046]\n",
            "loss: 0.003044  [136080/170046]\n",
            "loss: 0.002342  [138600/170046]\n",
            "loss: 0.002588  [141120/170046]\n",
            "loss: 0.002829  [143640/170046]\n",
            "loss: 0.002430  [146160/170046]\n",
            "loss: 0.002659  [148680/170046]\n",
            "loss: 0.003570  [151200/170046]\n",
            "loss: 0.002526  [153720/170046]\n",
            "loss: 0.003029  [156240/170046]\n",
            "loss: 0.002578  [158760/170046]\n",
            "loss: 0.003533  [161280/170046]\n",
            "loss: 0.002814  [163800/170046]\n",
            "loss: 0.002277  [166320/170046]\n",
            "loss: 0.003326  [168840/170046]\n",
            "chorus depth: avg MSE: 0.004452, avg abs error: 0.0479\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 15\n",
            "loss: 0.002871  [  0/170046]\n",
            "loss: 0.002530  [2520/170046]\n",
            "loss: 0.002734  [5040/170046]\n",
            "loss: 0.002085  [7560/170046]\n",
            "loss: 0.002705  [10080/170046]\n",
            "loss: 0.002462  [12600/170046]\n",
            "loss: 0.002504  [15120/170046]\n",
            "loss: 0.003014  [17640/170046]\n",
            "loss: 0.002072  [20160/170046]\n",
            "loss: 0.002240  [22680/170046]\n",
            "loss: 0.002511  [25200/170046]\n",
            "loss: 0.002121  [27720/170046]\n",
            "loss: 0.002844  [30240/170046]\n",
            "loss: 0.003278  [32760/170046]\n",
            "loss: 0.002444  [35280/170046]\n",
            "loss: 0.002227  [37800/170046]\n",
            "loss: 0.002101  [40320/170046]\n",
            "loss: 0.002819  [42840/170046]\n",
            "loss: 0.002224  [45360/170046]\n",
            "loss: 0.003456  [47880/170046]\n",
            "loss: 0.002596  [50400/170046]\n",
            "loss: 0.003169  [52920/170046]\n",
            "loss: 0.002566  [55440/170046]\n",
            "loss: 0.002920  [57960/170046]\n",
            "loss: 0.002514  [60480/170046]\n",
            "loss: 0.002792  [63000/170046]\n",
            "loss: 0.003208  [65520/170046]\n",
            "loss: 0.002939  [68040/170046]\n",
            "loss: 0.002343  [70560/170046]\n",
            "loss: 0.001833  [73080/170046]\n",
            "loss: 0.002131  [75600/170046]\n",
            "loss: 0.002363  [78120/170046]\n",
            "loss: 0.002204  [80640/170046]\n",
            "loss: 0.002367  [83160/170046]\n",
            "loss: 0.003172  [85680/170046]\n",
            "loss: 0.001887  [88200/170046]\n",
            "loss: 0.003132  [90720/170046]\n",
            "loss: 0.003085  [93240/170046]\n",
            "loss: 0.003107  [95760/170046]\n",
            "loss: 0.002864  [98280/170046]\n",
            "loss: 0.003430  [100800/170046]\n",
            "loss: 0.002666  [103320/170046]\n",
            "loss: 0.003241  [105840/170046]\n",
            "loss: 0.002938  [108360/170046]\n",
            "loss: 0.002301  [110880/170046]\n",
            "loss: 0.002325  [113400/170046]\n",
            "loss: 0.002193  [115920/170046]\n",
            "loss: 0.003395  [118440/170046]\n",
            "loss: 0.002632  [120960/170046]\n",
            "loss: 0.002477  [123480/170046]\n",
            "loss: 0.002954  [126000/170046]\n",
            "loss: 0.002877  [128520/170046]\n",
            "loss: 0.002384  [131040/170046]\n",
            "loss: 0.002566  [133560/170046]\n",
            "loss: 0.003289  [136080/170046]\n",
            "loss: 0.002698  [138600/170046]\n",
            "loss: 0.002715  [141120/170046]\n",
            "loss: 0.003215  [143640/170046]\n",
            "loss: 0.002195  [146160/170046]\n",
            "loss: 0.001908  [148680/170046]\n",
            "loss: 0.002490  [151200/170046]\n",
            "loss: 0.003762  [153720/170046]\n",
            "loss: 0.002577  [156240/170046]\n",
            "loss: 0.002060  [158760/170046]\n",
            "loss: 0.002969  [161280/170046]\n",
            "loss: 0.002658  [163800/170046]\n",
            "loss: 0.003081  [166320/170046]\n",
            "loss: 0.002841  [168840/170046]\n",
            "chorus depth: avg MSE: 0.004913, avg abs error: 0.0508\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Finished training\n",
            "chorus depth: avg MSE: 0.004773, avg abs error: 0.0499\n"
          ]
        }
      ],
      "source": [
        "from src.util import plot_violin\n",
        "import numpy as np\n",
        "\n",
        "WEIGHTS_DIR = \"_weights/\"\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 15\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device {device}\")\n",
        "\n",
        "error = []\n",
        "\n",
        "fx = EFFECT_MAP.index(\"chorus\")\n",
        "\n",
        "WEIGHTS_PATH = os.path.join(WEIGHTS_DIR, EXPERIMENT_NAME + \"_\" + str(fx))\n",
        "\n",
        "if not os.path.exists('%s' % WEIGHTS_DIR):\n",
        "    os.makedirs('%s' % WEIGHTS_DIR)\n",
        "\n",
        "fxData = load_train_data(fx)\n",
        "# fxData, _ = torch.utils.data.random_split(fxData, lengths=[0.01, 0.99])\n",
        "\n",
        "train_dataloader, test_dataloader = split_data(fxData)\n",
        "val_dataloader = load_evaluation_data(fx)\n",
        "\n",
        "# construct model and assign it to device\n",
        "cnn = model.Extractor().to(device)\n",
        "\n",
        "# if fx == 0:\n",
        "#     signal, _, _, _, _ = fxData[0]\n",
        "#     print(f\"There are {len(fxData)} samples in the dataset.\")\n",
        "#     print(f\"Shape of signal: {signal.shape}\")\n",
        "\n",
        "#     print(\"input feature:\")\n",
        "#     log_writer.add_figure(\"Input Feature\", plot_spectrogram(signal[0], title=\"MFCC\"))\n",
        "#     log_writer.add_graph(cnn, signal.unsqueeze_(0))\n",
        "\n",
        "# initialise loss funtion + optimiser\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "optimiser = torch.optim.Adam(cnn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# train model\n",
        "train.train(cnn,\n",
        "            train_dataloader,\n",
        "            test_dataloader,\n",
        "            loss_fn,\n",
        "            optimiser,\n",
        "            device,\n",
        "            log_writer,\n",
        "            EPOCHS,\n",
        "            WEIGHTS_PATH,\n",
        "            effect=fx)\n",
        "\n",
        "_, _, log = train.test(cnn, val_dataloader, device, effect=fx)\n",
        "for _, data in enumerate(log):\n",
        "    error.append(data[3])\n",
        "\n",
        "arr = np.array(error)\n",
        "np.save(EVU_DIR + EXPERIMENT_NAME + \"_\" + str(fx) + \"_evaluation.npy\", arr)\n",
        "\n",
        "# log_writer.add_figure(\"Error Box\", \n",
        "#                       plot_violin(error, title=\"Error\", labels=EFFECT_MAP, ylabel=\"parameter value\", outlier=True))\n",
        "\n",
        "log_writer.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "torchaudio-tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
