{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Distortion Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMiPTdSA8C2F",
        "outputId": "a297ba92-a5c6-4015-d92d-1e9487daf896"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import os"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Define transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1gIdrZtW_JYf",
        "outputId": "c1bb6969-15a9-44eb-8cdd-8d79c89208f2"
      },
      "outputs": [],
      "source": [
        "SAMPLE_RATE = 22050\n",
        "NUM_SAMPLES = 22050*3\n",
        "\n",
        "mfcc = torchaudio.transforms.MFCC(\n",
        "    sample_rate = SAMPLE_RATE, \n",
        "    n_mfcc = 64,\n",
        "    melkwargs = {\n",
        "        \"n_fft\": 1024,\n",
        "        \"hop_length\": 1024,\n",
        "        \"n_mels\": 64,\n",
        "        \"center\": False})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Functions for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.gtfxdataset import GtFxDataset\n",
        "from src.util import plot_spectrogram\n",
        "from src.extrector import train\n",
        "from src.extrector import model\n",
        "from torch import nn\n",
        "\n",
        "AUDIO_DIR = \"_assets/DATASET/GT-FX-C53/\"\n",
        "ANNOTATIONS_FILE = os.path.join(AUDIO_DIR, \"train.csv\")\n",
        "EVU_ANNOTATIONS_FILE = os.path.join(AUDIO_DIR, \"evaluation.csv\")\n",
        "EFFECT_MAP = [\"distortion\", \"chorus\", \"tremolo\", \"delay\", \"reverb\"]\n",
        "\n",
        "def load_train_data(effect):\n",
        "    \n",
        "    fxData = GtFxDataset(ANNOTATIONS_FILE,\n",
        "                        AUDIO_DIR,\n",
        "                        mfcc,\n",
        "                        SAMPLE_RATE,\n",
        "                        NUM_SAMPLES,\n",
        "                        device,\n",
        "                        effect=EFFECT_MAP[effect])\n",
        "    return fxData\n",
        "\n",
        "def load_evaluation_data(effect):\n",
        "\n",
        "    evuData = GtFxDataset(EVU_ANNOTATIONS_FILE,\n",
        "                        AUDIO_DIR,\n",
        "                        mfcc,\n",
        "                        SAMPLE_RATE,\n",
        "                        NUM_SAMPLES,\n",
        "                        device,\n",
        "                        effect=EFFECT_MAP[effect])\n",
        "\n",
        "    BATCH_SIZE = round(len(evuData) / 1500)\n",
        "    val_dataloader = train.create_data_loader(evuData, BATCH_SIZE)\n",
        "    return val_dataloader\n",
        "\n",
        "def split_data(data):\n",
        "\n",
        "    BATCH_SIZE = round(len(data) / 1500)\n",
        "\n",
        "    split_ratio = [0.9, 0.1]\n",
        "    train_set, test_set = torch.utils.data.random_split(data, lengths=split_ratio)\n",
        "\n",
        "    train_dataloader = train.create_data_loader(train_set, BATCH_SIZE)\n",
        "    test_dataloader = train.create_data_loader(test_set, BATCH_SIZE)\n",
        "\n",
        "    return train_dataloader, test_dataloader   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Add Tensorboard to record data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "EXPERIMENT_NAME = \"c53_parameter\"\n",
        "LOG_DIR = \"_log/\" + EXPERIMENT_NAME\n",
        "EVU_DIR = \"_log/Evaluation/\"\n",
        "\n",
        "if not os.path.exists('%s' % LOG_DIR):\n",
        "    os.makedirs('%s' % LOG_DIR)\n",
        "\n",
        "if not os.path.exists('%s' % EVU_DIR):\n",
        "    os.makedirs('%s' % EVU_DIR)\n",
        "\n",
        "log_writer = SummaryWriter(LOG_DIR)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8O9r5WI2zuI",
        "outputId": "ed2a4ebd-f644-4e35-8fda-86170924dfe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device cpu\n",
            "Epoch 1\n",
            "loss: 0.087666  [  0/170046]\n",
            "loss: 0.311293  [2520/170046]\n",
            "loss: 0.297591  [5040/170046]\n",
            "loss: 0.067609  [7560/170046]\n",
            "loss: 0.056309  [10080/170046]\n",
            "loss: 0.058064  [12600/170046]\n",
            "loss: 0.029724  [15120/170046]\n",
            "loss: 0.022559  [17640/170046]\n",
            "loss: 0.017389  [20160/170046]\n",
            "loss: 0.016694  [22680/170046]\n",
            "loss: 0.018302  [25200/170046]\n",
            "loss: 0.017817  [27720/170046]\n",
            "loss: 0.013206  [30240/170046]\n",
            "loss: 0.011643  [32760/170046]\n",
            "loss: 0.009713  [35280/170046]\n",
            "loss: 0.010264  [37800/170046]\n",
            "loss: 0.010293  [40320/170046]\n",
            "loss: 0.019304  [42840/170046]\n",
            "loss: 0.010920  [45360/170046]\n",
            "loss: 0.006517  [47880/170046]\n",
            "loss: 0.010229  [50400/170046]\n",
            "loss: 0.009471  [52920/170046]\n",
            "loss: 0.013455  [55440/170046]\n",
            "loss: 0.006518  [57960/170046]\n",
            "loss: 0.010180  [60480/170046]\n",
            "loss: 0.009495  [63000/170046]\n",
            "loss: 0.007485  [65520/170046]\n",
            "loss: 0.008946  [68040/170046]\n",
            "loss: 0.008431  [70560/170046]\n",
            "loss: 0.006278  [73080/170046]\n",
            "loss: 0.006704  [75600/170046]\n",
            "loss: 0.007561  [78120/170046]\n",
            "loss: 0.008891  [80640/170046]\n",
            "loss: 0.008327  [83160/170046]\n",
            "loss: 0.008338  [85680/170046]\n",
            "loss: 0.005682  [88200/170046]\n",
            "loss: 0.008246  [90720/170046]\n",
            "loss: 0.007499  [93240/170046]\n",
            "loss: 0.005846  [95760/170046]\n",
            "loss: 0.007105  [98280/170046]\n",
            "loss: 0.010590  [100800/170046]\n",
            "loss: 0.012256  [103320/170046]\n",
            "loss: 0.007457  [105840/170046]\n",
            "loss: 0.004586  [108360/170046]\n",
            "loss: 0.004516  [110880/170046]\n",
            "loss: 0.006021  [113400/170046]\n",
            "loss: 0.005491  [115920/170046]\n",
            "loss: 0.006172  [118440/170046]\n",
            "loss: 0.005071  [120960/170046]\n",
            "loss: 0.006794  [123480/170046]\n",
            "loss: 0.006112  [126000/170046]\n",
            "loss: 0.005375  [128520/170046]\n",
            "loss: 0.005972  [131040/170046]\n",
            "loss: 0.006030  [133560/170046]\n",
            "loss: 0.006043  [136080/170046]\n",
            "loss: 0.004907  [138600/170046]\n",
            "loss: 0.006521  [141120/170046]\n",
            "loss: 0.006694  [143640/170046]\n",
            "loss: 0.003688  [146160/170046]\n",
            "loss: 0.004785  [148680/170046]\n",
            "loss: 0.006834  [151200/170046]\n",
            "loss: 0.008603  [153720/170046]\n",
            "loss: 0.007993  [156240/170046]\n",
            "loss: 0.009913  [158760/170046]\n",
            "loss: 0.004098  [161280/170046]\n",
            "loss: 0.004476  [163800/170046]\n",
            "loss: 0.007296  [166320/170046]\n",
            "loss: 0.004331  [168840/170046]\n",
            "tremolo rate: avg MSE: 0.011156, avg abs error: 0.0702\n",
            "learning rate: 0.001000 -> 0.000910\n",
            "---------------------------\n",
            "\n",
            "Epoch 2\n",
            "loss: 0.006502  [  0/170046]\n",
            "loss: 0.003678  [2520/170046]\n",
            "loss: 0.004078  [5040/170046]\n",
            "loss: 0.003603  [7560/170046]\n",
            "loss: 0.004428  [10080/170046]\n",
            "loss: 0.005046  [12600/170046]\n",
            "loss: 0.004500  [15120/170046]\n",
            "loss: 0.006493  [17640/170046]\n",
            "loss: 0.005145  [20160/170046]\n",
            "loss: 0.002532  [22680/170046]\n",
            "loss: 0.005369  [25200/170046]\n",
            "loss: 0.005730  [27720/170046]\n",
            "loss: 0.012553  [30240/170046]\n",
            "loss: 0.003167  [32760/170046]\n",
            "loss: 0.005014  [35280/170046]\n",
            "loss: 0.004344  [37800/170046]\n",
            "loss: 0.003763  [40320/170046]\n",
            "loss: 0.006232  [42840/170046]\n",
            "loss: 0.004551  [45360/170046]\n",
            "loss: 0.004089  [47880/170046]\n",
            "loss: 0.004803  [50400/170046]\n",
            "loss: 0.003550  [52920/170046]\n",
            "loss: 0.003545  [55440/170046]\n",
            "loss: 0.006493  [57960/170046]\n",
            "loss: 0.005684  [60480/170046]\n",
            "loss: 0.006273  [63000/170046]\n",
            "loss: 0.005147  [65520/170046]\n",
            "loss: 0.007320  [68040/170046]\n",
            "loss: 0.004768  [70560/170046]\n",
            "loss: 0.004517  [73080/170046]\n",
            "loss: 0.004863  [75600/170046]\n",
            "loss: 0.007185  [78120/170046]\n",
            "loss: 0.002103  [80640/170046]\n",
            "loss: 0.004395  [83160/170046]\n",
            "loss: 0.003703  [85680/170046]\n",
            "loss: 0.005208  [88200/170046]\n",
            "loss: 0.004088  [90720/170046]\n",
            "loss: 0.003924  [93240/170046]\n",
            "loss: 0.004197  [95760/170046]\n",
            "loss: 0.007053  [98280/170046]\n",
            "loss: 0.005610  [100800/170046]\n",
            "loss: 0.006915  [103320/170046]\n",
            "loss: 0.006045  [105840/170046]\n",
            "loss: 0.006947  [108360/170046]\n",
            "loss: 0.003789  [110880/170046]\n",
            "loss: 0.004530  [113400/170046]\n",
            "loss: 0.004154  [115920/170046]\n",
            "loss: 0.003793  [118440/170046]\n",
            "loss: 0.005753  [120960/170046]\n",
            "loss: 0.003713  [123480/170046]\n",
            "loss: 0.004357  [126000/170046]\n",
            "loss: 0.003872  [128520/170046]\n",
            "loss: 0.003425  [131040/170046]\n",
            "loss: 0.005028  [133560/170046]\n",
            "loss: 0.002887  [136080/170046]\n",
            "loss: 0.004324  [138600/170046]\n",
            "loss: 0.003729  [141120/170046]\n",
            "loss: 0.006239  [143640/170046]\n",
            "loss: 0.003114  [146160/170046]\n",
            "loss: 0.003096  [148680/170046]\n",
            "loss: 0.004690  [151200/170046]\n",
            "loss: 0.002790  [153720/170046]\n",
            "loss: 0.007012  [156240/170046]\n",
            "loss: 0.005292  [158760/170046]\n",
            "loss: 0.009161  [161280/170046]\n",
            "loss: 0.003652  [163800/170046]\n",
            "loss: 0.004852  [166320/170046]\n",
            "loss: 0.006060  [168840/170046]\n",
            "tremolo rate: avg MSE: 0.006061, avg abs error: 0.0482\n",
            "learning rate: 0.000910 -> 0.000820\n",
            "---------------------------\n",
            "\n",
            "Epoch 3\n",
            "loss: 0.005600  [  0/170046]\n",
            "loss: 0.003751  [2520/170046]\n",
            "loss: 0.003326  [5040/170046]\n",
            "loss: 0.003131  [7560/170046]\n",
            "loss: 0.002525  [10080/170046]\n",
            "loss: 0.004309  [12600/170046]\n",
            "loss: 0.007088  [15120/170046]\n",
            "loss: 0.006938  [17640/170046]\n",
            "loss: 0.003155  [20160/170046]\n",
            "loss: 0.005179  [22680/170046]\n",
            "loss: 0.003344  [25200/170046]\n",
            "loss: 0.002404  [27720/170046]\n",
            "loss: 0.002320  [30240/170046]\n",
            "loss: 0.004380  [32760/170046]\n",
            "loss: 0.002282  [35280/170046]\n",
            "loss: 0.004410  [37800/170046]\n",
            "loss: 0.006704  [40320/170046]\n",
            "loss: 0.004357  [42840/170046]\n",
            "loss: 0.005375  [45360/170046]\n",
            "loss: 0.005598  [47880/170046]\n",
            "loss: 0.001924  [50400/170046]\n",
            "loss: 0.006443  [52920/170046]\n",
            "loss: 0.003187  [55440/170046]\n",
            "loss: 0.005660  [57960/170046]\n",
            "loss: 0.003329  [60480/170046]\n",
            "loss: 0.003610  [63000/170046]\n",
            "loss: 0.003238  [65520/170046]\n",
            "loss: 0.008743  [68040/170046]\n",
            "loss: 0.002092  [70560/170046]\n",
            "loss: 0.002518  [73080/170046]\n",
            "loss: 0.004173  [75600/170046]\n",
            "loss: 0.003066  [78120/170046]\n",
            "loss: 0.003563  [80640/170046]\n",
            "loss: 0.002566  [83160/170046]\n",
            "loss: 0.004538  [85680/170046]\n",
            "loss: 0.001843  [88200/170046]\n",
            "loss: 0.003461  [90720/170046]\n",
            "loss: 0.003584  [93240/170046]\n",
            "loss: 0.005797  [95760/170046]\n",
            "loss: 0.003736  [98280/170046]\n",
            "loss: 0.005411  [100800/170046]\n",
            "loss: 0.002496  [103320/170046]\n",
            "loss: 0.004095  [105840/170046]\n",
            "loss: 0.004395  [108360/170046]\n",
            "loss: 0.002655  [110880/170046]\n",
            "loss: 0.004888  [113400/170046]\n",
            "loss: 0.003240  [115920/170046]\n",
            "loss: 0.002035  [118440/170046]\n",
            "loss: 0.005704  [120960/170046]\n",
            "loss: 0.005710  [123480/170046]\n",
            "loss: 0.003843  [126000/170046]\n",
            "loss: 0.002197  [128520/170046]\n",
            "loss: 0.006838  [131040/170046]\n",
            "loss: 0.001392  [133560/170046]\n",
            "loss: 0.002239  [136080/170046]\n",
            "loss: 0.002692  [138600/170046]\n",
            "loss: 0.003146  [141120/170046]\n",
            "loss: 0.006738  [143640/170046]\n",
            "loss: 0.004495  [146160/170046]\n",
            "loss: 0.004354  [148680/170046]\n",
            "loss: 0.004366  [151200/170046]\n",
            "loss: 0.002543  [153720/170046]\n",
            "loss: 0.003177  [156240/170046]\n",
            "loss: 0.001463  [158760/170046]\n",
            "loss: 0.004447  [161280/170046]\n",
            "loss: 0.005726  [163800/170046]\n",
            "loss: 0.003290  [166320/170046]\n",
            "loss: 0.003647  [168840/170046]\n",
            "tremolo rate: avg MSE: 0.003454, avg abs error: 0.037\n",
            "learning rate: 0.000820 -> 0.000730\n",
            "---------------------------\n",
            "\n",
            "Epoch 4\n",
            "loss: 0.002987  [  0/170046]\n",
            "loss: 0.003014  [2520/170046]\n",
            "loss: 0.002811  [5040/170046]\n",
            "loss: 0.001615  [7560/170046]\n",
            "loss: 0.004672  [10080/170046]\n",
            "loss: 0.002269  [12600/170046]\n",
            "loss: 0.004264  [15120/170046]\n",
            "loss: 0.001142  [17640/170046]\n",
            "loss: 0.003718  [20160/170046]\n",
            "loss: 0.003891  [22680/170046]\n",
            "loss: 0.002961  [25200/170046]\n",
            "loss: 0.004597  [27720/170046]\n",
            "loss: 0.003177  [30240/170046]\n",
            "loss: 0.005255  [32760/170046]\n",
            "loss: 0.002131  [35280/170046]\n",
            "loss: 0.003861  [37800/170046]\n",
            "loss: 0.003472  [40320/170046]\n",
            "loss: 0.002457  [42840/170046]\n",
            "loss: 0.001852  [45360/170046]\n",
            "loss: 0.003141  [47880/170046]\n",
            "loss: 0.003550  [50400/170046]\n",
            "loss: 0.003312  [52920/170046]\n",
            "loss: 0.001873  [55440/170046]\n",
            "loss: 0.002498  [57960/170046]\n",
            "loss: 0.002646  [60480/170046]\n",
            "loss: 0.004278  [63000/170046]\n",
            "loss: 0.002534  [65520/170046]\n",
            "loss: 0.004358  [68040/170046]\n",
            "loss: 0.001889  [70560/170046]\n",
            "loss: 0.002556  [73080/170046]\n",
            "loss: 0.002147  [75600/170046]\n",
            "loss: 0.002324  [78120/170046]\n",
            "loss: 0.002963  [80640/170046]\n",
            "loss: 0.001643  [83160/170046]\n",
            "loss: 0.002224  [85680/170046]\n",
            "loss: 0.002404  [88200/170046]\n",
            "loss: 0.002645  [90720/170046]\n",
            "loss: 0.002313  [93240/170046]\n",
            "loss: 0.002279  [95760/170046]\n",
            "loss: 0.002275  [98280/170046]\n",
            "loss: 0.002007  [100800/170046]\n",
            "loss: 0.002141  [103320/170046]\n",
            "loss: 0.002402  [105840/170046]\n",
            "loss: 0.003893  [108360/170046]\n",
            "loss: 0.002249  [110880/170046]\n",
            "loss: 0.003890  [113400/170046]\n",
            "loss: 0.005688  [115920/170046]\n",
            "loss: 0.001977  [118440/170046]\n",
            "loss: 0.002303  [120960/170046]\n",
            "loss: 0.002497  [123480/170046]\n",
            "loss: 0.002648  [126000/170046]\n",
            "loss: 0.002571  [128520/170046]\n",
            "loss: 0.002686  [131040/170046]\n",
            "loss: 0.002993  [133560/170046]\n",
            "loss: 0.002089  [136080/170046]\n",
            "loss: 0.001953  [138600/170046]\n",
            "loss: 0.001781  [141120/170046]\n",
            "loss: 0.005460  [143640/170046]\n",
            "loss: 0.003822  [146160/170046]\n",
            "loss: 0.002579  [148680/170046]\n",
            "loss: 0.002988  [151200/170046]\n",
            "loss: 0.003695  [153720/170046]\n",
            "loss: 0.003727  [156240/170046]\n",
            "loss: 0.002276  [158760/170046]\n",
            "loss: 0.002648  [161280/170046]\n",
            "loss: 0.002052  [163800/170046]\n",
            "loss: 0.003046  [166320/170046]\n",
            "loss: 0.001898  [168840/170046]\n",
            "tremolo rate: avg MSE: 0.003026, avg abs error: 0.0353\n",
            "learning rate: 0.000730 -> 0.000640\n",
            "---------------------------\n",
            "\n",
            "Epoch 5\n",
            "loss: 0.002459  [  0/170046]\n",
            "loss: 0.002428  [2520/170046]\n",
            "loss: 0.002901  [5040/170046]\n",
            "loss: 0.002791  [7560/170046]\n",
            "loss: 0.001813  [10080/170046]\n",
            "loss: 0.004600  [12600/170046]\n",
            "loss: 0.002107  [15120/170046]\n",
            "loss: 0.002015  [17640/170046]\n",
            "loss: 0.001489  [20160/170046]\n",
            "loss: 0.002758  [22680/170046]\n",
            "loss: 0.001593  [25200/170046]\n",
            "loss: 0.001927  [27720/170046]\n",
            "loss: 0.002417  [30240/170046]\n",
            "loss: 0.002420  [32760/170046]\n",
            "loss: 0.003279  [35280/170046]\n",
            "loss: 0.002542  [37800/170046]\n",
            "loss: 0.004565  [40320/170046]\n",
            "loss: 0.002370  [42840/170046]\n",
            "loss: 0.002943  [45360/170046]\n",
            "loss: 0.002958  [47880/170046]\n",
            "loss: 0.003196  [50400/170046]\n",
            "loss: 0.003032  [52920/170046]\n",
            "loss: 0.003819  [55440/170046]\n",
            "loss: 0.001777  [57960/170046]\n",
            "loss: 0.002463  [60480/170046]\n",
            "loss: 0.002631  [63000/170046]\n",
            "loss: 0.002305  [65520/170046]\n",
            "loss: 0.002671  [68040/170046]\n",
            "loss: 0.002807  [70560/170046]\n",
            "loss: 0.002008  [73080/170046]\n",
            "loss: 0.001273  [75600/170046]\n",
            "loss: 0.002109  [78120/170046]\n",
            "loss: 0.003025  [80640/170046]\n",
            "loss: 0.002255  [83160/170046]\n",
            "loss: 0.002634  [85680/170046]\n",
            "loss: 0.001533  [88200/170046]\n",
            "loss: 0.001716  [90720/170046]\n",
            "loss: 0.003641  [93240/170046]\n",
            "loss: 0.003952  [95760/170046]\n",
            "loss: 0.002792  [98280/170046]\n",
            "loss: 0.002697  [100800/170046]\n",
            "loss: 0.002919  [103320/170046]\n",
            "loss: 0.001508  [105840/170046]\n",
            "loss: 0.001630  [108360/170046]\n",
            "loss: 0.005709  [110880/170046]\n",
            "loss: 0.003331  [113400/170046]\n",
            "loss: 0.003342  [115920/170046]\n",
            "loss: 0.002808  [118440/170046]\n",
            "loss: 0.001243  [120960/170046]\n",
            "loss: 0.002862  [123480/170046]\n",
            "loss: 0.001853  [126000/170046]\n",
            "loss: 0.003278  [128520/170046]\n",
            "loss: 0.003720  [131040/170046]\n",
            "loss: 0.002950  [133560/170046]\n",
            "loss: 0.001265  [136080/170046]\n",
            "loss: 0.001115  [138600/170046]\n",
            "loss: 0.002536  [141120/170046]\n",
            "loss: 0.001313  [143640/170046]\n",
            "loss: 0.004128  [146160/170046]\n",
            "loss: 0.001653  [148680/170046]\n",
            "loss: 0.002476  [151200/170046]\n",
            "loss: 0.002527  [153720/170046]\n",
            "loss: 0.002143  [156240/170046]\n",
            "loss: 0.001850  [158760/170046]\n",
            "loss: 0.003520  [161280/170046]\n",
            "loss: 0.003185  [163800/170046]\n",
            "loss: 0.001940  [166320/170046]\n",
            "loss: 0.002753  [168840/170046]\n",
            "tremolo rate: avg MSE: 0.002955, avg abs error: 0.0335\n",
            "learning rate: 0.000640 -> 0.000550\n",
            "---------------------------\n",
            "\n",
            "Epoch 6\n",
            "loss: 0.002102  [  0/170046]\n",
            "loss: 0.000896  [2520/170046]\n",
            "loss: 0.003220  [5040/170046]\n",
            "loss: 0.001368  [7560/170046]\n",
            "loss: 0.003237  [10080/170046]\n",
            "loss: 0.001630  [12600/170046]\n",
            "loss: 0.001789  [15120/170046]\n",
            "loss: 0.001431  [17640/170046]\n",
            "loss: 0.001426  [20160/170046]\n",
            "loss: 0.002685  [22680/170046]\n",
            "loss: 0.002370  [25200/170046]\n",
            "loss: 0.002093  [27720/170046]\n",
            "loss: 0.002196  [30240/170046]\n",
            "loss: 0.002698  [32760/170046]\n",
            "loss: 0.001695  [35280/170046]\n",
            "loss: 0.002418  [37800/170046]\n",
            "loss: 0.001968  [40320/170046]\n",
            "loss: 0.004028  [42840/170046]\n",
            "loss: 0.002338  [45360/170046]\n",
            "loss: 0.003030  [47880/170046]\n",
            "loss: 0.002651  [50400/170046]\n",
            "loss: 0.002119  [52920/170046]\n",
            "loss: 0.000864  [55440/170046]\n",
            "loss: 0.002715  [57960/170046]\n",
            "loss: 0.002921  [60480/170046]\n",
            "loss: 0.001226  [63000/170046]\n",
            "loss: 0.002369  [65520/170046]\n",
            "loss: 0.001531  [68040/170046]\n",
            "loss: 0.003996  [70560/170046]\n",
            "loss: 0.003321  [73080/170046]\n",
            "loss: 0.001738  [75600/170046]\n",
            "loss: 0.003484  [78120/170046]\n",
            "loss: 0.001392  [80640/170046]\n",
            "loss: 0.000897  [83160/170046]\n",
            "loss: 0.002630  [85680/170046]\n",
            "loss: 0.002648  [88200/170046]\n",
            "loss: 0.002439  [90720/170046]\n",
            "loss: 0.002157  [93240/170046]\n",
            "loss: 0.002228  [95760/170046]\n",
            "loss: 0.002874  [98280/170046]\n",
            "loss: 0.001276  [100800/170046]\n",
            "loss: 0.002481  [103320/170046]\n",
            "loss: 0.001250  [105840/170046]\n",
            "loss: 0.002361  [108360/170046]\n",
            "loss: 0.002978  [110880/170046]\n",
            "loss: 0.001165  [113400/170046]\n",
            "loss: 0.002903  [115920/170046]\n",
            "loss: 0.002569  [118440/170046]\n",
            "loss: 0.001856  [120960/170046]\n",
            "loss: 0.003276  [123480/170046]\n",
            "loss: 0.002321  [126000/170046]\n",
            "loss: 0.003901  [128520/170046]\n",
            "loss: 0.001282  [131040/170046]\n",
            "loss: 0.001704  [133560/170046]\n",
            "loss: 0.003818  [136080/170046]\n",
            "loss: 0.003065  [138600/170046]\n",
            "loss: 0.001218  [141120/170046]\n",
            "loss: 0.003466  [143640/170046]\n",
            "loss: 0.002687  [146160/170046]\n",
            "loss: 0.002145  [148680/170046]\n",
            "loss: 0.001984  [151200/170046]\n",
            "loss: 0.002205  [153720/170046]\n",
            "loss: 0.002054  [156240/170046]\n",
            "loss: 0.004191  [158760/170046]\n",
            "loss: 0.002853  [161280/170046]\n",
            "loss: 0.002516  [163800/170046]\n",
            "loss: 0.001216  [166320/170046]\n",
            "loss: 0.001528  [168840/170046]\n",
            "tremolo rate: avg MSE: 0.002401, avg abs error: 0.0283\n",
            "learning rate: 0.000550 -> 0.000460\n",
            "---------------------------\n",
            "\n",
            "Epoch 7\n",
            "loss: 0.001780  [  0/170046]\n",
            "loss: 0.000725  [2520/170046]\n",
            "loss: 0.001339  [5040/170046]\n",
            "loss: 0.001370  [7560/170046]\n",
            "loss: 0.001811  [10080/170046]\n",
            "loss: 0.002045  [12600/170046]\n",
            "loss: 0.001061  [15120/170046]\n",
            "loss: 0.001456  [17640/170046]\n",
            "loss: 0.002727  [20160/170046]\n",
            "loss: 0.001926  [22680/170046]\n",
            "loss: 0.001848  [25200/170046]\n",
            "loss: 0.001419  [27720/170046]\n",
            "loss: 0.001217  [30240/170046]\n",
            "loss: 0.001959  [32760/170046]\n",
            "loss: 0.001698  [35280/170046]\n",
            "loss: 0.002818  [37800/170046]\n",
            "loss: 0.003265  [40320/170046]\n",
            "loss: 0.002330  [42840/170046]\n",
            "loss: 0.001067  [45360/170046]\n",
            "loss: 0.001801  [47880/170046]\n",
            "loss: 0.001485  [50400/170046]\n",
            "loss: 0.003010  [52920/170046]\n",
            "loss: 0.001612  [55440/170046]\n",
            "loss: 0.001058  [57960/170046]\n",
            "loss: 0.001847  [60480/170046]\n",
            "loss: 0.002360  [63000/170046]\n",
            "loss: 0.002095  [65520/170046]\n",
            "loss: 0.002325  [68040/170046]\n",
            "loss: 0.001068  [70560/170046]\n",
            "loss: 0.002210  [73080/170046]\n",
            "loss: 0.001635  [75600/170046]\n",
            "loss: 0.003946  [78120/170046]\n",
            "loss: 0.001611  [80640/170046]\n",
            "loss: 0.002253  [83160/170046]\n",
            "loss: 0.002147  [85680/170046]\n",
            "loss: 0.001011  [88200/170046]\n",
            "loss: 0.003130  [90720/170046]\n",
            "loss: 0.002175  [93240/170046]\n",
            "loss: 0.001705  [95760/170046]\n",
            "loss: 0.001837  [98280/170046]\n",
            "loss: 0.002427  [100800/170046]\n",
            "loss: 0.001118  [103320/170046]\n",
            "loss: 0.003018  [105840/170046]\n",
            "loss: 0.001774  [108360/170046]\n",
            "loss: 0.002058  [110880/170046]\n",
            "loss: 0.001834  [113400/170046]\n",
            "loss: 0.001915  [115920/170046]\n",
            "loss: 0.001577  [118440/170046]\n",
            "loss: 0.001838  [120960/170046]\n",
            "loss: 0.001321  [123480/170046]\n",
            "loss: 0.001680  [126000/170046]\n",
            "loss: 0.001813  [128520/170046]\n",
            "loss: 0.001509  [131040/170046]\n",
            "loss: 0.002726  [133560/170046]\n",
            "loss: 0.001444  [136080/170046]\n",
            "loss: 0.002052  [138600/170046]\n",
            "loss: 0.002127  [141120/170046]\n",
            "loss: 0.001971  [143640/170046]\n",
            "loss: 0.001580  [146160/170046]\n",
            "loss: 0.001949  [148680/170046]\n",
            "loss: 0.001412  [151200/170046]\n",
            "loss: 0.001482  [153720/170046]\n",
            "loss: 0.000933  [156240/170046]\n",
            "loss: 0.003816  [158760/170046]\n",
            "loss: 0.002402  [161280/170046]\n",
            "loss: 0.001536  [163800/170046]\n",
            "loss: 0.002100  [166320/170046]\n",
            "loss: 0.002485  [168840/170046]\n",
            "tremolo rate: avg MSE: 0.002591, avg abs error: 0.0292\n",
            "learning rate: 0.000460 -> 0.000370\n",
            "---------------------------\n",
            "\n",
            "Epoch 8\n",
            "loss: 0.002287  [  0/170046]\n",
            "loss: 0.001788  [2520/170046]\n",
            "loss: 0.001938  [5040/170046]\n",
            "loss: 0.002679  [7560/170046]\n",
            "loss: 0.002642  [10080/170046]\n",
            "loss: 0.001169  [12600/170046]\n",
            "loss: 0.001150  [15120/170046]\n",
            "loss: 0.001252  [17640/170046]\n",
            "loss: 0.000909  [20160/170046]\n",
            "loss: 0.001509  [22680/170046]\n",
            "loss: 0.002061  [25200/170046]\n",
            "loss: 0.001283  [27720/170046]\n",
            "loss: 0.001786  [30240/170046]\n",
            "loss: 0.000840  [32760/170046]\n",
            "loss: 0.001053  [35280/170046]\n",
            "loss: 0.001302  [37800/170046]\n",
            "loss: 0.001337  [40320/170046]\n",
            "loss: 0.001094  [42840/170046]\n",
            "loss: 0.001513  [45360/170046]\n",
            "loss: 0.001117  [47880/170046]\n",
            "loss: 0.002728  [50400/170046]\n",
            "loss: 0.002131  [52920/170046]\n",
            "loss: 0.001236  [55440/170046]\n",
            "loss: 0.000764  [57960/170046]\n",
            "loss: 0.001407  [60480/170046]\n",
            "loss: 0.001559  [63000/170046]\n",
            "loss: 0.001126  [65520/170046]\n",
            "loss: 0.000913  [68040/170046]\n",
            "loss: 0.001985  [70560/170046]\n",
            "loss: 0.001586  [73080/170046]\n",
            "loss: 0.001693  [75600/170046]\n",
            "loss: 0.000946  [78120/170046]\n",
            "loss: 0.002041  [80640/170046]\n",
            "loss: 0.001817  [83160/170046]\n",
            "loss: 0.001406  [85680/170046]\n",
            "loss: 0.001461  [88200/170046]\n",
            "loss: 0.002318  [90720/170046]\n",
            "loss: 0.002115  [93240/170046]\n",
            "loss: 0.001562  [95760/170046]\n",
            "loss: 0.001928  [98280/170046]\n",
            "loss: 0.001258  [100800/170046]\n",
            "loss: 0.002262  [103320/170046]\n",
            "loss: 0.001876  [105840/170046]\n",
            "loss: 0.001290  [108360/170046]\n",
            "loss: 0.001284  [110880/170046]\n",
            "loss: 0.001475  [113400/170046]\n",
            "loss: 0.001186  [115920/170046]\n",
            "loss: 0.001264  [118440/170046]\n",
            "loss: 0.001802  [120960/170046]\n",
            "loss: 0.002226  [123480/170046]\n",
            "loss: 0.001806  [126000/170046]\n",
            "loss: 0.004614  [128520/170046]\n",
            "loss: 0.000975  [131040/170046]\n",
            "loss: 0.003598  [133560/170046]\n",
            "loss: 0.002346  [136080/170046]\n",
            "loss: 0.001440  [138600/170046]\n",
            "loss: 0.001901  [141120/170046]\n",
            "loss: 0.002399  [143640/170046]\n",
            "loss: 0.001821  [146160/170046]\n",
            "loss: 0.001217  [148680/170046]\n",
            "loss: 0.002196  [151200/170046]\n",
            "loss: 0.001825  [153720/170046]\n",
            "loss: 0.001195  [156240/170046]\n",
            "loss: 0.001710  [158760/170046]\n",
            "loss: 0.001611  [161280/170046]\n",
            "loss: 0.001267  [163800/170046]\n",
            "loss: 0.001162  [166320/170046]\n",
            "loss: 0.001766  [168840/170046]\n",
            "tremolo rate: avg MSE: 0.002639, avg abs error: 0.0325\n",
            "learning rate: 0.000370 -> 0.000280\n",
            "---------------------------\n",
            "\n",
            "Epoch 9\n",
            "loss: 0.001854  [  0/170046]\n",
            "loss: 0.001584  [2520/170046]\n",
            "loss: 0.001040  [5040/170046]\n",
            "loss: 0.002632  [7560/170046]\n",
            "loss: 0.002338  [10080/170046]\n",
            "loss: 0.001475  [12600/170046]\n",
            "loss: 0.000813  [15120/170046]\n",
            "loss: 0.000619  [17640/170046]\n",
            "loss: 0.001037  [20160/170046]\n",
            "loss: 0.002081  [22680/170046]\n",
            "loss: 0.001595  [25200/170046]\n",
            "loss: 0.001291  [27720/170046]\n",
            "loss: 0.000833  [30240/170046]\n",
            "loss: 0.001377  [32760/170046]\n",
            "loss: 0.000674  [35280/170046]\n",
            "loss: 0.001161  [37800/170046]\n",
            "loss: 0.003448  [40320/170046]\n",
            "loss: 0.001346  [42840/170046]\n",
            "loss: 0.001371  [45360/170046]\n",
            "loss: 0.001125  [47880/170046]\n",
            "loss: 0.001753  [50400/170046]\n",
            "loss: 0.001120  [52920/170046]\n",
            "loss: 0.001061  [55440/170046]\n",
            "loss: 0.001318  [57960/170046]\n",
            "loss: 0.001122  [60480/170046]\n",
            "loss: 0.001672  [63000/170046]\n",
            "loss: 0.001293  [65520/170046]\n",
            "loss: 0.001268  [68040/170046]\n",
            "loss: 0.001350  [70560/170046]\n",
            "loss: 0.001900  [73080/170046]\n",
            "loss: 0.002733  [75600/170046]\n",
            "loss: 0.000958  [78120/170046]\n",
            "loss: 0.001521  [80640/170046]\n",
            "loss: 0.001163  [83160/170046]\n",
            "loss: 0.000823  [85680/170046]\n",
            "loss: 0.002008  [88200/170046]\n",
            "loss: 0.000789  [90720/170046]\n",
            "loss: 0.001589  [93240/170046]\n",
            "loss: 0.000932  [95760/170046]\n",
            "loss: 0.001675  [98280/170046]\n",
            "loss: 0.001530  [100800/170046]\n",
            "loss: 0.003674  [103320/170046]\n",
            "loss: 0.000777  [105840/170046]\n",
            "loss: 0.001526  [108360/170046]\n",
            "loss: 0.001202  [110880/170046]\n",
            "loss: 0.002024  [113400/170046]\n",
            "loss: 0.001221  [115920/170046]\n",
            "loss: 0.001237  [118440/170046]\n",
            "loss: 0.000885  [120960/170046]\n",
            "loss: 0.001659  [123480/170046]\n",
            "loss: 0.001507  [126000/170046]\n",
            "loss: 0.001717  [128520/170046]\n",
            "loss: 0.001611  [131040/170046]\n",
            "loss: 0.001257  [133560/170046]\n",
            "loss: 0.001727  [136080/170046]\n",
            "loss: 0.001214  [138600/170046]\n",
            "loss: 0.001063  [141120/170046]\n",
            "loss: 0.001090  [143640/170046]\n",
            "loss: 0.001814  [146160/170046]\n",
            "loss: 0.000875  [148680/170046]\n",
            "loss: 0.000723  [151200/170046]\n",
            "loss: 0.001374  [153720/170046]\n",
            "loss: 0.001592  [156240/170046]\n",
            "loss: 0.001092  [158760/170046]\n",
            "loss: 0.002243  [161280/170046]\n",
            "loss: 0.001161  [163800/170046]\n",
            "loss: 0.001336  [166320/170046]\n",
            "loss: 0.001450  [168840/170046]\n",
            "tremolo rate: avg MSE: 0.002118, avg abs error: 0.0277\n",
            "learning rate: 0.000280 -> 0.000190\n",
            "---------------------------\n",
            "\n",
            "Epoch 10\n",
            "loss: 0.000911  [  0/170046]\n",
            "loss: 0.000973  [2520/170046]\n",
            "loss: 0.000926  [5040/170046]\n",
            "loss: 0.000958  [7560/170046]\n",
            "loss: 0.000820  [10080/170046]\n",
            "loss: 0.001529  [12600/170046]\n",
            "loss: 0.001440  [15120/170046]\n",
            "loss: 0.000792  [17640/170046]\n",
            "loss: 0.001284  [20160/170046]\n",
            "loss: 0.002179  [22680/170046]\n",
            "loss: 0.001254  [25200/170046]\n",
            "loss: 0.001265  [27720/170046]\n",
            "loss: 0.001752  [30240/170046]\n",
            "loss: 0.000984  [32760/170046]\n",
            "loss: 0.000934  [35280/170046]\n",
            "loss: 0.001522  [37800/170046]\n",
            "loss: 0.000932  [40320/170046]\n",
            "loss: 0.000969  [42840/170046]\n",
            "loss: 0.001011  [45360/170046]\n",
            "loss: 0.001116  [47880/170046]\n",
            "loss: 0.000753  [50400/170046]\n",
            "loss: 0.001438  [52920/170046]\n",
            "loss: 0.001119  [55440/170046]\n",
            "loss: 0.000731  [57960/170046]\n",
            "loss: 0.002287  [60480/170046]\n",
            "loss: 0.000756  [63000/170046]\n",
            "loss: 0.001129  [65520/170046]\n",
            "loss: 0.001204  [68040/170046]\n",
            "loss: 0.000749  [70560/170046]\n",
            "loss: 0.001791  [73080/170046]\n",
            "loss: 0.001172  [75600/170046]\n",
            "loss: 0.001126  [78120/170046]\n",
            "loss: 0.001043  [80640/170046]\n",
            "loss: 0.001071  [83160/170046]\n",
            "loss: 0.001231  [85680/170046]\n",
            "loss: 0.001019  [88200/170046]\n",
            "loss: 0.002231  [90720/170046]\n",
            "loss: 0.001010  [93240/170046]\n",
            "loss: 0.000861  [95760/170046]\n",
            "loss: 0.000533  [98280/170046]\n",
            "loss: 0.001592  [100800/170046]\n",
            "loss: 0.002358  [103320/170046]\n",
            "loss: 0.001395  [105840/170046]\n",
            "loss: 0.001159  [108360/170046]\n",
            "loss: 0.001240  [110880/170046]\n",
            "loss: 0.000929  [113400/170046]\n",
            "loss: 0.001210  [115920/170046]\n",
            "loss: 0.001512  [118440/170046]\n",
            "loss: 0.000976  [120960/170046]\n",
            "loss: 0.000899  [123480/170046]\n",
            "loss: 0.001276  [126000/170046]\n",
            "loss: 0.001232  [128520/170046]\n",
            "loss: 0.001321  [131040/170046]\n",
            "loss: 0.000896  [133560/170046]\n",
            "loss: 0.001222  [136080/170046]\n",
            "loss: 0.000734  [138600/170046]\n",
            "loss: 0.001621  [141120/170046]\n",
            "loss: 0.001825  [143640/170046]\n",
            "loss: 0.000988  [146160/170046]\n",
            "loss: 0.001348  [148680/170046]\n",
            "loss: 0.001763  [151200/170046]\n",
            "loss: 0.001071  [153720/170046]\n",
            "loss: 0.001271  [156240/170046]\n",
            "loss: 0.001090  [158760/170046]\n",
            "loss: 0.001356  [161280/170046]\n",
            "loss: 0.000604  [163800/170046]\n",
            "loss: 0.001075  [166320/170046]\n",
            "loss: 0.001517  [168840/170046]\n",
            "tremolo rate: avg MSE: 0.001863, avg abs error: 0.0243\n",
            "learning rate: 0.000190 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 11\n",
            "loss: 0.001046  [  0/170046]\n",
            "loss: 0.000911  [2520/170046]\n",
            "loss: 0.000793  [5040/170046]\n",
            "loss: 0.000881  [7560/170046]\n",
            "loss: 0.000575  [10080/170046]\n",
            "loss: 0.001303  [12600/170046]\n",
            "loss: 0.001113  [15120/170046]\n",
            "loss: 0.000716  [17640/170046]\n",
            "loss: 0.000546  [20160/170046]\n",
            "loss: 0.001234  [22680/170046]\n",
            "loss: 0.000977  [25200/170046]\n",
            "loss: 0.000781  [27720/170046]\n",
            "loss: 0.000679  [30240/170046]\n",
            "loss: 0.001184  [32760/170046]\n",
            "loss: 0.001140  [35280/170046]\n",
            "loss: 0.000929  [37800/170046]\n",
            "loss: 0.001113  [40320/170046]\n",
            "loss: 0.001000  [42840/170046]\n",
            "loss: 0.000969  [45360/170046]\n",
            "loss: 0.000627  [47880/170046]\n",
            "loss: 0.001012  [50400/170046]\n",
            "loss: 0.000490  [52920/170046]\n",
            "loss: 0.000724  [55440/170046]\n",
            "loss: 0.000782  [57960/170046]\n",
            "loss: 0.001043  [60480/170046]\n",
            "loss: 0.001439  [63000/170046]\n",
            "loss: 0.001033  [65520/170046]\n",
            "loss: 0.001389  [68040/170046]\n",
            "loss: 0.002002  [70560/170046]\n",
            "loss: 0.000624  [73080/170046]\n",
            "loss: 0.001311  [75600/170046]\n",
            "loss: 0.001252  [78120/170046]\n",
            "loss: 0.001442  [80640/170046]\n",
            "loss: 0.001249  [83160/170046]\n",
            "loss: 0.001659  [85680/170046]\n",
            "loss: 0.000889  [88200/170046]\n",
            "loss: 0.000769  [90720/170046]\n",
            "loss: 0.001429  [93240/170046]\n",
            "loss: 0.001041  [95760/170046]\n",
            "loss: 0.001128  [98280/170046]\n",
            "loss: 0.000605  [100800/170046]\n",
            "loss: 0.000680  [103320/170046]\n",
            "loss: 0.001333  [105840/170046]\n",
            "loss: 0.001032  [108360/170046]\n",
            "loss: 0.000834  [110880/170046]\n",
            "loss: 0.000870  [113400/170046]\n",
            "loss: 0.000970  [115920/170046]\n",
            "loss: 0.000994  [118440/170046]\n",
            "loss: 0.001135  [120960/170046]\n",
            "loss: 0.000761  [123480/170046]\n",
            "loss: 0.002187  [126000/170046]\n",
            "loss: 0.000677  [128520/170046]\n",
            "loss: 0.001032  [131040/170046]\n",
            "loss: 0.000885  [133560/170046]\n",
            "loss: 0.000842  [136080/170046]\n",
            "loss: 0.000911  [138600/170046]\n",
            "loss: 0.000707  [141120/170046]\n",
            "loss: 0.000929  [143640/170046]\n",
            "loss: 0.001439  [146160/170046]\n",
            "loss: 0.000933  [148680/170046]\n",
            "loss: 0.001123  [151200/170046]\n",
            "loss: 0.001130  [153720/170046]\n",
            "loss: 0.000926  [156240/170046]\n",
            "loss: 0.001145  [158760/170046]\n",
            "loss: 0.001090  [161280/170046]\n",
            "loss: 0.000495  [163800/170046]\n",
            "loss: 0.000751  [166320/170046]\n",
            "loss: 0.000675  [168840/170046]\n",
            "tremolo rate: avg MSE: 0.001784, avg abs error: 0.0236\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 12\n",
            "loss: 0.000620  [  0/170046]\n",
            "loss: 0.001162  [2520/170046]\n",
            "loss: 0.000490  [5040/170046]\n",
            "loss: 0.001262  [7560/170046]\n",
            "loss: 0.000972  [10080/170046]\n",
            "loss: 0.000554  [12600/170046]\n",
            "loss: 0.000728  [15120/170046]\n",
            "loss: 0.000694  [17640/170046]\n",
            "loss: 0.001441  [20160/170046]\n",
            "loss: 0.001003  [22680/170046]\n",
            "loss: 0.001117  [25200/170046]\n",
            "loss: 0.000813  [27720/170046]\n",
            "loss: 0.000807  [30240/170046]\n",
            "loss: 0.000809  [32760/170046]\n",
            "loss: 0.000472  [35280/170046]\n",
            "loss: 0.000765  [37800/170046]\n",
            "loss: 0.001195  [40320/170046]\n",
            "loss: 0.000838  [42840/170046]\n",
            "loss: 0.001129  [45360/170046]\n",
            "loss: 0.000625  [47880/170046]\n",
            "loss: 0.000870  [50400/170046]\n",
            "loss: 0.000727  [52920/170046]\n",
            "loss: 0.000880  [55440/170046]\n",
            "loss: 0.000729  [57960/170046]\n",
            "loss: 0.000963  [60480/170046]\n",
            "loss: 0.000560  [63000/170046]\n",
            "loss: 0.000734  [65520/170046]\n",
            "loss: 0.001256  [68040/170046]\n",
            "loss: 0.000841  [70560/170046]\n",
            "loss: 0.000894  [73080/170046]\n",
            "loss: 0.000842  [75600/170046]\n",
            "loss: 0.000729  [78120/170046]\n",
            "loss: 0.001642  [80640/170046]\n",
            "loss: 0.000697  [83160/170046]\n",
            "loss: 0.001304  [85680/170046]\n",
            "loss: 0.000486  [88200/170046]\n",
            "loss: 0.000874  [90720/170046]\n",
            "loss: 0.000818  [93240/170046]\n",
            "loss: 0.001089  [95760/170046]\n",
            "loss: 0.000991  [98280/170046]\n",
            "loss: 0.000682  [100800/170046]\n",
            "loss: 0.001498  [103320/170046]\n",
            "loss: 0.000570  [105840/170046]\n",
            "loss: 0.000694  [108360/170046]\n",
            "loss: 0.001119  [110880/170046]\n",
            "loss: 0.000797  [113400/170046]\n",
            "loss: 0.000681  [115920/170046]\n",
            "loss: 0.000804  [118440/170046]\n",
            "loss: 0.001178  [120960/170046]\n",
            "loss: 0.000636  [123480/170046]\n",
            "loss: 0.001069  [126000/170046]\n",
            "loss: 0.001004  [128520/170046]\n",
            "loss: 0.001271  [131040/170046]\n",
            "loss: 0.000890  [133560/170046]\n",
            "loss: 0.000954  [136080/170046]\n",
            "loss: 0.001361  [138600/170046]\n",
            "loss: 0.000758  [141120/170046]\n",
            "loss: 0.001444  [143640/170046]\n",
            "loss: 0.000931  [146160/170046]\n",
            "loss: 0.000900  [148680/170046]\n",
            "loss: 0.001471  [151200/170046]\n",
            "loss: 0.000891  [153720/170046]\n",
            "loss: 0.001182  [156240/170046]\n",
            "loss: 0.001061  [158760/170046]\n",
            "loss: 0.000756  [161280/170046]\n",
            "loss: 0.001100  [163800/170046]\n",
            "loss: 0.000973  [166320/170046]\n",
            "loss: 0.000696  [168840/170046]\n",
            "tremolo rate: avg MSE: 0.001900, avg abs error: 0.0253\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 13\n",
            "loss: 0.000775  [  0/170046]\n",
            "loss: 0.000718  [2520/170046]\n",
            "loss: 0.000819  [5040/170046]\n",
            "loss: 0.000685  [7560/170046]\n",
            "loss: 0.000913  [10080/170046]\n",
            "loss: 0.000746  [12600/170046]\n",
            "loss: 0.000917  [15120/170046]\n",
            "loss: 0.001229  [17640/170046]\n",
            "loss: 0.001451  [20160/170046]\n",
            "loss: 0.000944  [22680/170046]\n",
            "loss: 0.000633  [25200/170046]\n",
            "loss: 0.000738  [27720/170046]\n",
            "loss: 0.000723  [30240/170046]\n",
            "loss: 0.001146  [32760/170046]\n",
            "loss: 0.000901  [35280/170046]\n",
            "loss: 0.000968  [37800/170046]\n",
            "loss: 0.001058  [40320/170046]\n",
            "loss: 0.001129  [42840/170046]\n",
            "loss: 0.001106  [45360/170046]\n",
            "loss: 0.000737  [47880/170046]\n",
            "loss: 0.000704  [50400/170046]\n",
            "loss: 0.000843  [52920/170046]\n",
            "loss: 0.000861  [55440/170046]\n",
            "loss: 0.000770  [57960/170046]\n",
            "loss: 0.000883  [60480/170046]\n",
            "loss: 0.000604  [63000/170046]\n",
            "loss: 0.000786  [65520/170046]\n",
            "loss: 0.000663  [68040/170046]\n",
            "loss: 0.000663  [70560/170046]\n",
            "loss: 0.000835  [73080/170046]\n",
            "loss: 0.001034  [75600/170046]\n",
            "loss: 0.000489  [78120/170046]\n",
            "loss: 0.000668  [80640/170046]\n",
            "loss: 0.000764  [83160/170046]\n",
            "loss: 0.001505  [85680/170046]\n",
            "loss: 0.000966  [88200/170046]\n",
            "loss: 0.001202  [90720/170046]\n",
            "loss: 0.000714  [93240/170046]\n",
            "loss: 0.000792  [95760/170046]\n",
            "loss: 0.001585  [98280/170046]\n",
            "loss: 0.001244  [100800/170046]\n",
            "loss: 0.001072  [103320/170046]\n",
            "loss: 0.001671  [105840/170046]\n",
            "loss: 0.000943  [108360/170046]\n",
            "loss: 0.001439  [110880/170046]\n",
            "loss: 0.000516  [113400/170046]\n",
            "loss: 0.001356  [115920/170046]\n",
            "loss: 0.000774  [118440/170046]\n",
            "loss: 0.000866  [120960/170046]\n",
            "loss: 0.000888  [123480/170046]\n",
            "loss: 0.001018  [126000/170046]\n",
            "loss: 0.000727  [128520/170046]\n",
            "loss: 0.000759  [131040/170046]\n",
            "loss: 0.000869  [133560/170046]\n",
            "loss: 0.000832  [136080/170046]\n",
            "loss: 0.000755  [138600/170046]\n",
            "loss: 0.001339  [141120/170046]\n",
            "loss: 0.000976  [143640/170046]\n",
            "loss: 0.000802  [146160/170046]\n",
            "loss: 0.000701  [148680/170046]\n",
            "loss: 0.000709  [151200/170046]\n",
            "loss: 0.000801  [153720/170046]\n",
            "loss: 0.000710  [156240/170046]\n",
            "loss: 0.001365  [158760/170046]\n",
            "loss: 0.001044  [161280/170046]\n",
            "loss: 0.000911  [163800/170046]\n",
            "loss: 0.000764  [166320/170046]\n",
            "loss: 0.001018  [168840/170046]\n",
            "tremolo rate: avg MSE: 0.001842, avg abs error: 0.0245\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 14\n",
            "loss: 0.000660  [  0/170046]\n",
            "loss: 0.000585  [2520/170046]\n",
            "loss: 0.000805  [5040/170046]\n",
            "loss: 0.000444  [7560/170046]\n",
            "loss: 0.001248  [10080/170046]\n",
            "loss: 0.001217  [12600/170046]\n",
            "loss: 0.000895  [15120/170046]\n",
            "loss: 0.000784  [17640/170046]\n",
            "loss: 0.000488  [20160/170046]\n",
            "loss: 0.000931  [22680/170046]\n",
            "loss: 0.001046  [25200/170046]\n",
            "loss: 0.001240  [27720/170046]\n",
            "loss: 0.000761  [30240/170046]\n",
            "loss: 0.000798  [32760/170046]\n",
            "loss: 0.000894  [35280/170046]\n",
            "loss: 0.001025  [37800/170046]\n",
            "loss: 0.000622  [40320/170046]\n",
            "loss: 0.000798  [42840/170046]\n",
            "loss: 0.000823  [45360/170046]\n",
            "loss: 0.001015  [47880/170046]\n",
            "loss: 0.001600  [50400/170046]\n",
            "loss: 0.001326  [52920/170046]\n",
            "loss: 0.000644  [55440/170046]\n",
            "loss: 0.000711  [57960/170046]\n",
            "loss: 0.000758  [60480/170046]\n",
            "loss: 0.000632  [63000/170046]\n",
            "loss: 0.000902  [65520/170046]\n",
            "loss: 0.000720  [68040/170046]\n",
            "loss: 0.000793  [70560/170046]\n",
            "loss: 0.000837  [73080/170046]\n",
            "loss: 0.000756  [75600/170046]\n",
            "loss: 0.001461  [78120/170046]\n",
            "loss: 0.000942  [80640/170046]\n",
            "loss: 0.000832  [83160/170046]\n",
            "loss: 0.000391  [85680/170046]\n",
            "loss: 0.000719  [88200/170046]\n",
            "loss: 0.000664  [90720/170046]\n",
            "loss: 0.000994  [93240/170046]\n",
            "loss: 0.000718  [95760/170046]\n",
            "loss: 0.001358  [98280/170046]\n",
            "loss: 0.000628  [100800/170046]\n",
            "loss: 0.000871  [103320/170046]\n",
            "loss: 0.000659  [105840/170046]\n",
            "loss: 0.000829  [108360/170046]\n",
            "loss: 0.000604  [110880/170046]\n",
            "loss: 0.001003  [113400/170046]\n",
            "loss: 0.000725  [115920/170046]\n",
            "loss: 0.000953  [118440/170046]\n",
            "loss: 0.000700  [120960/170046]\n",
            "loss: 0.001049  [123480/170046]\n",
            "loss: 0.000699  [126000/170046]\n",
            "loss: 0.000833  [128520/170046]\n",
            "loss: 0.000954  [131040/170046]\n",
            "loss: 0.000933  [133560/170046]\n",
            "loss: 0.000886  [136080/170046]\n",
            "loss: 0.000894  [138600/170046]\n",
            "loss: 0.000835  [141120/170046]\n",
            "loss: 0.000904  [143640/170046]\n",
            "loss: 0.001230  [146160/170046]\n",
            "loss: 0.000597  [148680/170046]\n",
            "loss: 0.000583  [151200/170046]\n",
            "loss: 0.001632  [153720/170046]\n",
            "loss: 0.000663  [156240/170046]\n",
            "loss: 0.000939  [158760/170046]\n",
            "loss: 0.000639  [161280/170046]\n",
            "loss: 0.000962  [163800/170046]\n",
            "loss: 0.000900  [166320/170046]\n",
            "loss: 0.000616  [168840/170046]\n",
            "tremolo rate: avg MSE: 0.001818, avg abs error: 0.0241\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Epoch 15\n",
            "loss: 0.000767  [  0/170046]\n",
            "loss: 0.000973  [2520/170046]\n",
            "loss: 0.000988  [5040/170046]\n",
            "loss: 0.000527  [7560/170046]\n",
            "loss: 0.000856  [10080/170046]\n",
            "loss: 0.001254  [12600/170046]\n",
            "loss: 0.000541  [15120/170046]\n",
            "loss: 0.000551  [17640/170046]\n",
            "loss: 0.000958  [20160/170046]\n",
            "loss: 0.000959  [22680/170046]\n",
            "loss: 0.000624  [25200/170046]\n",
            "loss: 0.000669  [27720/170046]\n",
            "loss: 0.001438  [30240/170046]\n",
            "loss: 0.000887  [32760/170046]\n",
            "loss: 0.000600  [35280/170046]\n",
            "loss: 0.000742  [37800/170046]\n",
            "loss: 0.001200  [40320/170046]\n",
            "loss: 0.000745  [42840/170046]\n",
            "loss: 0.000869  [45360/170046]\n",
            "loss: 0.000597  [47880/170046]\n",
            "loss: 0.000836  [50400/170046]\n",
            "loss: 0.000924  [52920/170046]\n",
            "loss: 0.001003  [55440/170046]\n",
            "loss: 0.000732  [57960/170046]\n",
            "loss: 0.000632  [60480/170046]\n",
            "loss: 0.000956  [63000/170046]\n",
            "loss: 0.000767  [65520/170046]\n",
            "loss: 0.000544  [68040/170046]\n",
            "loss: 0.000937  [70560/170046]\n",
            "loss: 0.000640  [73080/170046]\n",
            "loss: 0.000615  [75600/170046]\n",
            "loss: 0.001016  [78120/170046]\n",
            "loss: 0.000684  [80640/170046]\n",
            "loss: 0.000839  [83160/170046]\n",
            "loss: 0.000540  [85680/170046]\n",
            "loss: 0.000735  [88200/170046]\n",
            "loss: 0.000605  [90720/170046]\n",
            "loss: 0.000760  [93240/170046]\n",
            "loss: 0.000809  [95760/170046]\n",
            "loss: 0.000765  [98280/170046]\n",
            "loss: 0.000636  [100800/170046]\n",
            "loss: 0.000829  [103320/170046]\n",
            "loss: 0.000956  [105840/170046]\n",
            "loss: 0.000638  [108360/170046]\n",
            "loss: 0.000616  [110880/170046]\n",
            "loss: 0.001425  [113400/170046]\n",
            "loss: 0.000872  [115920/170046]\n",
            "loss: 0.000958  [118440/170046]\n",
            "loss: 0.000646  [120960/170046]\n",
            "loss: 0.000792  [123480/170046]\n",
            "loss: 0.000733  [126000/170046]\n",
            "loss: 0.001437  [128520/170046]\n",
            "loss: 0.001064  [131040/170046]\n",
            "loss: 0.000561  [133560/170046]\n",
            "loss: 0.000731  [136080/170046]\n",
            "loss: 0.000897  [138600/170046]\n",
            "loss: 0.001016  [141120/170046]\n",
            "loss: 0.000905  [143640/170046]\n",
            "loss: 0.000772  [146160/170046]\n",
            "loss: 0.000523  [148680/170046]\n",
            "loss: 0.001235  [151200/170046]\n",
            "loss: 0.001346  [153720/170046]\n",
            "loss: 0.001091  [156240/170046]\n",
            "loss: 0.000703  [158760/170046]\n",
            "loss: 0.000806  [161280/170046]\n",
            "loss: 0.000636  [163800/170046]\n",
            "loss: 0.000906  [166320/170046]\n",
            "loss: 0.000718  [168840/170046]\n",
            "tremolo rate: avg MSE: 0.001819, avg abs error: 0.0245\n",
            "learning rate: 0.000100 -> 0.000100\n",
            "---------------------------\n",
            "\n",
            "Finished training\n",
            "tremolo rate: avg MSE: 0.001862, avg abs error: 0.0248\n"
          ]
        }
      ],
      "source": [
        "from src.util import plot_violin\n",
        "import numpy as np\n",
        "\n",
        "WEIGHTS_DIR = \"_weights/\"\n",
        "LEARNING_RATE = 0.001\n",
        "EPOCHS = 15\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "print(f\"Using device {device}\")\n",
        "\n",
        "error = []\n",
        "\n",
        "fx = EFFECT_MAP.index(\"tremolo\")\n",
        "\n",
        "WEIGHTS_PATH = os.path.join(WEIGHTS_DIR, EXPERIMENT_NAME + \"_\" + str(fx))\n",
        "\n",
        "if not os.path.exists('%s' % WEIGHTS_DIR):\n",
        "    os.makedirs('%s' % WEIGHTS_DIR)\n",
        "\n",
        "fxData = load_train_data(fx)\n",
        "# fxData, _ = torch.utils.data.random_split(fxData, lengths=[0.01, 0.99])\n",
        "\n",
        "train_dataloader, test_dataloader = split_data(fxData)\n",
        "val_dataloader = load_evaluation_data(fx)\n",
        "\n",
        "# construct model and assign it to device\n",
        "cnn = model.Extractor().to(device)\n",
        "\n",
        "# if fx == 0:\n",
        "#     signal, _, _, _, _ = fxData[0]\n",
        "#     print(f\"There are {len(fxData)} samples in the dataset.\")\n",
        "#     print(f\"Shape of signal: {signal.shape}\")\n",
        "\n",
        "#     print(\"input feature:\")\n",
        "#     log_writer.add_figure(\"Input Feature\", plot_spectrogram(signal[0], title=\"MFCC\"))\n",
        "#     log_writer.add_graph(cnn, signal.unsqueeze_(0))\n",
        "\n",
        "# initialise loss funtion + optimiser\n",
        "loss_fn = nn.MSELoss(reduction='mean')\n",
        "optimiser = torch.optim.Adam(cnn.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# train model\n",
        "train.train(cnn,\n",
        "            train_dataloader,\n",
        "            test_dataloader,\n",
        "            loss_fn,\n",
        "            optimiser,\n",
        "            device,\n",
        "            log_writer,\n",
        "            EPOCHS,\n",
        "            WEIGHTS_PATH,\n",
        "            effect=fx)\n",
        "\n",
        "_, _, log = train.test(cnn, val_dataloader, device, effect=fx)\n",
        "for _, data in enumerate(log):\n",
        "    error.append(data[3])\n",
        "\n",
        "arr = np.array(error)\n",
        "np.save(EVU_DIR + EXPERIMENT_NAME + \"_\" + str(fx) + \"_evaluation.npy\", arr)\n",
        "\n",
        "# log_writer.add_figure(\"Error Box\", \n",
        "#                       plot_violin(error, title=\"Error\", labels=EFFECT_MAP, ylabel=\"parameter value\", outlier=True))\n",
        "\n",
        "log_writer.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "torchaudio-tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
